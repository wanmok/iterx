2023-11-07 18:32:48,116 - INFO - torch.distributed.nn.jit.instantiator - Created a temporary directory at /tmp/tmpufd8pfb7
2023-11-07 18:32:48,117 - INFO - torch.distributed.nn.jit.instantiator - Writing /tmp/tmpufd8pfb7/_remote_module_non_scriptable.py
2023-11-07 18:32:49,619 - INFO - numexpr.utils - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2023-11-07 18:32:49,619 - INFO - numexpr.utils - NumExpr defaulting to 8 threads.
2023-11-07 18:32:50,068 - INFO - allennlp.common.params - evaluation = None
2023-11-07 18:32:50,069 - INFO - allennlp.common.params - include_in_archive = None
2023-11-07 18:32:50,070 - INFO - allennlp.common.params - random_seed = 13370
2023-11-07 18:32:50,070 - INFO - allennlp.common.params - numpy_seed = 1337
2023-11-07 18:32:50,070 - INFO - allennlp.common.params - pytorch_seed = 133
2023-11-07 18:32:50,076 - INFO - allennlp.common.checks - Pytorch version: 2.0.1
2023-11-07 18:32:50,076 - INFO - allennlp.common.params - type = default
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.type = famus
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.definition_file = resources/data/famus_with_prefixed_roles/definitions.json
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = t5-large
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 1024
2023-11-07 18:32:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
/home/svashis3/anaconda3/envs/iterx/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
2023-11-07 18:32:50,967 - INFO - allennlp.common.params - dataset_reader.is_training = True
2023-11-07 18:32:50,967 - INFO - allennlp.common.params - dataset_reader.skip_docs_without_templates = False
2023-11-07 18:32:50,968 - INFO - allennlp.common.params - dataset_reader.skip_docs_without_spans = True
2023-11-07 18:32:50,968 - INFO - allennlp.common.params - dataset_reader.verbose = False
2023-11-07 18:32:50,971 - INFO - allennlp.common.params - train_data_path = resources/data/famus_with_prefixed_roles/source_data/mixed_spans/train.jsonl
2023-11-07 18:32:50,972 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2023-11-07 18:32:50,972 - INFO - allennlp.common.params - validation_dataset_reader.type = famus
2023-11-07 18:32:50,972 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2023-11-07 18:32:50,972 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2023-11-07 18:32:50,972 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False
2023-11-07 18:32:50,972 - INFO - allennlp.common.params - validation_dataset_reader.definition_file = resources/data/famus_with_prefixed_roles/definitions.json
2023-11-07 18:32:50,973 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched
2023-11-07 18:32:50,973 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0
2023-11-07 18:32:50,973 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = t5-large
2023-11-07 18:32:50,973 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags
2023-11-07 18:32:50,973 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 1024
2023-11-07 18:32:50,973 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None
2023-11-07 18:32:50,974 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2023-11-07 18:32:50,974 - INFO - allennlp.common.params - validation_dataset_reader.skip_docs_without_templates = True
2023-11-07 18:32:50,974 - INFO - allennlp.common.params - validation_dataset_reader.skip_docs_without_spans = True
2023-11-07 18:32:50,974 - INFO - allennlp.common.params - validation_dataset_reader.verbose = False
2023-11-07 18:32:50,977 - INFO - allennlp.common.params - validation_data_path = resources/data/famus_with_prefixed_roles/source_data/mixed_spans/dev.jsonl
2023-11-07 18:32:50,977 - INFO - allennlp.common.params - validation_data_loader = None
2023-11-07 18:32:50,977 - INFO - allennlp.common.params - test_data_path = resources/data/famus_with_prefixed_roles/source_data/mixed_spans/test.jsonl
2023-11-07 18:32:50,977 - INFO - allennlp.common.params - evaluate_on_test = False
2023-11-07 18:32:50,977 - INFO - allennlp.common.params - batch_weight_key = 
2023-11-07 18:32:50,977 - INFO - allennlp.common.params - data_loader.type = multiprocess
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.batch_size = None
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.drop_last = False
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.shuffle = False
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.num_workers = 0
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.start_method = fork
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.cuda_device = None
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.quiet = False
2023-11-07 18:32:50,978 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f14e15d0b50>
loading instances: 0it [00:00, ?it/s]2023-11-07 18:32:50,986 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'event_types'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.
2023-11-07 18:32:50,986 - WARNING - allennlp.data.fields.label_field - Your label namespace was 'slot_types'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.
loading instances: 10it [00:00, 48.21it/s]loading instances: 26it [00:00, 56.78it/s]loading instances: 40it [00:00, 54.23it/s]loading instances: 58it [00:01, 53.87it/s]loading instances: 78it [00:01, 55.98it/s]loading instances: 103it [00:01, 83.82it/s]loading instances: 116it [00:01, 63.41it/s]loading instances: 137it [00:01, 84.45it/s]loading instances: 150it [00:02, 59.83it/s]loading instances: 176it [00:02, 87.07it/s]loading instances: 196it [00:02, 65.37it/s]loading instances: 221it [00:03, 88.34it/s]loading instances: 244it [00:03, 109.48it/s]loading instances: 262it [00:03, 63.69it/s] loading instances: 279it [00:03, 75.82it/s]loading instances: 297it [00:03, 90.56it/s]loading instances: 313it [00:04, 52.51it/s]loading instances: 334it [00:04, 69.35it/s]loading instances: 360it [00:04, 94.81it/s]loading instances: 388it [00:04, 124.44it/s]loading instances: 409it [00:05, 62.45it/s] loading instances: 427it [00:05, 74.47it/s]loading instances: 453it [00:05, 97.58it/s]loading instances: 475it [00:06, 116.07it/s]loading instances: 500it [00:06, 139.94it/s]loading instances: 521it [00:07, 56.19it/s] loading instances: 540it [00:07, 68.83it/s]loading instances: 571it [00:07, 97.92it/s]loading instances: 597it [00:07, 120.77it/s]loading instances: 621it [00:07, 140.35it/s]loading instances: 646it [00:07, 161.21it/s]loading instances: 669it [00:08, 53.88it/s] loading instances: 686it [00:08, 63.12it/s]loading instances: 707it [00:08, 78.96it/s]loading instances: 725it [00:09, 90.10it/s]loading instances: 749it [00:09, 113.64it/s]2023-11-07 18:33:00,219 - WARNING - iterx.data.dataset.famus_dataset - Read 759 documents. Of these, 759 had both templates and spans. 0 had no templates and 0 had no spans.
loading instances: 759it [00:09, 82.14it/s] 
2023-11-07 18:33:00,219 - INFO - allennlp.common.params - data_loader.type = multiprocess
2023-11-07 18:33:00,219 - INFO - allennlp.common.params - data_loader.batch_size = None
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.drop_last = False
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.shuffle = False
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.num_workers = 0
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.start_method = fork
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.cuda_device = None
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.quiet = False
2023-11-07 18:33:00,220 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f14e15d0b50>
loading instances: 0it [00:00, ?it/s]loading instances: 19it [00:00, 184.09it/s]loading instances: 38it [00:01, 20.07it/s] loading instances: 55it [00:01, 32.08it/s]loading instances: 71it [00:01, 45.08it/s]loading instances: 87it [00:01, 59.13it/s]loading instances: 106it [00:02, 78.83it/s]loading instances: 122it [00:02, 91.21it/s]loading instances: 141it [00:02, 108.79it/s]loading instances: 160it [00:02, 126.11it/s]loading instances: 181it [00:02, 144.84it/s]loading instances: 199it [00:04, 31.53it/s] loading instances: 223it [00:04, 45.34it/s]loading instances: 240it [00:04, 56.30it/s]2023-11-07 18:33:04,653 - WARNING - iterx.data.dataset.famus_dataset - Read 253 documents. Of these, 253 had both templates and spans. 0 had no templates and 0 had no spans.
loading instances: 253it [00:04, 57.07it/s]
2023-11-07 18:33:04,654 - INFO - allennlp.common.params - data_loader.type = multiprocess
2023-11-07 18:33:04,654 - INFO - allennlp.common.params - data_loader.batch_size = None
2023-11-07 18:33:04,654 - INFO - allennlp.common.params - data_loader.drop_last = False
2023-11-07 18:33:04,654 - INFO - allennlp.common.params - data_loader.shuffle = False
2023-11-07 18:33:04,654 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2023-11-07 18:33:04,654 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1
2023-11-07 18:33:04,654 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.num_workers = 0
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.start_method = fork
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.cuda_device = None
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.quiet = False
2023-11-07 18:33:04,655 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f14e15d0b50>
loading instances: 0it [00:00, ?it/s]loading instances: 28it [00:00, 267.52it/s]loading instances: 55it [00:00, 238.45it/s]loading instances: 80it [00:00, 242.26it/s]loading instances: 105it [00:00, 214.72it/s]loading instances: 130it [00:00, 224.87it/s]loading instances: 153it [00:00, 225.36it/s]loading instances: 176it [00:02, 32.77it/s] loading instances: 205it [00:02, 48.07it/s]loading instances: 227it [00:02, 61.52it/s]2023-11-07 18:33:07,651 - WARNING - iterx.data.dataset.famus_dataset - Read 253 documents. Of these, 253 had both templates and spans. 0 had no templates and 0 had no spans.
loading instances: 253it [00:02, 84.43it/s]
2023-11-07 18:33:07,652 - INFO - allennlp.common.params - vocabulary.type = from_files
2023-11-07 18:33:07,652 - INFO - allennlp.common.params - vocabulary.directory = resources/data/famus_with_prefixed_roles/vocabulary
2023-11-07 18:33:07,652 - INFO - allennlp.common.params - vocabulary.padding_token = @@PADDING@@
2023-11-07 18:33:07,652 - INFO - allennlp.common.params - vocabulary.oov_token = @@UNKNOWN@@
2023-11-07 18:33:07,652 - INFO - allennlp.data.vocabulary - Loading token dictionary from resources/data/famus_with_prefixed_roles/vocabulary.
2023-11-07 18:33:07,657 - INFO - allennlp.common.params - model.type = iterative_template_extraction_famus
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.regularizer = None
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.definition_file = resources/data/famus_with_prefixed_roles/definitions.json
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.type = pytorch_transformer
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.input_dim = 1024
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.num_layers = 3
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.feedforward_hidden_dim = 2048
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.num_attention_heads = 64
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.positional_encoding = None
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.positional_embedding_size = 512
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.dropout_prob = 0.3
2023-11-07 18:33:07,658 - INFO - allennlp.common.params - model.graph_encoder.activation = gelu
2023-11-07 18:33:07,940 - INFO - allennlp.common.params - model.span_sampling_rate = 1
2023-11-07 18:33:07,940 - INFO - allennlp.common.params - model.use_stochastic_layer_selection = False
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = t5-large
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 1024
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.load_weights = True
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None
2023-11-07 18:33:07,941 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32100. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.text_field_embedder_output_dim = 1024
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.expert_roll_out = 0.6
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.pairwise_scoring = True
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.span_slot_none_penalty = 0.0
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.non_span_slot_none_penalty = 0.0
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.span_slot_none_weight = 0.5
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.non_span_slot_none_weight = 0.1
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.lexical_dropout = 0.2
2023-11-07 18:33:18,696 - INFO - allennlp.common.params - model.feature_dropout = 0.3
2023-11-07 18:33:18,697 - INFO - allennlp.common.params - model.metrics.famus.type = iterx_famus
2023-11-07 18:33:18,697 - INFO - allennlp.common.params - model.metrics.famus.convert_doc_id = False
2023-11-07 18:33:18,697 - INFO - allennlp.common.params - model.metrics.famus.ignore_no_template_doc = True
2023-11-07 18:33:18,697 - INFO - allennlp.common.params - model.metrics.famus.sanitize_special_chars = True
2023-11-07 18:33:18,697 - INFO - allennlp.common.params - model.metrics.famus.scorer_type = phi-3
2023-11-07 18:33:19,642 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = normal
2023-11-07 18:33:19,642 - INFO - allennlp.common.params - model.initializer.regexes.0.1.mean = 0
2023-11-07 18:33:19,642 - INFO - allennlp.common.params - model.initializer.regexes.0.1.std = 0.02
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.1.1.type = normal
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.1.1.mean = 0
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.1.1.std = 0.02
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.2.1.type = normal
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.2.1.mean = 0
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.2.1.std = 0.02
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.3.1.type = normal
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.3.1.mean = 0
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.3.1.std = 0.02
2023-11-07 18:33:19,643 - INFO - allennlp.common.params - model.initializer.regexes.4.1.type = normal
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.4.1.mean = 0
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.4.1.std = 0.02
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.5.1.type = normal
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.5.1.mean = 0
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.5.1.std = 0.02
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.6.1.type = normal
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.6.1.mean = 0
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.6.1.std = 0.02
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.7.1.type = normal
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.7.1.mean = 0
2023-11-07 18:33:19,644 - INFO - allennlp.common.params - model.initializer.regexes.7.1.std = 0.02
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.8.1.type = normal
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.8.1.mean = 0
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.8.1.std = 0.02
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.9.1.type = normal
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.9.1.mean = 0
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.9.1.std = 0.02
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.10.1.type = normal
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.10.1.mean = 0
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.10.1.std = 0.02
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.11.1.type = normal
2023-11-07 18:33:19,645 - INFO - allennlp.common.params - model.initializer.regexes.11.1.mean = 0
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.initializer.regexes.11.1.std = 0.02
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.initializer.regexes.12.1.type = normal
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.initializer.regexes.12.1.mean = 0
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.initializer.regexes.12.1.std = 0.02
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.initializer.regexes.13.1.type = constant
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.initializer.regexes.13.1.val = 1
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.initializer.regexes.14.1.type = zero
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.iteration_cutoff = 1
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.grad_discount_factor = 1
2023-11-07 18:33:19,646 - INFO - allennlp.common.params - model.span_reuse_discount_factor = None
2023-11-07 18:33:19,647 - INFO - allennlp.common.params - model.teacher_configs = None
2023-11-07 18:33:19,647 - INFO - allennlp.common.params - model.template_set_decoder_use_sampling = False
2023-11-07 18:33:19,647 - INFO - allennlp.common.params - model.use_checkpoint = True
2023-11-07 18:33:20,001 - INFO - allennlp.nn.initializers - Initializing parameters
2023-11-07 18:33:20,030 - INFO - allennlp.nn.initializers - Initializing attentive_span_extractor._global_attention._module.weight using .*_global_attention.*weight initializer
2023-11-07 18:33:20,031 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.self_attn.in_proj_weight using .*_transformer.*self_attn.*\.in_proj_weight initializer
2023-11-07 18:33:20,052 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.self_attn.out_proj.weight using .*_transformer.*self_attn.*\.weight initializer
2023-11-07 18:33:20,060 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.self_attn.out_proj.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,060 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.linear1.weight using .*_transformer.*linear.*\.weight initializer
2023-11-07 18:33:20,075 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.linear1.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,075 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.linear2.weight using .*_transformer.*linear.*\.weight initializer
2023-11-07 18:33:20,089 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.linear2.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,089 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.norm1.weight using .*_transformer.*norm.*\.weight initializer
2023-11-07 18:33:20,089 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.norm1.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,089 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.norm2.weight using .*_transformer.*norm.*\.weight initializer
2023-11-07 18:33:20,090 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.0.norm2.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,090 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.self_attn.in_proj_weight using .*_transformer.*self_attn.*\.in_proj_weight initializer
2023-11-07 18:33:20,111 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.self_attn.out_proj.weight using .*_transformer.*self_attn.*\.weight initializer
2023-11-07 18:33:20,119 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.self_attn.out_proj.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,119 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.linear1.weight using .*_transformer.*linear.*\.weight initializer
2023-11-07 18:33:20,134 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.linear1.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,134 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.linear2.weight using .*_transformer.*linear.*\.weight initializer
2023-11-07 18:33:20,148 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.linear2.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,148 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.norm1.weight using .*_transformer.*norm.*\.weight initializer
2023-11-07 18:33:20,149 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.norm1.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,149 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.norm2.weight using .*_transformer.*norm.*\.weight initializer
2023-11-07 18:33:20,149 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.1.norm2.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,149 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.self_attn.in_proj_weight using .*_transformer.*self_attn.*\.in_proj_weight initializer
2023-11-07 18:33:20,171 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.self_attn.out_proj.weight using .*_transformer.*self_attn.*\.weight initializer
2023-11-07 18:33:20,178 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.self_attn.out_proj.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,178 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.linear1.weight using .*_transformer.*linear.*\.weight initializer
2023-11-07 18:33:20,193 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.linear1.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,193 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.linear2.weight using .*_transformer.*linear.*\.weight initializer
2023-11-07 18:33:20,208 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.linear2.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,208 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.norm1.weight using .*_transformer.*norm.*\.weight initializer
2023-11-07 18:33:20,208 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.norm1.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,208 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.norm2.weight using .*_transformer.*norm.*\.weight initializer
2023-11-07 18:33:20,208 - INFO - allennlp.nn.initializers - Initializing graph_encoder._transformer.layers.2.norm2.bias using .*_transformer.*\.bias initializer
2023-11-07 18:33:20,208 - INFO - allennlp.nn.initializers - Initializing template_embeddings.weight using template_embeddings.weight initializer
2023-11-07 18:33:20,210 - INFO - allennlp.nn.initializers - Initializing event_type_embeddings.weight using event_type_embeddings.weight initializer
2023-11-07 18:33:20,210 - INFO - allennlp.nn.initializers - Initializing span_type_embeddings.weight using span_type_embeddings.weight initializer
2023-11-07 18:33:20,210 - INFO - allennlp.nn.initializers - Initializing slot_type_embeddings.weight using slot_type_embeddings.weight initializer
2023-11-07 18:33:20,216 - INFO - allennlp.nn.initializers - Initializing compression_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer
2023-11-07 18:33:20,238 - INFO - allennlp.nn.initializers - Initializing compression_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer
2023-11-07 18:33:20,245 - INFO - allennlp.nn.initializers - Initializing eos_feedforward.0._linear_layers.0.weight using .*linear_layers.*weight initializer
2023-11-07 18:33:20,253 - INFO - allennlp.nn.initializers - Initializing eos_feedforward.0._linear_layers.1.weight using .*linear_layers.*weight initializer
2023-11-07 18:33:20,260 - INFO - allennlp.nn.initializers - Initializing slot_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer
2023-11-07 18:33:20,275 - INFO - allennlp.nn.initializers - Initializing slot_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer
2023-11-07 18:33:20,282 - INFO - allennlp.nn.initializers - Initializing slot_memory_update_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer
2023-11-07 18:33:20,297 - INFO - allennlp.nn.initializers - Initializing template_memory_update_feedforward._linear_layers.0.weight using .*linear_layers.*weight initializer
2023-11-07 18:33:20,305 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: event_arg_embeddings.weight
2023-11-07 18:33:20,305 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*_span_updating_gated_sum.*weight
2023-11-07 18:33:20,305 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: hetero_gnn.*lin.*\.weight
2023-11-07 18:33:20,305 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: hetero_gnn.*att
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    attentive_span_extractor._global_attention._module.bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    compression_feedforward._module._linear_layers.0.bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    compression_feedforward._module._linear_layers.1.bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    eos_feedforward.0._linear_layers.0.bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    eos_feedforward.0._linear_layers.1.bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    eos_feedforward.1.bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    eos_feedforward.1.weight
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    graph_encoder._transformer.layers.0.self_attn.in_proj_bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    graph_encoder._transformer.layers.1.self_attn.in_proj_bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    graph_encoder._transformer.layers.2.self_attn.in_proj_bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    memory_updator.bias_hh
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    memory_updator.bias_ih
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    memory_updator.weight_hh
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    memory_updator.weight_ih
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    non_span_embeddings.weight
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    non_span_slot_label_embeddings.weight
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    slot_feedforward._module._linear_layers.0.bias
2023-11-07 18:33:20,305 - INFO - allennlp.nn.initializers -    slot_feedforward._module._linear_layers.1.bias
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    slot_memory_update_feedforward._module._linear_layers.0.bias
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    template_memory_update_feedforward._linear_layers.0.bias
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.relative_attention_bias.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.layer_norm.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.layer_norm.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.layer_norm.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.layer_norm.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.layer_norm.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.layer_norm.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.layer_norm.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,306 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.layer_norm.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,307 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.layer_norm.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,308 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.layer_norm.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,309 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.layer_norm.weight
2023-11-07 18:33:20,310 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.layer_norm.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.layer_norm.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.layer_norm.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.k.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.o.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.q.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.v.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.layer_norm.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.DenseReluDense.wi.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.DenseReluDense.wo.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.layer_norm.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embed_tokens.weight
2023-11-07 18:33:20,311 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.final_layer_norm.weight
2023-11-07 18:34:20,180 - INFO - allennlp.common.params - trainer.type = gradient_descent
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.cuda_device = None
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.distributed = False
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.world_size = 1
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.patience = 30
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.validation_metric = +iterx_famus_slot_f1
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.num_epochs = 150
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.grad_norm = 1
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.grad_clipping = None
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.use_amp = False
2023-11-07 18:34:20,181 - INFO - allennlp.common.params - trainer.no_grad = None
2023-11-07 18:34:20,182 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2023-11-07 18:34:20,182 - INFO - allennlp.common.params - trainer.moving_average = None
2023-11-07 18:34:20,182 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f14894c2c90>
2023-11-07 18:34:20,182 - INFO - allennlp.common.params - trainer.callbacks = None
2023-11-07 18:34:20,182 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True
2023-11-07 18:34:20,182 - INFO - allennlp.common.params - trainer.run_confidence_checks = True
2023-11-07 18:34:20,182 - INFO - allennlp.common.params - trainer.grad_scaling = True
2023-11-07 18:34:23,316 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2023-11-07 18:34:23,317 - INFO - allennlp.common.params - trainer.optimizer.lr = 3e-05
2023-11-07 18:34:23,317 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)
2023-11-07 18:34:23,317 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08
2023-11-07 18:34:23,317 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.01
2023-11-07 18:34:23,317 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2023-11-07 18:34:23,318 - INFO - allennlp.training.optimizers - Done constructing parameter groups.
2023-11-07 18:34:23,318 - INFO - allennlp.training.optimizers - Group 0: ['text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.DenseReluDense.wo.weight', 'graph_encoder._transformer.layers.1.norm1.bias', 'graph_encoder._transformer.layers.1.linear2.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.v.weight', 'graph_encoder._transformer.layers.2.self_attn.out_proj.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.q.weight', 'graph_encoder._transformer.layers.0.self_attn.out_proj.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.q.weight', 'graph_encoder._transformer.layers.2.linear2.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.DenseReluDense.wo.weight', 'graph_encoder._transformer.layers.0.norm1.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.layer_norm.weight', 'graph_encoder._transformer.layers.0.self_attn.in_proj_weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.o.weight', 'graph_encoder._transformer.layers.1.linear2.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.layer_norm.weight', 'graph_encoder._transformer.layers.0.norm2.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.k.weight', 'graph_encoder._transformer.layers.2.self_attn.in_proj_bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.final_layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.layer_norm.weight', 'graph_encoder._transformer.layers.1.norm2.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.q.weight', 'graph_encoder._transformer.layers.2.linear1.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embed_tokens.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.DenseReluDense.wo.weight', 'graph_encoder._transformer.layers.0.linear1.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.layer_norm.weight', 'graph_encoder._transformer.layers.2.linear2.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.DenseReluDense.wi.weight', 'graph_encoder._transformer.layers.1.self_attn.out_proj.weight', 'graph_encoder._transformer.layers.2.linear1.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.q.weight', 'graph_encoder._transformer.layers.1.norm1.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.layer_norm.weight', 'graph_encoder._transformer.layers.2.self_attn.out_proj.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.DenseReluDense.wi.weight', 'graph_encoder._transformer.layers.1.linear1.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.q.weight', 'graph_encoder._transformer.layers.2.norm2.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.q.weight', 'graph_encoder._transformer.layers.0.linear1.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.q.weight', 'graph_encoder._transformer.layers.1.self_attn.out_proj.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.o.weight', 'graph_encoder._transformer.layers.0.linear2.weight', 'graph_encoder._transformer.layers.0.norm1.bias', 'graph_encoder._transformer.layers.0.linear2.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.layer_norm.weight', 'graph_encoder._transformer.layers.1.norm2.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.layer_norm.weight', 'graph_encoder._transformer.layers.2.norm2.bias', 'graph_encoder._transformer.layers.1.linear1.weight', 'graph_encoder._transformer.layers.0.self_attn.in_proj_bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.o.weight', 'graph_encoder._transformer.layers.2.norm1.bias', 'graph_encoder._transformer.layers.2.self_attn.in_proj_weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.q.weight', 'graph_encoder._transformer.layers.1.self_attn.in_proj_bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.layer_norm.weight', 'graph_encoder._transformer.layers.1.self_attn.in_proj_weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.v.weight', 'graph_encoder._transformer.layers.2.norm1.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.DenseReluDense.wi.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.layer_norm.weight', 'graph_encoder._transformer.layers.0.self_attn.out_proj.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.o.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.DenseReluDense.wo.weight', 'graph_encoder._transformer.layers.0.norm2.bias', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.v.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.DenseReluDense.wo.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.q.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.k.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.layer_norm.weight', 'text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.k.weight'], {'lr': 1e-05}
2023-11-07 18:34:23,318 - INFO - allennlp.training.optimizers - Group 1: ['template_embeddings.weight', 'eos_feedforward.1.weight', 'slot_feedforward._module._linear_layers.0.bias', 'attentive_span_extractor._global_attention._module.weight', 'compression_feedforward._module._linear_layers.0.bias', 'slot_feedforward._module._linear_layers.0.weight', 'non_span_slot_label_embeddings.weight', 'eos_feedforward.1.bias', 'slot_feedforward._module._linear_layers.1.weight', 'eos_feedforward.0._linear_layers.1.weight', 'memory_updator.weight_ih', 'attentive_span_extractor._global_attention._module.bias', 'event_type_embeddings.weight', 'eos_feedforward.0._linear_layers.1.bias', 'slot_memory_update_feedforward._module._linear_layers.0.bias', 'compression_feedforward._module._linear_layers.1.bias', 'memory_updator.weight_hh', 'non_span_embeddings.weight', 'memory_updator.bias_ih', 'eos_feedforward.0._linear_layers.0.bias', 'slot_memory_update_feedforward._module._linear_layers.0.weight', 'slot_type_embeddings.weight', 'compression_feedforward._module._linear_layers.1.weight', 'memory_updator.bias_hh', 'span_type_embeddings.weight', 'eos_feedforward.0._linear_layers.0.weight', 'compression_feedforward._module._linear_layers.0.weight', 'slot_feedforward._module._linear_layers.1.bias', 'template_memory_update_feedforward._linear_layers.0.weight', 'template_memory_update_feedforward._linear_layers.0.bias'], {}
2023-11-07 18:34:23,318 - INFO - allennlp.training.optimizers - Number of trainable parameters: 380123651
/home/svashis3/anaconda3/envs/iterx/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-11-07 18:34:23,326 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2023-11-07 18:34:23,328 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2023-11-07 18:34:23,328 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embed_tokens.weight
2023-11-07 18:34:23,328 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,328 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,328 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,328 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,328 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.SelfAttention.relative_attention_bias.weight
2023-11-07 18:34:23,328 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.0.layer_norm.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.0.layer.1.layer_norm.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.0.layer_norm.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.1.layer.1.layer_norm.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.0.layer_norm.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.2.layer.1.layer_norm.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.0.layer_norm.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.3.layer.1.layer_norm.weight
2023-11-07 18:34:23,329 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.0.layer_norm.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.4.layer.1.layer_norm.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.0.layer_norm.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.5.layer.1.layer_norm.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.0.layer_norm.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.6.layer.1.layer_norm.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.0.layer_norm.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,330 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.7.layer.1.layer_norm.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.0.layer_norm.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.8.layer.1.layer_norm.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.0.layer_norm.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.9.layer.1.layer_norm.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.0.layer_norm.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.10.layer.1.layer_norm.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,331 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.0.layer_norm.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.11.layer.1.layer_norm.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.0.layer_norm.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.12.layer.1.layer_norm.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.0.layer_norm.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.13.layer.1.layer_norm.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.0.layer_norm.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.14.layer.1.layer_norm.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,332 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.0.layer_norm.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.15.layer.1.layer_norm.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.0.layer_norm.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.16.layer.1.layer_norm.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.0.layer_norm.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.17.layer.1.layer_norm.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.0.layer_norm.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.18.layer.1.layer_norm.weight
2023-11-07 18:34:23,333 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.0.layer_norm.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.19.layer.1.layer_norm.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.0.layer_norm.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.20.layer.1.layer_norm.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.0.layer_norm.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.21.layer.1.layer_norm.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.0.layer_norm.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,334 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.22.layer.1.layer_norm.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.q.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.k.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.v.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.SelfAttention.o.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.0.layer_norm.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.DenseReluDense.wi.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.DenseReluDense.wo.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.block.23.layer.1.layer_norm.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.final_layer_norm.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - attentive_span_extractor._global_attention._module.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - attentive_span_extractor._global_attention._module.bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.self_attn.in_proj_weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.self_attn.in_proj_bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.self_attn.out_proj.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.self_attn.out_proj.bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.linear1.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.linear1.bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.linear2.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.linear2.bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.norm1.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.norm1.bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.norm2.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.0.norm2.bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.self_attn.in_proj_weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.self_attn.in_proj_bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.self_attn.out_proj.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.self_attn.out_proj.bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.linear1.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.linear1.bias
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.linear2.weight
2023-11-07 18:34:23,335 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.linear2.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.norm1.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.norm1.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.norm2.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.1.norm2.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.self_attn.in_proj_weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.self_attn.in_proj_bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.self_attn.out_proj.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.self_attn.out_proj.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.linear1.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.linear1.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.linear2.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.linear2.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.norm1.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.norm1.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.norm2.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - graph_encoder._transformer.layers.2.norm2.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - template_embeddings.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - event_type_embeddings.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - span_type_embeddings.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - slot_type_embeddings.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - non_span_embeddings.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - non_span_slot_label_embeddings.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - compression_feedforward._module._linear_layers.0.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - compression_feedforward._module._linear_layers.0.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - compression_feedforward._module._linear_layers.1.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - compression_feedforward._module._linear_layers.1.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - eos_feedforward.0._linear_layers.0.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - eos_feedforward.0._linear_layers.0.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - eos_feedforward.0._linear_layers.1.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - eos_feedforward.0._linear_layers.1.bias
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - eos_feedforward.1.weight
2023-11-07 18:34:23,336 - INFO - allennlp.common.util - eos_feedforward.1.bias
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - slot_feedforward._module._linear_layers.0.weight
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - slot_feedforward._module._linear_layers.0.bias
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - slot_feedforward._module._linear_layers.1.weight
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - slot_feedforward._module._linear_layers.1.bias
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - slot_memory_update_feedforward._module._linear_layers.0.weight
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - slot_memory_update_feedforward._module._linear_layers.0.bias
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - template_memory_update_feedforward._linear_layers.0.weight
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - template_memory_update_feedforward._linear_layers.0.bias
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - memory_updator.weight_ih
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - memory_updator.weight_hh
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - memory_updator.bias_ih
2023-11-07 18:34:23,337 - INFO - allennlp.common.util - memory_updator.bias_hh
2023-11-07 18:34:23,337 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = polynomial_decay
2023-11-07 18:34:23,337 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.power = 2
2023-11-07 18:34:23,337 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.warmup_steps = 5000
2023-11-07 18:34:23,338 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.end_learning_rate = 1e-09
2023-11-07 18:34:23,338 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2023-11-07 18:34:23,338 - INFO - allennlp.common.params - type = default
2023-11-07 18:34:23,338 - INFO - allennlp.common.params - save_completed_epochs = True
2023-11-07 18:34:23,338 - INFO - allennlp.common.params - save_every_num_seconds = None
2023-11-07 18:34:23,338 - INFO - allennlp.common.params - save_every_num_batches = None
2023-11-07 18:34:23,338 - INFO - allennlp.common.params - keep_most_recent_by_count = 2
2023-11-07 18:34:23,338 - INFO - allennlp.common.params - keep_most_recent_by_age = None
2023-11-07 18:34:23,345 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.
2023-11-07 18:34:23,345 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/149
2023-11-07 18:34:23,345 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 8.4G
/home/svashis3/anaconda3/envs/iterx/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
2023-11-07 18:34:23,347 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 1.4G
2023-11-07 18:34:23,348 - INFO - allennlp.training.gradient_descent_trainer - Training
  0%|          | 0/759 [00:00<?, ?it/s]/home/svashis3/anaconda3/envs/iterx/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
2023-11-07 18:34:24,285 - INFO - allennlp.training.callbacks.console_logger - Batch inputs
2023-11-07 18:34:24,285 - INFO - allennlp.training.callbacks.console_logger - Field : "batch_input/metadata" : (Length 1 of type "<class 'dict'>")
2023-11-07 18:34:24,285 - INFO - allennlp.training.callbacks.console_logger - batch_input/template_type (Shape: 1)
tensor([107], device='cuda:0')
2023-11-07 18:34:24,286 - INFO - allennlp.training.callbacks.console_logger - Field : "batch_input/is_training" : True
2023-11-07 18:34:24,286 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/token_ids (Shape: 1 x 346)
tensor([[ 461,  335, 1797,  ...,    3,    5,    1]], device='cuda:0')
2023-11-07 18:34:24,287 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/mask (Shape: 1 x 257)
tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')
2023-11-07 18:34:24,288 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/type_ids (Shape: 1 x 346)
tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')
2023-11-07 18:34:24,288 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/wordpiece_mask (Shape: 1 x 346)
tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')
2023-11-07 18:34:24,289 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/segment_concat_mask (Shape: 1 x 346)
tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')
2023-11-07 18:34:24,290 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/offsets (Shape: 1 x 257 x 2)
tensor([[[  0,   0],
         [  1,   1],
         [  2,   2],
         ...,
         [339, 339],
         [340, 342],
         [343, 344]]], device='cuda:0')
2023-11-07 18:34:24,291 - INFO - allennlp.training.callbacks.console_logger - batch_input/spans (Shape: 1 x 101 x 2)
tensor([[[ 55,  57],
         [ 94,  95],
         [ 95,  95],
         ...,
         [251, 252],
         [252, 252],
         [253, 255]]], device='cuda:0')
2023-11-07 18:34:24,292 - INFO - allennlp.training.callbacks.console_logger - batch_input/span_types (Shape: 1 x 101)
tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')
2023-11-07 18:34:24,293 - INFO - allennlp.training.callbacks.console_logger - batch_input/event_types (Shape: 1 x 101)
tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
2023-11-07 18:34:24,294 - INFO - allennlp.training.callbacks.console_logger - batch_input/gold_template_span_labels (Shape: 1 x 1 x 101)
tensor([[[2, 2, 2,  ..., 2, 2, 2]]], device='cuda:0')
batch_loss: 7.4635, loss: 7.4635 ||:   0%|          | 1/759 [00:00<11:58,  1.06it/s]batch_loss: 7.3407, loss: 7.4021 ||:   0%|          | 2/759 [00:01<05:49,  2.16it/s]batch_loss: 7.4064, loss: 7.4036 ||:   0%|          | 3/759 [00:01<05:29,  2.30it/s]batch_loss: 7.4105, loss: 7.4053 ||:   1%|          | 4/759 [00:02<07:51,  1.60it/s]batch_loss: 7.1689, loss: 7.3580 ||:   1%|          | 5/759 [00:02<07:03,  1.78it/s]batch_loss: 7.7436, loss: 7.4223 ||:   1%|          | 6/759 [00:03<06:06,  2.06it/s]batch_loss: 7.2667, loss: 7.4001 ||:   1%|          | 7/759 [00:03<05:28,  2.29it/s]batch_loss: 7.2411, loss: 7.3802 ||:   1%|1         | 8/759 [00:03<04:21,  2.87it/s]batch_loss: 7.5167, loss: 7.3953 ||:   1%|1         | 9/759 [00:04<04:17,  2.91it/s]batch_loss: 7.3034, loss: 7.3862 ||:   1%|1         | 10/759 [00:04<04:09,  3.01it/s]batch_loss: 7.4605, loss: 7.3929 ||:   1%|1         | 11/759 [00:05<05:33,  2.24it/s]batch_loss: 7.2912, loss: 7.3844 ||:   2%|1         | 12/759 [00:05<05:11,  2.40it/s]batch_loss: 7.2262, loss: 7.3723 ||:   2%|1         | 13/759 [00:05<05:40,  2.19it/s]batch_loss: 7.4624, loss: 7.3787 ||:   2%|1         | 14/759 [00:06<05:10,  2.40it/s]batch_loss: 7.5563, loss: 7.3905 ||:   2%|1         | 15/759 [00:06<04:51,  2.55it/s]batch_loss: 7.4607, loss: 7.3949 ||:   2%|2         | 16/759 [00:06<03:59,  3.10it/s]batch_loss: 7.4276, loss: 7.3968 ||:   2%|2         | 17/759 [00:07<05:17,  2.34it/s]batch_loss: 7.3360, loss: 7.3935 ||:   2%|2         | 18/759 [00:07<04:17,  2.88it/s]batch_loss: 7.2931, loss: 7.3882 ||:   3%|2         | 19/759 [00:07<04:21,  2.83it/s]batch_loss: 7.4359, loss: 7.3932 ||:   3%|2         | 21/759 [00:08<03:55,  3.13it/s]batch_loss: 7.5003, loss: 7.3981 ||:   3%|2         | 22/759 [00:08<03:27,  3.56it/s]batch_loss: 7.3628, loss: 7.3966 ||:   3%|3         | 23/759 [00:08<02:55,  4.19it/s]batch_loss: 7.6056, loss: 7.4053 ||:   3%|3         | 24/759 [00:08<02:33,  4.80it/s]batch_loss: 7.3761, loss: 7.4041 ||:   3%|3         | 25/759 [00:09<03:41,  3.31it/s]batch_loss: 7.3779, loss: 7.4031 ||:   3%|3         | 26/759 [00:09<03:12,  3.81it/s]batch_loss: 7.3877, loss: 7.4025 ||:   4%|3         | 27/759 [00:10<03:58,  3.06it/s]batch_loss: 7.5035, loss: 7.4061 ||:   4%|3         | 28/759 [00:10<03:52,  3.14it/s]batch_loss: 7.4467, loss: 7.4075 ||:   4%|3         | 29/759 [00:10<03:19,  3.66it/s]batch_loss: 7.2878, loss: 7.4035 ||:   4%|3         | 30/759 [00:10<02:47,  4.35it/s]batch_loss: 7.5004, loss: 7.4067 ||:   4%|4         | 31/759 [00:10<02:23,  5.08it/s]batch_loss: 7.3354, loss: 7.4044 ||:   4%|4         | 32/759 [00:11<02:56,  4.11it/s]batch_loss: 7.5374, loss: 7.4085 ||:   4%|4         | 33/759 [00:12<07:07,  1.70it/s]batch_loss: 7.4672, loss: 7.4102 ||:   4%|4         | 34/759 [00:12<06:10,  1.96it/s]batch_loss: 7.2298, loss: 7.4050 ||:   5%|4         | 35/759 [00:13<04:53,  2.47it/s]batch_loss: 7.3267, loss: 7.4029 ||:   5%|4         | 36/759 [00:13<03:50,  3.14it/s]batch_loss: 7.5521, loss: 7.4069 ||:   5%|4         | 37/759 [00:13<04:35,  2.62it/s]batch_loss: 7.5679, loss: 7.4111 ||:   5%|5         | 38/759 [00:14<04:31,  2.65it/s]batch_loss: 7.4636, loss: 7.4125 ||:   5%|5         | 39/759 [00:14<03:51,  3.11it/s]batch_loss: 7.3863, loss: 7.4118 ||:   5%|5         | 40/759 [00:14<03:53,  3.08it/s]batch_loss: 7.2621, loss: 7.4082 ||:   5%|5         | 41/759 [00:14<03:12,  3.72it/s]batch_loss: 7.4155, loss: 7.4083 ||:   6%|5         | 42/759 [00:14<02:42,  4.40it/s]batch_loss: 7.4238, loss: 7.4087 ||:   6%|5         | 43/759 [00:14<02:16,  5.23it/s]batch_loss: 7.4833, loss: 7.4104 ||:   6%|5         | 44/759 [00:15<02:51,  4.16it/s]batch_loss: 7.4211, loss: 7.4116 ||:   6%|6         | 46/759 [00:15<02:42,  4.39it/s]batch_loss: 7.4319, loss: 7.4120 ||:   6%|6         | 47/759 [00:15<02:22,  4.99it/s]batch_loss: 7.4885, loss: 7.4136 ||:   6%|6         | 48/759 [00:16<02:20,  5.08it/s]batch_loss: 7.4230, loss: 7.4138 ||:   6%|6         | 49/759 [00:16<02:11,  5.38it/s]batch_loss: 7.2531, loss: 7.4106 ||:   7%|6         | 50/759 [00:16<02:13,  5.33it/s]batch_loss: 7.2130, loss: 7.4067 ||:   7%|6         | 51/759 [00:16<02:01,  5.81it/s]batch_loss: 7.2842, loss: 7.4043 ||:   7%|6         | 52/759 [00:16<02:40,  4.41it/s]batch_loss: 7.4693, loss: 7.4056 ||:   7%|6         | 53/759 [00:17<03:32,  3.32it/s]batch_loss: 7.4642, loss: 7.4066 ||:   7%|7         | 54/759 [00:17<03:02,  3.86it/s]batch_loss: 7.3297, loss: 7.4052 ||:   7%|7         | 55/759 [00:17<03:21,  3.50it/s]batch_loss: 7.2762, loss: 7.4029 ||:   7%|7         | 56/759 [00:18<04:03,  2.89it/s]batch_loss: 7.3129, loss: 7.4014 ||:   8%|7         | 57/759 [00:19<05:24,  2.17it/s]batch_loss: 7.2034, loss: 7.3979 ||:   8%|7         | 58/759 [00:19<04:52,  2.40it/s]batch_loss: 7.3328, loss: 7.3968 ||:   8%|7         | 59/759 [00:19<03:45,  3.10it/s]batch_loss: 7.3964, loss: 7.3968 ||:   8%|7         | 60/759 [00:19<03:02,  3.82it/s]batch_loss: 7.2532, loss: 7.3986 ||:   8%|8         | 62/759 [00:19<02:08,  5.41it/s]batch_loss: 7.3117, loss: 7.3972 ||:   8%|8         | 63/759 [00:20<02:40,  4.34it/s]batch_loss: 7.3136, loss: 7.3959 ||:   8%|8         | 64/759 [00:20<02:37,  4.40it/s]batch_loss: 7.3880, loss: 7.3958 ||:   9%|8         | 65/759 [00:21<04:07,  2.80it/s]batch_loss: 7.3726, loss: 7.3955 ||:   9%|8         | 66/759 [00:21<03:27,  3.33it/s]batch_loss: 7.2250, loss: 7.3929 ||:   9%|8         | 67/759 [00:21<03:41,  3.12it/s]batch_loss: 7.3239, loss: 7.3919 ||:   9%|8         | 68/759 [00:22<04:14,  2.72it/s]batch_loss: 7.1832, loss: 7.3889 ||:   9%|9         | 69/759 [00:22<03:21,  3.42it/s]batch_loss: 7.2058, loss: 7.3863 ||:   9%|9         | 70/759 [00:22<03:01,  3.80it/s]batch_loss: 7.2418, loss: 7.3842 ||:   9%|9         | 71/759 [00:22<02:40,  4.27it/s]batch_loss: 7.3335, loss: 7.3835 ||:   9%|9         | 72/759 [00:23<03:40,  3.12it/s]batch_loss: 7.3305, loss: 7.3828 ||:  10%|9         | 73/759 [00:23<03:48,  3.01it/s]batch_loss: 7.2721, loss: 7.3813 ||:  10%|9         | 74/759 [00:23<03:50,  2.97it/s]batch_loss: 7.2362, loss: 7.3794 ||:  10%|9         | 75/759 [00:23<03:04,  3.70it/s]batch_loss: 7.3210, loss: 7.3786 ||:  10%|#         | 76/759 [00:24<03:18,  3.44it/s]batch_loss: 7.0366, loss: 7.3742 ||:  10%|#         | 77/759 [00:24<02:45,  4.12it/s]batch_loss: 7.2149, loss: 7.3721 ||:  10%|#         | 78/759 [00:24<03:05,  3.67it/s]batch_loss: 7.4497, loss: 7.3731 ||:  10%|#         | 79/759 [00:25<03:17,  3.45it/s]batch_loss: 7.4646, loss: 7.3742 ||:  11%|#         | 80/759 [00:25<02:52,  3.94it/s]batch_loss: 7.2960, loss: 7.3733 ||:  11%|#         | 81/759 [00:25<02:37,  4.31it/s]batch_loss: 7.3751, loss: 7.3733 ||:  11%|#         | 82/759 [00:25<02:11,  5.16it/s]batch_loss: 7.3418, loss: 7.3729 ||:  11%|#         | 83/759 [00:25<02:06,  5.33it/s]batch_loss: 7.1716, loss: 7.3705 ||:  11%|#1        | 84/759 [00:26<03:13,  3.49it/s]batch_loss: 7.4124, loss: 7.3710 ||:  11%|#1        | 85/759 [00:26<04:01,  2.79it/s]batch_loss: 7.1607, loss: 7.3686 ||:  11%|#1        | 86/759 [00:27<03:59,  2.81it/s]batch_loss: 7.3658, loss: 7.3685 ||:  11%|#1        | 87/759 [00:28<06:18,  1.77it/s]batch_loss: 7.3324, loss: 7.3681 ||:  12%|#1        | 88/759 [00:28<05:02,  2.22it/s]batch_loss: 7.4199, loss: 7.3687 ||:  12%|#1        | 89/759 [00:28<04:05,  2.73it/s]batch_loss: 7.1740, loss: 7.3666 ||:  12%|#1        | 90/759 [00:28<03:55,  2.84it/s]batch_loss: 7.1329, loss: 7.3640 ||:  12%|#1        | 91/759 [00:29<03:25,  3.25it/s]batch_loss: 7.1282, loss: 7.3614 ||:  12%|#2        | 92/759 [00:29<03:28,  3.20it/s]batch_loss: 7.2958, loss: 7.3607 ||:  12%|#2        | 93/759 [00:30<06:33,  1.69it/s]batch_loss: 7.2278, loss: 7.3593 ||:  12%|#2        | 94/759 [00:30<05:41,  1.95it/s]batch_loss: 7.2284, loss: 7.3579 ||:  13%|#2        | 95/759 [00:31<06:46,  1.63it/s]batch_loss: 7.2554, loss: 7.3569 ||:  13%|#2        | 96/759 [00:32<06:05,  1.82it/s]batch_loss: 7.2669, loss: 7.3559 ||:  13%|#2        | 97/759 [00:32<05:30,  2.00it/s]batch_loss: 7.1790, loss: 7.3541 ||:  13%|#2        | 98/759 [00:32<05:02,  2.19it/s]batch_loss: 7.3800, loss: 7.3544 ||:  13%|#3        | 99/759 [00:33<06:39,  1.65it/s]batch_loss: 7.1469, loss: 7.3523 ||:  13%|#3        | 100/759 [00:33<05:01,  2.19it/s]batch_loss: 7.2303, loss: 7.3511 ||:  13%|#3        | 101/759 [00:34<04:07,  2.66it/s]batch_loss: 7.1830, loss: 7.3495 ||:  13%|#3        | 102/759 [00:34<03:36,  3.03it/s]batch_loss: 7.1907, loss: 7.3479 ||:  14%|#3        | 103/759 [00:34<02:52,  3.81it/s]batch_loss: 7.2816, loss: 7.3473 ||:  14%|#3        | 104/759 [00:34<02:39,  4.11it/s]batch_loss: 7.2530, loss: 7.3464 ||:  14%|#3        | 105/759 [00:34<02:30,  4.35it/s]batch_loss: 7.2244, loss: 7.3452 ||:  14%|#3        | 106/759 [00:35<02:09,  5.06it/s]batch_loss: 7.1894, loss: 7.3438 ||:  14%|#4        | 107/759 [00:35<03:55,  2.77it/s]batch_loss: 7.1193, loss: 7.3417 ||:  14%|#4        | 108/759 [00:35<03:15,  3.32it/s]batch_loss: 7.2572, loss: 7.3409 ||:  14%|#4        | 109/759 [00:36<02:45,  3.93it/s]batch_loss: 7.0523, loss: 7.3383 ||:  14%|#4        | 110/759 [00:36<03:01,  3.57it/s]batch_loss: 7.3169, loss: 7.3381 ||:  15%|#4        | 111/759 [00:36<03:09,  3.42it/s]batch_loss: 7.0872, loss: 7.3359 ||:  15%|#4        | 112/759 [00:36<02:50,  3.79it/s]batch_loss: 7.2654, loss: 7.3352 ||:  15%|#4        | 113/759 [00:37<04:50,  2.22it/s]batch_loss: 7.0229, loss: 7.3325 ||:  15%|#5        | 114/759 [00:38<04:36,  2.33it/s]batch_loss: 7.1163, loss: 7.3306 ||:  15%|#5        | 115/759 [00:38<03:47,  2.83it/s]batch_loss: 7.0670, loss: 7.3283 ||:  15%|#5        | 116/759 [00:38<03:20,  3.21it/s]batch_loss: 7.2028, loss: 7.3273 ||:  15%|#5        | 117/759 [00:39<03:58,  2.70it/s]batch_loss: 7.2495, loss: 7.3257 ||:  16%|#5        | 119/759 [00:39<03:07,  3.41it/s]batch_loss: 6.9767, loss: 7.3228 ||:  16%|#5        | 120/759 [00:39<03:10,  3.36it/s]batch_loss: 6.9980, loss: 7.3201 ||:  16%|#5        | 121/759 [00:40<03:14,  3.28it/s]batch_loss: 7.0044, loss: 7.3151 ||:  16%|#6        | 123/759 [00:40<02:41,  3.94it/s]batch_loss: 7.0141, loss: 7.3118 ||:  16%|#6        | 125/759 [00:40<02:05,  5.05it/s]batch_loss: 7.1013, loss: 7.3101 ||:  17%|#6        | 126/759 [00:40<01:55,  5.48it/s]batch_loss: 7.1666, loss: 7.3090 ||:  17%|#6        | 127/759 [00:40<01:43,  6.11it/s]batch_loss: 7.0805, loss: 7.3072 ||:  17%|#6        | 128/759 [00:41<02:06,  4.99it/s]batch_loss: 6.9354, loss: 7.3017 ||:  17%|#7        | 130/759 [00:41<02:02,  5.12it/s]batch_loss: 7.0885, loss: 7.3000 ||:  17%|#7        | 131/759 [00:41<02:23,  4.37it/s]batch_loss: 7.0260, loss: 7.2980 ||:  17%|#7        | 132/759 [00:42<02:36,  4.00it/s]batch_loss: 6.9482, loss: 7.2953 ||:  18%|#7        | 133/759 [00:42<02:19,  4.48it/s]batch_loss: 7.0101, loss: 7.2932 ||:  18%|#7        | 134/759 [00:42<03:01,  3.45it/s]batch_loss: 6.9494, loss: 7.2907 ||:  18%|#7        | 135/759 [00:43<03:04,  3.38it/s]batch_loss: 6.8941, loss: 7.2877 ||:  18%|#7        | 136/759 [00:43<03:10,  3.28it/s]batch_loss: 6.9434, loss: 7.2852 ||:  18%|#8        | 137/759 [00:43<02:44,  3.77it/s]batch_loss: 7.1579, loss: 7.2843 ||:  18%|#8        | 138/759 [00:43<02:25,  4.26it/s]batch_loss: 6.8437, loss: 7.2811 ||:  18%|#8        | 139/759 [00:44<02:41,  3.84it/s]batch_loss: 6.9673, loss: 7.2789 ||:  18%|#8        | 140/759 [00:44<02:54,  3.55it/s]batch_loss: 6.9680, loss: 7.2767 ||:  19%|#8        | 141/759 [00:45<03:35,  2.87it/s]batch_loss: 7.0233, loss: 7.2749 ||:  19%|#8        | 142/759 [00:45<04:14,  2.42it/s]batch_loss: 7.0162, loss: 7.2731 ||:  19%|#8        | 143/759 [00:45<03:21,  3.05it/s]batch_loss: 7.0587, loss: 7.2716 ||:  19%|#8        | 144/759 [00:46<03:57,  2.59it/s]batch_loss: 7.1283, loss: 7.2706 ||:  19%|#9        | 145/759 [00:46<03:49,  2.67it/s]batch_loss: 7.0211, loss: 7.2689 ||:  19%|#9        | 146/759 [00:46<03:01,  3.38it/s]batch_loss: 6.9964, loss: 7.2671 ||:  19%|#9        | 147/759 [00:46<02:38,  3.85it/s]batch_loss: 6.9338, loss: 7.2648 ||:  19%|#9        | 148/759 [00:47<02:50,  3.59it/s]batch_loss: 6.7399, loss: 7.2613 ||:  20%|#9        | 149/759 [00:47<02:57,  3.43it/s]batch_loss: 6.9829, loss: 7.2594 ||:  20%|#9        | 150/759 [00:48<04:16,  2.37it/s]batch_loss: 6.7731, loss: 7.2562 ||:  20%|#9        | 151/759 [00:48<03:32,  2.86it/s]batch_loss: 6.8197, loss: 7.2533 ||:  20%|##        | 152/759 [00:48<02:52,  3.52it/s]batch_loss: 6.9384, loss: 7.2513 ||:  20%|##        | 153/759 [00:49<03:37,  2.79it/s]batch_loss: 6.8223, loss: 7.2485 ||:  20%|##        | 154/759 [00:49<03:33,  2.84it/s]batch_loss: 6.8267, loss: 7.2438 ||:  21%|##        | 156/759 [00:49<02:19,  4.32it/s]batch_loss: 6.8944, loss: 7.2415 ||:  21%|##        | 157/759 [00:49<02:35,  3.88it/s]batch_loss: 6.7786, loss: 7.2386 ||:  21%|##        | 158/759 [00:50<03:09,  3.18it/s]batch_loss: 6.6569, loss: 7.2349 ||:  21%|##        | 159/759 [00:50<02:40,  3.74it/s]batch_loss: 6.7078, loss: 7.2290 ||:  21%|##1       | 161/759 [00:50<02:11,  4.56it/s]batch_loss: 6.8087, loss: 7.2264 ||:  21%|##1       | 162/759 [00:51<02:29,  3.99it/s]batch_loss: 7.0253, loss: 7.2251 ||:  21%|##1       | 163/759 [00:51<03:31,  2.82it/s]batch_loss: 6.9370, loss: 7.2234 ||:  22%|##1       | 164/759 [00:52<03:25,  2.90it/s]batch_loss: 6.7469, loss: 7.2205 ||:  22%|##1       | 165/759 [00:52<03:22,  2.93it/s]batch_loss: 6.9171, loss: 7.2187 ||:  22%|##1       | 166/759 [00:53<03:49,  2.58it/s]batch_loss: 6.9962, loss: 7.2173 ||:  22%|##2       | 167/759 [00:53<03:38,  2.71it/s]batch_loss: 6.7430, loss: 7.2145 ||:  22%|##2       | 168/759 [00:53<02:54,  3.39it/s]batch_loss: 6.7891, loss: 7.2120 ||:  22%|##2       | 169/759 [00:53<02:22,  4.15it/s]batch_loss: 6.7591, loss: 7.2093 ||:  22%|##2       | 170/759 [00:53<02:34,  3.82it/s]batch_loss: 6.7541, loss: 7.2067 ||:  23%|##2       | 171/759 [00:54<02:08,  4.56it/s]batch_loss: 6.6550, loss: 7.2035 ||:  23%|##2       | 172/759 [00:54<02:25,  4.04it/s]batch_loss: 6.7516, loss: 7.2009 ||:  23%|##2       | 173/759 [00:54<03:09,  3.10it/s]batch_loss: 6.8367, loss: 7.1988 ||:  23%|##2       | 174/759 [00:55<03:13,  3.02it/s]batch_loss: 6.8207, loss: 7.1955 ||:  23%|##3       | 176/759 [00:55<02:09,  4.51it/s]batch_loss: 6.5370, loss: 7.1918 ||:  23%|##3       | 177/759 [00:55<02:22,  4.07it/s]batch_loss: 7.0223, loss: 7.1909 ||:  23%|##3       | 178/759 [00:57<05:13,  1.86it/s]batch_loss: 6.6361, loss: 7.1878 ||:  24%|##3       | 179/759 [00:57<04:13,  2.29it/s]batch_loss: 6.7530, loss: 7.1853 ||:  24%|##3       | 180/759 [00:57<04:20,  2.22it/s]batch_loss: 6.6203, loss: 7.1822 ||:  24%|##3       | 181/759 [00:58<04:28,  2.15it/s]batch_loss: 6.7452, loss: 7.1798 ||:  24%|##3       | 182/759 [00:58<04:01,  2.39it/s]batch_loss: 6.9028, loss: 7.1783 ||:  24%|##4       | 183/759 [00:59<05:55,  1.62it/s]batch_loss: 6.6430, loss: 7.1739 ||:  24%|##4       | 185/759 [00:59<03:37,  2.63it/s]batch_loss: 6.5953, loss: 7.1680 ||:  25%|##4       | 187/759 [01:00<02:54,  3.27it/s]batch_loss: 6.7183, loss: 7.1656 ||:  25%|##4       | 188/759 [01:00<02:54,  3.26it/s]batch_loss: 6.7462, loss: 7.1634 ||:  25%|##4       | 189/759 [01:01<03:52,  2.45it/s]batch_loss: 6.5795, loss: 7.1603 ||:  25%|##5       | 190/759 [01:01<03:21,  2.83it/s]batch_loss: 6.5643, loss: 7.1572 ||:  25%|##5       | 191/759 [01:01<02:51,  3.32it/s]batch_loss: 6.6559, loss: 7.1546 ||:  25%|##5       | 192/759 [01:01<02:28,  3.81it/s]batch_loss: 6.6239, loss: 7.1519 ||:  25%|##5       | 193/759 [01:01<02:07,  4.44it/s]batch_loss: 6.6593, loss: 7.1493 ||:  26%|##5       | 194/759 [01:02<03:22,  2.79it/s]batch_loss: 6.5690, loss: 7.1444 ||:  26%|##5       | 196/759 [01:02<02:42,  3.47it/s]batch_loss: 6.3929, loss: 7.1406 ||:  26%|##5       | 197/759 [01:03<02:45,  3.40it/s]batch_loss: 6.8005, loss: 7.1389 ||:  26%|##6       | 198/759 [01:04<04:06,  2.27it/s]batch_loss: 6.3287, loss: 7.1348 ||:  26%|##6       | 199/759 [01:04<03:25,  2.72it/s]batch_loss: 6.4073, loss: 7.1312 ||:  26%|##6       | 200/759 [01:04<02:47,  3.33it/s]batch_loss: 6.4755, loss: 7.1264 ||:  27%|##6       | 202/759 [01:04<02:21,  3.94it/s]batch_loss: 6.4404, loss: 7.1230 ||:  27%|##6       | 203/759 [01:04<02:11,  4.24it/s]batch_loss: 6.5681, loss: 7.1178 ||:  27%|##7       | 205/759 [01:05<01:43,  5.34it/s]batch_loss: 6.3203, loss: 7.1140 ||:  27%|##7       | 206/759 [01:05<01:46,  5.21it/s]batch_loss: 6.4917, loss: 7.1110 ||:  27%|##7       | 207/759 [01:05<01:50,  4.99it/s]batch_loss: 6.4465, loss: 7.1078 ||:  27%|##7       | 208/759 [01:06<02:13,  4.12it/s]batch_loss: 6.2337, loss: 7.1036 ||:  28%|##7       | 209/759 [01:06<02:24,  3.80it/s]batch_loss: 6.7991, loss: 7.1021 ||:  28%|##7       | 210/759 [01:07<03:57,  2.31it/s]batch_loss: 6.5489, loss: 7.0968 ||:  28%|##7       | 212/759 [01:07<02:37,  3.47it/s]batch_loss: 6.5220, loss: 7.0941 ||:  28%|##8       | 213/759 [01:07<03:02,  2.98it/s]batch_loss: 6.3447, loss: 7.0906 ||:  28%|##8       | 214/759 [01:08<02:39,  3.41it/s]batch_loss: 6.2692, loss: 7.0868 ||:  28%|##8       | 215/759 [01:08<02:45,  3.28it/s]batch_loss: 6.2306, loss: 7.0800 ||:  29%|##8       | 217/759 [01:08<02:18,  3.92it/s]batch_loss: 6.2128, loss: 7.0738 ||:  29%|##8       | 219/759 [01:08<01:43,  5.19it/s]batch_loss: 6.2314, loss: 7.0700 ||:  29%|##8       | 220/759 [01:09<01:38,  5.47it/s]batch_loss: 6.3669, loss: 7.0668 ||:  29%|##9       | 221/759 [01:09<02:17,  3.92it/s]batch_loss: 6.1660, loss: 7.0628 ||:  29%|##9       | 222/759 [01:09<02:06,  4.25it/s]batch_loss: 6.3487, loss: 7.0596 ||:  29%|##9       | 223/759 [01:10<02:27,  3.63it/s]batch_loss: 6.3199, loss: 7.0563 ||:  30%|##9       | 224/759 [01:10<02:36,  3.42it/s]batch_loss: 6.3494, loss: 7.0531 ||:  30%|##9       | 225/759 [01:10<02:41,  3.30it/s]batch_loss: 6.2332, loss: 7.0495 ||:  30%|##9       | 226/759 [01:11<02:19,  3.83it/s]batch_loss: 6.2044, loss: 7.0458 ||:  30%|##9       | 227/759 [01:11<02:32,  3.48it/s]batch_loss: 6.4111, loss: 7.0430 ||:  30%|###       | 228/759 [01:12<03:36,  2.45it/s]batch_loss: 6.5386, loss: 7.0408 ||:  30%|###       | 229/759 [01:13<05:23,  1.64it/s]batch_loss: 6.3298, loss: 7.0377 ||:  30%|###       | 230/759 [01:13<04:39,  1.89it/s]batch_loss: 6.0195, loss: 7.0333 ||:  30%|###       | 231/759 [01:13<03:33,  2.48it/s]batch_loss: 6.1343, loss: 7.0294 ||:  31%|###       | 232/759 [01:13<02:49,  3.11it/s]batch_loss: 6.2143, loss: 7.0259 ||:  31%|###       | 233/759 [01:14<02:55,  2.99it/s]batch_loss: 6.0712, loss: 7.0176 ||:  31%|###       | 235/759 [01:14<02:07,  4.10it/s]batch_loss: 6.4170, loss: 7.0150 ||:  31%|###1      | 236/759 [01:14<01:56,  4.50it/s]batch_loss: 6.1803, loss: 7.0115 ||:  31%|###1      | 237/759 [01:14<02:15,  3.86it/s]batch_loss: 6.0680, loss: 7.0075 ||:  31%|###1      | 238/759 [01:15<02:04,  4.19it/s]batch_loss: 6.3104, loss: 7.0046 ||:  31%|###1      | 239/759 [01:15<02:51,  3.03it/s]batch_loss: 6.1448, loss: 7.0010 ||:  32%|###1      | 240/759 [01:15<02:24,  3.59it/s]batch_loss: 6.0646, loss: 6.9972 ||:  32%|###1      | 241/759 [01:16<02:38,  3.26it/s]batch_loss: 5.9332, loss: 6.9928 ||:  32%|###1      | 242/759 [01:16<02:12,  3.91it/s]batch_loss: 6.0882, loss: 6.9890 ||:  32%|###2      | 243/759 [01:16<02:23,  3.61it/s]batch_loss: 6.0660, loss: 6.9852 ||:  32%|###2      | 244/759 [01:17<03:25,  2.50it/s]batch_loss: 5.8148, loss: 6.9805 ||:  32%|###2      | 245/759 [01:17<02:48,  3.05it/s]batch_loss: 6.4463, loss: 6.9783 ||:  32%|###2      | 246/759 [01:18<05:21,  1.60it/s]batch_loss: 5.8463, loss: 6.9700 ||:  33%|###2      | 248/759 [01:19<03:20,  2.55it/s]batch_loss: 6.0361, loss: 6.9663 ||:  33%|###2      | 249/759 [01:19<03:14,  2.63it/s]batch_loss: 5.9019, loss: 6.9620 ||:  33%|###2      | 250/759 [01:19<03:08,  2.70it/s]batch_loss: 6.0487, loss: 6.9584 ||:  33%|###3      | 251/759 [01:20<03:23,  2.50it/s]batch_loss: 6.0288, loss: 6.9547 ||:  33%|###3      | 252/759 [01:20<04:01,  2.10it/s]batch_loss: 6.2396, loss: 6.9487 ||:  33%|###3      | 254/759 [01:20<02:27,  3.43it/s]batch_loss: 6.2410, loss: 6.9459 ||:  34%|###3      | 255/759 [01:21<03:40,  2.28it/s]batch_loss: 6.1438, loss: 6.9428 ||:  34%|###3      | 256/759 [01:22<03:47,  2.21it/s]batch_loss: 6.1051, loss: 6.9395 ||:  34%|###3      | 257/759 [01:22<03:01,  2.77it/s]batch_loss: 5.6874, loss: 6.9347 ||:  34%|###3      | 258/759 [01:22<02:29,  3.36it/s]batch_loss: 6.0627, loss: 6.9313 ||:  34%|###4      | 259/759 [01:23<03:27,  2.41it/s]batch_loss: 5.7249, loss: 6.9267 ||:  34%|###4      | 260/759 [01:23<02:48,  2.96it/s]batch_loss: 5.7356, loss: 6.9221 ||:  34%|###4      | 261/759 [01:23<02:43,  3.04it/s]batch_loss: 5.8055, loss: 6.9178 ||:  35%|###4      | 262/759 [01:23<02:11,  3.77it/s]batch_loss: 5.7086, loss: 6.9132 ||:  35%|###4      | 263/759 [01:24<02:19,  3.55it/s]batch_loss: 5.8728, loss: 6.9093 ||:  35%|###4      | 264/759 [01:24<02:30,  3.30it/s]batch_loss: 5.8427, loss: 6.9053 ||:  35%|###4      | 265/759 [01:24<02:16,  3.63it/s]batch_loss: 5.4187, loss: 6.8997 ||:  35%|###5      | 266/759 [01:25<02:16,  3.60it/s]batch_loss: 5.5697, loss: 6.8947 ||:  35%|###5      | 267/759 [01:25<02:22,  3.45it/s]batch_loss: 5.5087, loss: 6.8895 ||:  35%|###5      | 268/759 [01:25<02:30,  3.27it/s]batch_loss: 5.6689, loss: 6.8850 ||:  35%|###5      | 269/759 [01:25<02:02,  3.99it/s]batch_loss: 5.5310, loss: 6.8800 ||:  36%|###5      | 270/759 [01:26<01:49,  4.49it/s]batch_loss: 5.3929, loss: 6.8745 ||:  36%|###5      | 271/759 [01:26<01:44,  4.65it/s]batch_loss: 5.9294, loss: 6.8710 ||:  36%|###5      | 272/759 [01:26<03:00,  2.69it/s]batch_loss: 5.8134, loss: 6.8671 ||:  36%|###5      | 273/759 [01:27<03:00,  2.69it/s]batch_loss: 5.9906, loss: 6.8639 ||:  36%|###6      | 274/759 [01:27<03:26,  2.35it/s]batch_loss: 5.4534, loss: 6.8588 ||:  36%|###6      | 275/759 [01:28<03:09,  2.55it/s]batch_loss: 5.4662, loss: 6.8538 ||:  36%|###6      | 276/759 [01:28<03:01,  2.66it/s]batch_loss: 5.5774, loss: 6.8492 ||:  36%|###6      | 277/759 [01:28<02:28,  3.24it/s]batch_loss: 5.7002, loss: 6.8450 ||:  37%|###6      | 278/759 [01:28<01:59,  4.03it/s]batch_loss: 5.5777, loss: 6.8405 ||:  37%|###6      | 279/759 [01:29<02:10,  3.69it/s]batch_loss: 5.2379, loss: 6.8348 ||:  37%|###6      | 280/759 [01:29<01:55,  4.16it/s]batch_loss: 5.5501, loss: 6.8302 ||:  37%|###7      | 281/759 [01:29<02:28,  3.21it/s]batch_loss: 5.3140, loss: 6.8248 ||:  37%|###7      | 282/759 [01:30<02:28,  3.22it/s]batch_loss: 5.3155, loss: 6.8195 ||:  37%|###7      | 283/759 [01:30<02:27,  3.23it/s]batch_loss: 5.3490, loss: 6.8143 ||:  37%|###7      | 284/759 [01:30<02:09,  3.67it/s]batch_loss: 5.2813, loss: 6.8047 ||:  38%|###7      | 286/759 [01:30<01:57,  4.04it/s]batch_loss: 5.4141, loss: 6.7999 ||:  38%|###7      | 287/759 [01:31<02:06,  3.73it/s]batch_loss: 5.3768, loss: 6.7950 ||:  38%|###7      | 288/759 [01:31<02:11,  3.57it/s]batch_loss: 5.3097, loss: 6.7844 ||:  38%|###8      | 290/759 [01:32<02:01,  3.86it/s]batch_loss: 5.1592, loss: 6.7788 ||:  38%|###8      | 291/759 [01:32<02:06,  3.69it/s]batch_loss: 5.2677, loss: 6.7736 ||:  38%|###8      | 292/759 [01:32<02:30,  3.10it/s]batch_loss: 5.2427, loss: 6.7684 ||:  39%|###8      | 293/759 [01:33<02:32,  3.05it/s]batch_loss: 5.1663, loss: 6.7629 ||:  39%|###8      | 294/759 [01:33<02:11,  3.55it/s]batch_loss: 5.3556, loss: 6.7535 ||:  39%|###8      | 296/759 [01:33<01:53,  4.07it/s]batch_loss: 5.2564, loss: 6.7485 ||:  39%|###9      | 297/759 [01:33<01:37,  4.73it/s]batch_loss: 5.4472, loss: 6.7441 ||:  39%|###9      | 298/759 [01:34<02:10,  3.54it/s]batch_loss: 4.9460, loss: 6.7381 ||:  39%|###9      | 299/759 [01:34<01:52,  4.07it/s]batch_loss: 5.1894, loss: 6.7329 ||:  40%|###9      | 300/759 [01:34<02:03,  3.73it/s]batch_loss: 5.0486, loss: 6.7273 ||:  40%|###9      | 301/759 [01:34<01:45,  4.33it/s]batch_loss: 5.6219, loss: 6.7237 ||:  40%|###9      | 302/759 [01:35<03:08,  2.42it/s]batch_loss: 5.0256, loss: 6.7181 ||:  40%|###9      | 303/759 [01:35<02:30,  3.04it/s]batch_loss: 5.0795, loss: 6.7127 ||:  40%|####      | 304/759 [01:36<02:26,  3.10it/s]batch_loss: 5.0615, loss: 6.7073 ||:  40%|####      | 305/759 [01:36<02:03,  3.68it/s]batch_loss: 5.0856, loss: 6.7020 ||:  40%|####      | 306/759 [01:36<02:10,  3.46it/s]batch_loss: 5.1328, loss: 6.6969 ||:  40%|####      | 307/759 [01:37<02:37,  2.87it/s]batch_loss: 4.7858, loss: 6.6907 ||:  41%|####      | 308/759 [01:37<02:33,  2.95it/s]batch_loss: 5.0634, loss: 6.6854 ||:  41%|####      | 309/759 [01:37<02:05,  3.58it/s]batch_loss: 5.1306, loss: 6.6804 ||:  41%|####      | 310/759 [01:38<02:56,  2.55it/s]batch_loss: 4.9383, loss: 6.6748 ||:  41%|####      | 311/759 [01:38<02:50,  2.63it/s]batch_loss: 4.7341, loss: 6.6631 ||:  41%|####1     | 313/759 [01:39<02:12,  3.36it/s]batch_loss: 5.1646, loss: 6.6583 ||:  41%|####1     | 314/759 [01:39<02:38,  2.80it/s]batch_loss: 4.8108, loss: 6.6525 ||:  42%|####1     | 315/759 [01:40<02:37,  2.82it/s]batch_loss: 4.5794, loss: 6.6459 ||:  42%|####1     | 316/759 [01:40<02:31,  2.93it/s]batch_loss: 5.0692, loss: 6.6409 ||:  42%|####1     | 317/759 [01:40<02:49,  2.60it/s]batch_loss: 5.1321, loss: 6.6362 ||:  42%|####1     | 318/759 [01:41<03:34,  2.06it/s]batch_loss: 4.6729, loss: 6.6245 ||:  42%|####2     | 320/759 [01:41<02:22,  3.08it/s]batch_loss: 4.6722, loss: 6.6184 ||:  42%|####2     | 321/759 [01:42<02:23,  3.06it/s]batch_loss: 4.9800, loss: 6.6133 ||:  42%|####2     | 322/759 [01:42<03:13,  2.26it/s]batch_loss: 4.7224, loss: 6.6075 ||:  43%|####2     | 323/759 [01:43<02:34,  2.81it/s]batch_loss: 4.9187, loss: 6.6023 ||:  43%|####2     | 324/759 [01:43<02:48,  2.58it/s]batch_loss: 4.4821, loss: 6.5957 ||:  43%|####2     | 325/759 [01:43<02:38,  2.74it/s]batch_loss: 5.3318, loss: 6.5919 ||:  43%|####2     | 326/759 [01:44<03:58,  1.81it/s]batch_loss: 4.5269, loss: 6.5855 ||:  43%|####3     | 327/759 [01:45<03:29,  2.06it/s]batch_loss: 4.5968, loss: 6.5795 ||:  43%|####3     | 328/759 [01:45<02:41,  2.67it/s]batch_loss: 4.6419, loss: 6.5736 ||:  43%|####3     | 329/759 [01:45<02:06,  3.40it/s]batch_loss: 4.3571, loss: 6.5669 ||:  43%|####3     | 330/759 [01:45<01:46,  4.02it/s]batch_loss: 4.8712, loss: 6.5618 ||:  44%|####3     | 331/759 [01:46<02:42,  2.64it/s]batch_loss: 4.4402, loss: 6.5554 ||:  44%|####3     | 332/759 [01:46<02:15,  3.15it/s]batch_loss: 5.0590, loss: 6.5509 ||:  44%|####3     | 333/759 [01:47<03:56,  1.80it/s]batch_loss: 4.3303, loss: 6.5442 ||:  44%|####4     | 334/759 [01:47<03:25,  2.07it/s]batch_loss: 4.1574, loss: 6.5371 ||:  44%|####4     | 335/759 [01:48<03:03,  2.31it/s]batch_loss: 4.3375, loss: 6.5306 ||:  44%|####4     | 336/759 [01:48<02:49,  2.49it/s]batch_loss: 4.3001, loss: 6.5239 ||:  44%|####4     | 337/759 [01:48<02:41,  2.61it/s]batch_loss: 4.1451, loss: 6.5169 ||:  45%|####4     | 338/759 [01:49<02:38,  2.66it/s]batch_loss: 4.0399, loss: 6.5096 ||:  45%|####4     | 339/759 [01:49<02:30,  2.80it/s]batch_loss: 4.3108, loss: 6.5031 ||:  45%|####4     | 340/759 [01:49<02:26,  2.86it/s]batch_loss: 4.1743, loss: 6.4963 ||:  45%|####4     | 341/759 [01:50<02:23,  2.91it/s]batch_loss: 4.0582, loss: 6.4834 ||:  45%|####5     | 343/759 [01:50<01:54,  3.65it/s]batch_loss: 4.0544, loss: 6.4764 ||:  45%|####5     | 344/759 [01:50<01:57,  3.52it/s]batch_loss: 4.0435, loss: 6.4693 ||:  45%|####5     | 345/759 [01:50<01:37,  4.23it/s]batch_loss: 3.9197, loss: 6.4619 ||:  46%|####5     | 346/759 [01:51<01:23,  4.93it/s]batch_loss: 4.1721, loss: 6.4486 ||:  46%|####5     | 348/759 [01:51<01:23,  4.93it/s]batch_loss: 3.9701, loss: 6.4415 ||:  46%|####5     | 349/759 [01:51<01:37,  4.20it/s]batch_loss: 4.0132, loss: 6.4283 ||:  46%|####6     | 351/759 [01:51<01:14,  5.44it/s]batch_loss: 4.2992, loss: 6.4222 ||:  46%|####6     | 352/759 [01:52<02:09,  3.13it/s]batch_loss: 3.8134, loss: 6.4148 ||:  47%|####6     | 353/759 [01:53<02:10,  3.12it/s]batch_loss: 4.6609, loss: 6.4099 ||:  47%|####6     | 354/759 [01:54<03:20,  2.02it/s]batch_loss: 3.9169, loss: 6.3962 ||:  47%|####6     | 356/759 [01:54<02:29,  2.70it/s]batch_loss: 3.5195, loss: 6.3882 ||:  47%|####7     | 357/759 [01:54<02:23,  2.81it/s]batch_loss: 3.6662, loss: 6.3806 ||:  47%|####7     | 358/759 [01:55<02:18,  2.90it/s]batch_loss: 3.6396, loss: 6.3729 ||:  47%|####7     | 359/759 [01:55<01:54,  3.50it/s]batch_loss: 4.1496, loss: 6.3668 ||:  47%|####7     | 360/759 [01:55<02:19,  2.85it/s]batch_loss: 3.6731, loss: 6.3593 ||:  48%|####7     | 361/759 [01:55<01:58,  3.35it/s]batch_loss: 3.8198, loss: 6.3455 ||:  48%|####7     | 363/759 [01:56<01:56,  3.39it/s]batch_loss: 3.4267, loss: 6.3374 ||:  48%|####7     | 364/759 [01:56<02:01,  3.26it/s]batch_loss: 3.3338, loss: 6.3292 ||:  48%|####8     | 365/759 [01:57<02:01,  3.24it/s]batch_loss: 4.7926, loss: 6.3250 ||:  48%|####8     | 366/759 [01:58<03:50,  1.70it/s]batch_loss: 3.3819, loss: 6.3170 ||:  48%|####8     | 367/759 [01:58<03:19,  1.96it/s]batch_loss: 3.9430, loss: 6.3105 ||:  48%|####8     | 368/759 [01:59<03:18,  1.97it/s]batch_loss: 3.5453, loss: 6.3030 ||:  49%|####8     | 369/759 [01:59<03:06,  2.09it/s]batch_loss: 4.2203, loss: 6.2974 ||:  49%|####8     | 370/759 [02:00<03:27,  1.87it/s]batch_loss: 3.1576, loss: 6.2890 ||:  49%|####8     | 371/759 [02:00<02:46,  2.33it/s]batch_loss: 3.1371, loss: 6.2805 ||:  49%|####9     | 372/759 [02:00<02:17,  2.82it/s]batch_loss: 3.1254, loss: 6.2720 ||:  49%|####9     | 373/759 [02:00<01:56,  3.32it/s]batch_loss: 3.5364, loss: 6.2647 ||:  49%|####9     | 374/759 [02:01<02:17,  2.81it/s]batch_loss: 3.4192, loss: 6.2493 ||:  50%|####9     | 376/759 [02:01<02:04,  3.07it/s]batch_loss: 3.8040, loss: 6.2428 ||:  50%|####9     | 377/759 [02:02<02:50,  2.24it/s]batch_loss: 3.1852, loss: 6.2280 ||:  50%|####9     | 379/759 [02:03<02:11,  2.89it/s]batch_loss: 3.2237, loss: 6.2201 ||:  50%|#####     | 380/759 [02:03<02:10,  2.90it/s]batch_loss: 3.1037, loss: 6.2119 ||:  50%|#####     | 381/759 [02:03<01:51,  3.38it/s]batch_loss: 2.7422, loss: 6.1948 ||:  50%|#####     | 383/759 [02:04<01:36,  3.91it/s]batch_loss: 2.7527, loss: 6.1859 ||:  51%|#####     | 384/759 [02:04<01:28,  4.25it/s]batch_loss: 3.0233, loss: 6.1777 ||:  51%|#####     | 385/759 [02:04<01:37,  3.84it/s]batch_loss: 2.9237, loss: 6.1607 ||:  51%|#####     | 387/759 [02:05<01:41,  3.67it/s]batch_loss: 2.7977, loss: 6.1520 ||:  51%|#####1    | 388/759 [02:05<01:47,  3.45it/s]batch_loss: 2.5362, loss: 6.1427 ||:  51%|#####1    | 389/759 [02:05<01:52,  3.28it/s]batch_loss: 2.5558, loss: 6.1335 ||:  51%|#####1    | 390/759 [02:06<01:55,  3.20it/s]batch_loss: 3.9923, loss: 6.1280 ||:  52%|#####1    | 391/759 [02:07<02:46,  2.20it/s]batch_loss: 2.5464, loss: 6.1115 ||:  52%|#####1    | 393/759 [02:07<02:04,  2.94it/s]batch_loss: 2.9125, loss: 6.1033 ||:  52%|#####1    | 394/759 [02:07<02:17,  2.65it/s]batch_loss: 2.6338, loss: 6.0946 ||:  52%|#####2    | 395/759 [02:08<02:15,  2.68it/s]batch_loss: 3.6349, loss: 6.0815 ||:  52%|#####2    | 397/759 [02:09<02:28,  2.43it/s]batch_loss: 2.3517, loss: 6.0721 ||:  52%|#####2    | 398/759 [02:09<02:19,  2.58it/s]batch_loss: 2.6412, loss: 6.0565 ||:  53%|#####2    | 400/759 [02:09<01:36,  3.71it/s]batch_loss: 2.3697, loss: 6.0473 ||:  53%|#####2    | 401/759 [02:10<01:39,  3.60it/s]batch_loss: 2.0037, loss: 6.0372 ||:  53%|#####2    | 402/759 [02:10<01:42,  3.50it/s]batch_loss: 2.1332, loss: 6.0275 ||:  53%|#####3    | 403/759 [02:10<01:34,  3.79it/s]batch_loss: 2.3735, loss: 6.0185 ||:  53%|#####3    | 404/759 [02:11<01:57,  3.03it/s]batch_loss: 3.4801, loss: 6.0042 ||:  53%|#####3    | 406/759 [02:12<02:32,  2.32it/s]batch_loss: 2.1205, loss: 5.9946 ||:  54%|#####3    | 407/759 [02:12<02:22,  2.47it/s]batch_loss: 2.1528, loss: 5.9852 ||:  54%|#####3    | 408/759 [02:12<02:18,  2.54it/s]batch_loss: 2.1565, loss: 5.9759 ||:  54%|#####3    | 409/759 [02:13<01:56,  3.02it/s]batch_loss: 2.7091, loss: 5.9679 ||:  54%|#####4    | 410/759 [02:13<02:31,  2.30it/s]batch_loss: 2.2566, loss: 5.9589 ||:  54%|#####4    | 411/759 [02:14<02:22,  2.45it/s]batch_loss: 2.0717, loss: 5.9419 ||:  54%|#####4    | 413/759 [02:14<01:31,  3.76it/s]batch_loss: 2.6669, loss: 5.9264 ||:  55%|#####4    | 415/759 [02:14<01:43,  3.31it/s]batch_loss: 1.6916, loss: 5.9163 ||:  55%|#####4    | 416/759 [02:15<01:44,  3.30it/s]batch_loss: 1.6391, loss: 5.9060 ||:  55%|#####4    | 417/759 [02:15<01:31,  3.76it/s]batch_loss: 1.9944, loss: 5.8881 ||:  55%|#####5    | 419/759 [02:15<01:06,  5.11it/s]batch_loss: 2.1542, loss: 5.8701 ||:  55%|#####5    | 421/759 [02:15<00:54,  6.17it/s]batch_loss: 1.9064, loss: 5.8515 ||:  56%|#####5    | 423/759 [02:15<00:46,  7.18it/s]batch_loss: 1.3997, loss: 5.8410 ||:  56%|#####5    | 424/759 [02:16<00:48,  6.93it/s]batch_loss: 1.5858, loss: 5.8310 ||:  56%|#####5    | 425/759 [02:16<00:47,  7.03it/s]batch_loss: 1.2522, loss: 5.8110 ||:  56%|#####6    | 427/759 [02:16<00:44,  7.46it/s]batch_loss: 1.3618, loss: 5.8006 ||:  56%|#####6    | 428/759 [02:16<00:51,  6.47it/s]batch_loss: 1.2272, loss: 5.7900 ||:  57%|#####6    | 429/759 [02:17<01:03,  5.18it/s]batch_loss: 1.4140, loss: 5.7798 ||:  57%|#####6    | 430/759 [02:17<01:16,  4.32it/s]batch_loss: 1.4361, loss: 5.7697 ||:  57%|#####6    | 431/759 [02:17<01:38,  3.32it/s]batch_loss: 1.9154, loss: 5.7608 ||:  57%|#####6    | 432/759 [02:18<01:21,  4.01it/s]batch_loss: 1.4500, loss: 5.7509 ||:  57%|#####7    | 433/759 [02:18<01:08,  4.75it/s]batch_loss: 2.1480, loss: 5.7426 ||:  57%|#####7    | 434/759 [02:18<00:59,  5.48it/s]batch_loss: 1.4774, loss: 5.7327 ||:  57%|#####7    | 435/759 [02:18<01:14,  4.34it/s]batch_loss: 1.3530, loss: 5.7227 ||:  57%|#####7    | 436/759 [02:18<01:25,  3.79it/s]batch_loss: 1.1706, loss: 5.7123 ||:  58%|#####7    | 437/759 [02:19<01:29,  3.59it/s]batch_loss: 1.3114, loss: 5.7022 ||:  58%|#####7    | 438/759 [02:19<01:34,  3.38it/s]batch_loss: 1.2357, loss: 5.6828 ||:  58%|#####7    | 440/759 [02:19<01:03,  5.04it/s]batch_loss: 1.0326, loss: 5.6723 ||:  58%|#####8    | 441/759 [02:20<01:14,  4.29it/s]batch_loss: 1.2179, loss: 5.6524 ||:  58%|#####8    | 443/759 [02:20<00:55,  5.72it/s]batch_loss: 0.9652, loss: 5.6418 ||:  58%|#####8    | 444/759 [02:20<01:06,  4.72it/s]batch_loss: 0.9772, loss: 5.6220 ||:  59%|#####8    | 446/759 [02:20<00:56,  5.50it/s]batch_loss: 1.2405, loss: 5.6122 ||:  59%|#####8    | 447/759 [02:21<01:18,  3.95it/s]batch_loss: 1.1801, loss: 5.5937 ||:  59%|#####9    | 449/759 [02:21<01:19,  3.89it/s]batch_loss: 1.0348, loss: 5.5835 ||:  59%|#####9    | 450/759 [02:22<01:10,  4.37it/s]batch_loss: 1.6298, loss: 5.5748 ||:  59%|#####9    | 451/759 [02:22<01:56,  2.63it/s]batch_loss: 0.9094, loss: 5.5644 ||:  60%|#####9    | 452/759 [02:23<01:54,  2.69it/s]batch_loss: 1.2690, loss: 5.5550 ||:  60%|#####9    | 453/759 [02:23<01:33,  3.27it/s]batch_loss: 1.8777, loss: 5.5469 ||:  60%|#####9    | 454/759 [02:24<02:26,  2.08it/s]batch_loss: 0.8951, loss: 5.5366 ||:  60%|#####9    | 455/759 [02:24<02:25,  2.09it/s]batch_loss: 1.3334, loss: 5.5177 ||:  60%|######    | 457/759 [02:25<02:23,  2.10it/s]batch_loss: 0.7643, loss: 5.5073 ||:  60%|######    | 458/759 [02:26<02:11,  2.29it/s]batch_loss: 0.7378, loss: 5.4969 ||:  60%|######    | 459/759 [02:26<01:47,  2.79it/s]batch_loss: 0.8387, loss: 5.4868 ||:  61%|######    | 460/759 [02:26<01:27,  3.41it/s]batch_loss: 0.9854, loss: 5.4770 ||:  61%|######    | 461/759 [02:26<01:56,  2.56it/s]batch_loss: 0.5694, loss: 5.4664 ||:  61%|######    | 462/759 [02:27<01:33,  3.19it/s]batch_loss: 0.9714, loss: 5.4567 ||:  61%|######1   | 463/759 [02:27<01:47,  2.74it/s]batch_loss: 0.8097, loss: 5.4467 ||:  61%|######1   | 464/759 [02:28<01:57,  2.52it/s]batch_loss: 0.6757, loss: 5.4364 ||:  61%|######1   | 465/759 [02:28<01:48,  2.71it/s]batch_loss: 0.8977, loss: 5.4267 ||:  61%|######1   | 466/759 [02:28<02:12,  2.22it/s]batch_loss: 0.8708, loss: 5.4169 ||:  62%|######1   | 467/759 [02:29<01:44,  2.78it/s]batch_loss: 0.6221, loss: 5.4067 ||:  62%|######1   | 468/759 [02:29<01:40,  2.88it/s]batch_loss: 0.7162, loss: 5.3967 ||:  62%|######1   | 469/759 [02:29<01:52,  2.58it/s]batch_loss: 0.7149, loss: 5.3867 ||:  62%|######1   | 470/759 [02:30<01:28,  3.28it/s]batch_loss: 0.7135, loss: 5.3768 ||:  62%|######2   | 471/759 [02:30<01:11,  4.02it/s]batch_loss: 0.9788, loss: 5.3675 ||:  62%|######2   | 472/759 [02:30<00:59,  4.81it/s]batch_loss: 1.1595, loss: 5.3586 ||:  62%|######2   | 473/759 [02:31<02:07,  2.24it/s]batch_loss: 0.5241, loss: 5.3484 ||:  62%|######2   | 474/759 [02:31<01:57,  2.42it/s]batch_loss: 0.6489, loss: 5.3385 ||:  63%|######2   | 475/759 [02:31<01:34,  3.01it/s]batch_loss: 0.7697, loss: 5.3289 ||:  63%|######2   | 476/759 [02:32<02:01,  2.33it/s]batch_loss: 0.5748, loss: 5.3110 ||:  63%|######2   | 478/759 [02:32<01:30,  3.10it/s]batch_loss: 0.7101, loss: 5.3014 ||:  63%|######3   | 479/759 [02:33<01:53,  2.47it/s]batch_loss: 0.3978, loss: 5.2912 ||:  63%|######3   | 480/759 [02:33<01:47,  2.59it/s]batch_loss: 0.4629, loss: 5.2715 ||:  64%|######3   | 482/759 [02:34<01:34,  2.92it/s]batch_loss: 0.4632, loss: 5.2616 ||:  64%|######3   | 483/759 [02:34<01:20,  3.42it/s]batch_loss: 0.4972, loss: 5.2517 ||:  64%|######3   | 484/759 [02:34<01:22,  3.35it/s]batch_loss: 0.4944, loss: 5.2419 ||:  64%|######3   | 485/759 [02:35<01:23,  3.26it/s]batch_loss: 0.4357, loss: 5.2320 ||:  64%|######4   | 486/759 [02:35<01:08,  4.00it/s]batch_loss: 0.4900, loss: 5.2135 ||:  64%|######4   | 488/759 [02:35<01:12,  3.72it/s]batch_loss: 0.3433, loss: 5.1947 ||:  65%|######4   | 490/759 [02:36<01:03,  4.23it/s]batch_loss: 0.5859, loss: 5.1761 ||:  65%|######4   | 492/759 [02:36<01:17,  3.43it/s]batch_loss: 0.4176, loss: 5.1665 ||:  65%|######4   | 493/759 [02:37<01:08,  3.88it/s]batch_loss: 0.4893, loss: 5.1570 ||:  65%|######5   | 494/759 [02:37<01:11,  3.72it/s]batch_loss: 0.7749, loss: 5.1398 ||:  65%|######5   | 496/759 [02:37<00:50,  5.24it/s]batch_loss: 0.4168, loss: 5.1303 ||:  65%|######5   | 497/759 [02:37<00:57,  4.59it/s]batch_loss: 0.3998, loss: 5.1208 ||:  66%|######5   | 498/759 [02:38<01:12,  3.58it/s]batch_loss: 0.4132, loss: 5.1113 ||:  66%|######5   | 499/759 [02:38<01:17,  3.38it/s]batch_loss: 0.5012, loss: 5.1021 ||:  66%|######5   | 500/759 [02:39<01:55,  2.25it/s]batch_loss: 0.8047, loss: 5.0935 ||:  66%|######6   | 501/759 [02:40<02:35,  1.66it/s]batch_loss: 0.3496, loss: 5.0841 ||:  66%|######6   | 502/759 [02:40<02:15,  1.90it/s]batch_loss: 0.9911, loss: 5.0760 ||:  66%|######6   | 503/759 [02:42<03:15,  1.31it/s]batch_loss: 0.5573, loss: 5.0587 ||:  67%|######6   | 505/759 [02:42<01:57,  2.17it/s]batch_loss: 0.4310, loss: 5.0495 ||:  67%|######6   | 506/759 [02:42<01:48,  2.33it/s]batch_loss: 0.2619, loss: 5.0401 ||:  67%|######6   | 507/759 [02:43<01:43,  2.44it/s]batch_loss: 0.5785, loss: 5.0231 ||:  67%|######7   | 509/759 [02:43<01:07,  3.71it/s]batch_loss: 0.2786, loss: 5.0138 ||:  67%|######7   | 510/759 [02:43<01:12,  3.43it/s]batch_loss: 0.2615, loss: 5.0045 ||:  67%|######7   | 511/759 [02:43<01:16,  3.26it/s]batch_loss: 0.3268, loss: 4.9953 ||:  67%|######7   | 512/759 [02:44<01:27,  2.81it/s]batch_loss: 0.5152, loss: 4.9866 ||:  68%|######7   | 513/759 [02:44<01:24,  2.92it/s]batch_loss: 0.4343, loss: 4.9700 ||:  68%|######7   | 515/759 [02:45<01:07,  3.64it/s]batch_loss: 0.3811, loss: 4.9611 ||:  68%|######7   | 516/759 [02:45<01:00,  3.99it/s]batch_loss: 0.3078, loss: 4.9521 ||:  68%|######8   | 517/759 [02:46<01:27,  2.78it/s]batch_loss: 0.2402, loss: 4.9343 ||:  68%|######8   | 519/759 [02:46<01:11,  3.36it/s]batch_loss: 0.2767, loss: 4.9253 ||:  69%|######8   | 520/759 [02:46<01:03,  3.76it/s]batch_loss: 0.4379, loss: 4.9167 ||:  69%|######8   | 521/759 [02:46<01:06,  3.57it/s]batch_loss: 0.2349, loss: 4.9077 ||:  69%|######8   | 522/759 [02:47<01:09,  3.40it/s]batch_loss: 0.4329, loss: 4.8992 ||:  69%|######8   | 523/759 [02:47<00:58,  4.00it/s]batch_loss: 0.3809, loss: 4.8906 ||:  69%|######9   | 524/759 [02:47<01:02,  3.74it/s]batch_loss: 0.4305, loss: 4.8821 ||:  69%|######9   | 525/759 [02:48<01:52,  2.08it/s]batch_loss: 0.6379, loss: 4.8740 ||:  69%|######9   | 526/759 [02:49<01:41,  2.28it/s]batch_loss: 0.2341, loss: 4.8573 ||:  70%|######9   | 528/759 [02:49<01:19,  2.92it/s]batch_loss: 0.2461, loss: 4.8401 ||:  70%|######9   | 530/759 [02:50<01:28,  2.58it/s]batch_loss: 0.4897, loss: 4.8251 ||:  70%|#######   | 532/759 [02:50<01:12,  3.13it/s]batch_loss: 0.3028, loss: 4.8167 ||:  70%|#######   | 533/759 [02:51<01:29,  2.53it/s]batch_loss: 0.3718, loss: 4.8003 ||:  70%|#######   | 535/759 [02:51<01:11,  3.11it/s]batch_loss: 0.3972, loss: 4.7921 ||:  71%|#######   | 536/759 [02:52<01:22,  2.71it/s]batch_loss: 0.3963, loss: 4.7839 ||:  71%|#######   | 537/759 [02:52<01:09,  3.19it/s]batch_loss: 0.2992, loss: 4.7756 ||:  71%|#######   | 538/759 [02:52<01:08,  3.22it/s]batch_loss: 0.2886, loss: 4.7673 ||:  71%|#######1  | 539/759 [02:53<01:00,  3.63it/s]batch_loss: 0.4202, loss: 4.7592 ||:  71%|#######1  | 540/759 [02:53<01:03,  3.46it/s]batch_loss: 0.1209, loss: 4.7506 ||:  71%|#######1  | 541/759 [02:53<01:13,  2.98it/s]batch_loss: 0.2665, loss: 4.7424 ||:  71%|#######1  | 542/759 [02:54<01:58,  1.83it/s]batch_loss: 0.2808, loss: 4.7342 ||:  72%|#######1  | 543/759 [02:55<01:44,  2.06it/s]batch_loss: 0.3449, loss: 4.7187 ||:  72%|#######1  | 545/759 [02:55<01:17,  2.77it/s]batch_loss: 0.2628, loss: 4.7105 ||:  72%|#######1  | 546/759 [02:55<01:14,  2.87it/s]batch_loss: 1.0190, loss: 4.6991 ||:  72%|#######2  | 548/759 [02:56<00:50,  4.19it/s]batch_loss: 0.3273, loss: 4.6911 ||:  72%|#######2  | 549/759 [02:57<01:30,  2.33it/s]batch_loss: 0.3702, loss: 4.6833 ||:  72%|#######2  | 550/759 [02:57<01:14,  2.81it/s]batch_loss: 0.2939, loss: 4.6683 ||:  73%|#######2  | 552/759 [02:57<00:52,  3.94it/s]batch_loss: 0.6521, loss: 4.6611 ||:  73%|#######2  | 553/759 [02:57<00:46,  4.45it/s]batch_loss: 0.2288, loss: 4.6531 ||:  73%|#######2  | 554/759 [02:57<00:41,  4.88it/s]batch_loss: 0.2836, loss: 4.6452 ||:  73%|#######3  | 555/759 [02:58<00:49,  4.15it/s]batch_loss: 0.3414, loss: 4.6375 ||:  73%|#######3  | 556/759 [02:58<00:52,  3.87it/s]batch_loss: 0.2538, loss: 4.6296 ||:  73%|#######3  | 557/759 [02:58<00:57,  3.50it/s]batch_loss: 0.1739, loss: 4.6216 ||:  74%|#######3  | 558/759 [02:59<01:00,  3.33it/s]batch_loss: 0.1573, loss: 4.6136 ||:  74%|#######3  | 559/759 [02:59<00:51,  3.87it/s]batch_loss: 0.4322, loss: 4.5992 ||:  74%|#######3  | 561/759 [02:59<00:36,  5.42it/s]batch_loss: 0.1895, loss: 4.5913 ||:  74%|#######4  | 562/759 [03:00<00:50,  3.88it/s]batch_loss: 0.1795, loss: 4.5835 ||:  74%|#######4  | 563/759 [03:00<00:54,  3.60it/s]batch_loss: 0.1667, loss: 4.5757 ||:  74%|#######4  | 564/759 [03:01<01:16,  2.55it/s]batch_loss: 0.2397, loss: 4.5680 ||:  74%|#######4  | 565/759 [03:01<01:12,  2.69it/s]batch_loss: 0.2775, loss: 4.5531 ||:  75%|#######4  | 567/759 [03:01<00:57,  3.36it/s]batch_loss: 0.2168, loss: 4.5454 ||:  75%|#######4  | 568/759 [03:01<00:52,  3.63it/s]batch_loss: 0.1025, loss: 4.5376 ||:  75%|#######4  | 569/759 [03:02<01:04,  2.94it/s]batch_loss: 0.4270, loss: 4.5304 ||:  75%|#######5  | 570/759 [03:02<00:54,  3.45it/s]batch_loss: 0.2876, loss: 4.5230 ||:  75%|#######5  | 571/759 [03:02<00:55,  3.36it/s]batch_loss: 0.3087, loss: 4.5156 ||:  75%|#######5  | 572/759 [03:04<01:47,  1.73it/s]batch_loss: 0.1569, loss: 4.5080 ||:  75%|#######5  | 573/759 [03:05<02:00,  1.54it/s]batch_loss: 0.4563, loss: 4.5010 ||:  76%|#######5  | 574/759 [03:05<01:41,  1.82it/s]batch_loss: 0.2496, loss: 4.4936 ||:  76%|#######5  | 575/759 [03:06<02:35,  1.19it/s]batch_loss: 0.1156, loss: 4.4860 ||:  76%|#######5  | 576/759 [03:07<01:58,  1.55it/s]batch_loss: 0.1396, loss: 4.4784 ||:  76%|#######6  | 577/759 [03:07<01:39,  1.83it/s]batch_loss: 0.0812, loss: 4.4708 ||:  76%|#######6  | 578/759 [03:07<01:37,  1.85it/s]batch_loss: 0.1144, loss: 4.4633 ||:  76%|#######6  | 579/759 [03:08<01:36,  1.87it/s]batch_loss: 0.1589, loss: 4.4559 ||:  76%|#######6  | 580/759 [03:09<01:46,  1.68it/s]batch_loss: 0.3811, loss: 4.4489 ||:  77%|#######6  | 581/759 [03:09<01:25,  2.09it/s]batch_loss: 0.2092, loss: 4.4416 ||:  77%|#######6  | 582/759 [03:09<01:16,  2.31it/s]batch_loss: 0.2187, loss: 4.4343 ||:  77%|#######6  | 583/759 [03:10<01:11,  2.47it/s]batch_loss: 0.1988, loss: 4.4271 ||:  77%|#######6  | 584/759 [03:10<01:21,  2.13it/s]batch_loss: 0.1432, loss: 4.4198 ||:  77%|#######7  | 585/759 [03:11<01:14,  2.34it/s]batch_loss: 0.7406, loss: 4.4135 ||:  77%|#######7  | 586/759 [03:11<00:58,  2.97it/s]batch_loss: 0.5622, loss: 4.4069 ||:  77%|#######7  | 587/759 [03:11<00:46,  3.68it/s]batch_loss: 0.2017, loss: 4.3998 ||:  77%|#######7  | 588/759 [03:11<00:51,  3.34it/s]batch_loss: 0.1420, loss: 4.3925 ||:  78%|#######7  | 589/759 [03:12<01:20,  2.10it/s]batch_loss: 0.2580, loss: 4.3855 ||:  78%|#######7  | 590/759 [03:12<01:14,  2.28it/s]batch_loss: 0.2049, loss: 4.3785 ||:  78%|#######7  | 591/759 [03:13<01:00,  2.79it/s]batch_loss: 0.2634, loss: 4.3715 ||:  78%|#######7  | 592/759 [03:13<00:48,  3.47it/s]batch_loss: 0.1344, loss: 4.3644 ||:  78%|#######8  | 593/759 [03:13<01:07,  2.48it/s]batch_loss: 0.1156, loss: 4.3572 ||:  78%|#######8  | 594/759 [03:14<01:21,  2.03it/s]batch_loss: 0.2686, loss: 4.3503 ||:  78%|#######8  | 595/759 [03:14<01:02,  2.61it/s]batch_loss: 0.1112, loss: 4.3432 ||:  79%|#######8  | 596/759 [03:15<01:17,  2.12it/s]batch_loss: 0.0715, loss: 4.3361 ||:  79%|#######8  | 597/759 [03:15<01:17,  2.08it/s]batch_loss: 0.1004, loss: 4.3290 ||:  79%|#######8  | 598/759 [03:16<01:17,  2.08it/s]batch_loss: 0.3022, loss: 4.3223 ||:  79%|#######8  | 599/759 [03:16<00:59,  2.67it/s]batch_loss: 0.0920, loss: 4.3085 ||:  79%|#######9  | 601/759 [03:17<01:09,  2.28it/s]batch_loss: 0.0764, loss: 4.3014 ||:  79%|#######9  | 602/759 [03:18<01:12,  2.16it/s]batch_loss: 0.6374, loss: 4.2953 ||:  79%|#######9  | 603/759 [03:18<00:59,  2.63it/s]batch_loss: 0.1410, loss: 4.2885 ||:  80%|#######9  | 604/759 [03:18<00:57,  2.69it/s]batch_loss: 0.5566, loss: 4.2823 ||:  80%|#######9  | 605/759 [03:18<00:45,  3.35it/s]batch_loss: 0.2248, loss: 4.2756 ||:  80%|#######9  | 606/759 [03:18<00:46,  3.26it/s]batch_loss: 0.1855, loss: 4.2689 ||:  80%|#######9  | 607/759 [03:19<00:48,  3.14it/s]batch_loss: 0.2860, loss: 4.2623 ||:  80%|########  | 608/759 [03:19<00:55,  2.71it/s]batch_loss: 0.1775, loss: 4.2556 ||:  80%|########  | 609/759 [03:19<00:46,  3.23it/s]batch_loss: 0.3681, loss: 4.2492 ||:  80%|########  | 610/759 [03:20<00:39,  3.74it/s]batch_loss: 0.2493, loss: 4.2427 ||:  81%|########  | 611/759 [03:20<00:43,  3.41it/s]batch_loss: 0.1710, loss: 4.2360 ||:  81%|########  | 612/759 [03:20<00:36,  4.00it/s]batch_loss: 0.2434, loss: 4.2295 ||:  81%|########  | 613/759 [03:20<00:33,  4.42it/s]batch_loss: 0.1151, loss: 4.2228 ||:  81%|########  | 614/759 [03:21<00:40,  3.59it/s]batch_loss: 0.0888, loss: 4.2161 ||:  81%|########1 | 615/759 [03:21<01:00,  2.38it/s]batch_loss: 0.4194, loss: 4.2099 ||:  81%|########1 | 616/759 [03:22<00:46,  3.08it/s]batch_loss: 0.1379, loss: 4.2033 ||:  81%|########1 | 617/759 [03:22<01:10,  2.03it/s]batch_loss: 0.2194, loss: 4.1969 ||:  81%|########1 | 618/759 [03:23<01:02,  2.26it/s]batch_loss: 0.4343, loss: 4.1908 ||:  82%|########1 | 619/759 [03:23<00:47,  2.93it/s]batch_loss: 0.1476, loss: 4.1843 ||:  82%|########1 | 620/759 [03:24<01:01,  2.27it/s]batch_loss: 0.3757, loss: 4.1782 ||:  82%|########1 | 621/759 [03:24<00:55,  2.51it/s]batch_loss: 0.0948, loss: 4.1716 ||:  82%|########1 | 622/759 [03:24<01:04,  2.11it/s]batch_loss: 0.0743, loss: 4.1598 ||:  82%|########2 | 624/759 [03:25<00:48,  2.81it/s]batch_loss: 0.2644, loss: 4.1536 ||:  82%|########2 | 625/759 [03:25<00:52,  2.54it/s]batch_loss: 0.2086, loss: 4.1473 ||:  82%|########2 | 626/759 [03:26<00:42,  3.15it/s]batch_loss: 0.2402, loss: 4.1349 ||:  83%|########2 | 628/759 [03:26<00:28,  4.52it/s]batch_loss: 0.1399, loss: 4.1232 ||:  83%|########3 | 630/759 [03:26<00:31,  4.13it/s]batch_loss: 0.3811, loss: 4.1173 ||:  83%|########3 | 631/759 [03:26<00:29,  4.39it/s]batch_loss: 0.1021, loss: 4.1109 ||:  83%|########3 | 632/759 [03:27<00:34,  3.72it/s]batch_loss: 0.3018, loss: 4.1049 ||:  83%|########3 | 633/759 [03:27<00:29,  4.25it/s]batch_loss: 0.2114, loss: 4.0988 ||:  84%|########3 | 634/759 [03:27<00:32,  3.80it/s]batch_loss: 0.1838, loss: 4.0865 ||:  84%|########3 | 636/759 [03:28<00:24,  5.00it/s]batch_loss: 0.0911, loss: 4.0802 ||:  84%|########3 | 637/759 [03:28<00:32,  3.73it/s]batch_loss: 0.2541, loss: 4.0742 ||:  84%|########4 | 638/759 [03:28<00:33,  3.56it/s]batch_loss: 0.1126, loss: 4.0629 ||:  84%|########4 | 640/759 [03:29<00:34,  3.46it/s]batch_loss: 0.1158, loss: 4.0568 ||:  84%|########4 | 641/759 [03:30<00:53,  2.19it/s]batch_loss: 0.1722, loss: 4.0448 ||:  85%|########4 | 643/759 [03:30<00:40,  2.83it/s]batch_loss: 0.0934, loss: 4.0386 ||:  85%|########4 | 644/759 [03:31<00:40,  2.85it/s]batch_loss: 0.1658, loss: 4.0326 ||:  85%|########4 | 645/759 [03:31<00:39,  2.91it/s]batch_loss: 0.1543, loss: 4.0209 ||:  85%|########5 | 647/759 [03:32<00:35,  3.16it/s]batch_loss: 0.2125, loss: 4.0150 ||:  85%|########5 | 648/759 [03:32<00:35,  3.15it/s]batch_loss: 0.2907, loss: 4.0042 ||:  86%|########5 | 650/759 [03:32<00:29,  3.76it/s]batch_loss: 0.2277, loss: 3.9984 ||:  86%|########5 | 651/759 [03:33<00:27,  3.93it/s]batch_loss: 0.0992, loss: 3.9874 ||:  86%|########6 | 653/759 [03:33<00:31,  3.36it/s]batch_loss: 0.1549, loss: 3.9816 ||:  86%|########6 | 654/759 [03:34<00:33,  3.18it/s]batch_loss: 0.2253, loss: 3.9758 ||:  86%|########6 | 655/759 [03:34<00:27,  3.78it/s]batch_loss: 0.0828, loss: 3.9699 ||:  86%|########6 | 656/759 [03:34<00:38,  2.70it/s]batch_loss: 0.0557, loss: 3.9586 ||:  87%|########6 | 658/759 [03:35<00:31,  3.24it/s]batch_loss: 0.1407, loss: 3.9528 ||:  87%|########6 | 659/759 [03:35<00:31,  3.17it/s]batch_loss: 0.2174, loss: 3.9472 ||:  87%|########6 | 660/759 [03:36<00:31,  3.17it/s]batch_loss: 0.3215, loss: 3.9417 ||:  87%|########7 | 661/759 [03:36<00:25,  3.81it/s]batch_loss: 0.1239, loss: 3.9359 ||:  87%|########7 | 662/759 [03:36<00:21,  4.58it/s]batch_loss: 0.1316, loss: 3.9302 ||:  87%|########7 | 663/759 [03:36<00:28,  3.41it/s]batch_loss: 0.2873, loss: 3.9247 ||:  87%|########7 | 664/759 [03:36<00:23,  3.98it/s]batch_loss: 0.1149, loss: 3.9190 ||:  88%|########7 | 665/759 [03:37<00:24,  3.76it/s]batch_loss: 0.0664, loss: 3.9132 ||:  88%|########7 | 666/759 [03:37<00:39,  2.33it/s]batch_loss: 0.1386, loss: 3.9075 ||:  88%|########7 | 667/759 [03:38<00:40,  2.28it/s]batch_loss: 0.4603, loss: 3.9024 ||:  88%|########8 | 668/759 [03:38<00:32,  2.80it/s]batch_loss: 0.2491, loss: 3.8923 ||:  88%|########8 | 670/759 [03:38<00:25,  3.51it/s]batch_loss: 0.4261, loss: 3.8820 ||:  89%|########8 | 672/759 [03:39<00:21,  3.99it/s]batch_loss: 0.1113, loss: 3.8764 ||:  89%|########8 | 673/759 [03:39<00:22,  3.74it/s]batch_loss: 0.1284, loss: 3.8708 ||:  89%|########8 | 674/759 [03:40<00:33,  2.53it/s]batch_loss: 0.3512, loss: 3.8656 ||:  89%|########8 | 675/759 [03:40<00:28,  3.00it/s]batch_loss: 0.0945, loss: 3.8601 ||:  89%|########9 | 676/759 [03:41<00:32,  2.57it/s]batch_loss: 0.2396, loss: 3.8547 ||:  89%|########9 | 677/759 [03:41<00:30,  2.65it/s]batch_loss: 0.3147, loss: 3.8495 ||:  89%|########9 | 678/759 [03:41<00:29,  2.77it/s]batch_loss: 0.1092, loss: 3.8440 ||:  89%|########9 | 679/759 [03:42<00:27,  2.87it/s]batch_loss: 0.1587, loss: 3.8342 ||:  90%|########9 | 681/759 [03:42<00:21,  3.59it/s]batch_loss: 0.4010, loss: 3.8292 ||:  90%|########9 | 682/759 [03:42<00:18,  4.12it/s]batch_loss: 0.5971, loss: 3.8245 ||:  90%|########9 | 683/759 [03:42<00:15,  4.81it/s]batch_loss: 0.4831, loss: 3.8196 ||:  90%|######### | 684/759 [03:42<00:13,  5.39it/s]batch_loss: 0.0818, loss: 3.8141 ||:  90%|######### | 685/759 [03:43<00:30,  2.40it/s]batch_loss: 0.4771, loss: 3.8093 ||:  90%|######### | 686/759 [03:44<00:28,  2.54it/s]batch_loss: 0.6723, loss: 3.8047 ||:  91%|######### | 687/759 [03:44<00:23,  3.12it/s]batch_loss: 0.2397, loss: 3.7995 ||:  91%|######### | 688/759 [03:44<00:23,  3.05it/s]batch_loss: 0.0694, loss: 3.7941 ||:  91%|######### | 689/759 [03:45<00:30,  2.28it/s]batch_loss: 0.3883, loss: 3.7892 ||:  91%|######### | 690/759 [03:45<00:23,  2.89it/s]batch_loss: 0.1112, loss: 3.7838 ||:  91%|#########1| 691/759 [03:46<00:29,  2.29it/s]batch_loss: 0.1260, loss: 3.7786 ||:  91%|#########1| 692/759 [03:46<00:30,  2.22it/s]batch_loss: 0.3187, loss: 3.7736 ||:  91%|#########1| 693/759 [03:46<00:23,  2.78it/s]batch_loss: 0.6040, loss: 3.7690 ||:  91%|#########1| 694/759 [03:47<00:19,  3.30it/s]batch_loss: 0.2606, loss: 3.7639 ||:  92%|#########1| 695/759 [03:47<00:17,  3.65it/s]batch_loss: 0.1412, loss: 3.7587 ||:  92%|#########1| 696/759 [03:47<00:18,  3.48it/s]batch_loss: 0.1395, loss: 3.7535 ||:  92%|#########1| 697/759 [03:47<00:18,  3.30it/s]batch_loss: 0.1490, loss: 3.7484 ||:  92%|#########1| 698/759 [03:48<00:15,  3.82it/s]batch_loss: 0.2598, loss: 3.7434 ||:  92%|#########2| 699/759 [03:48<00:14,  4.13it/s]batch_loss: 0.1071, loss: 3.7382 ||:  92%|#########2| 700/759 [03:48<00:16,  3.61it/s]batch_loss: 0.1650, loss: 3.7331 ||:  92%|#########2| 701/759 [03:49<00:17,  3.35it/s]batch_loss: 0.2379, loss: 3.7281 ||:  92%|#########2| 702/759 [03:49<00:14,  4.07it/s]batch_loss: 0.0532, loss: 3.7229 ||:  93%|#########2| 703/759 [03:49<00:15,  3.51it/s]batch_loss: 0.9095, loss: 3.7189 ||:  93%|#########2| 704/759 [03:49<00:16,  3.44it/s]batch_loss: 0.4423, loss: 3.7142 ||:  93%|#########2| 705/759 [03:50<00:15,  3.38it/s]batch_loss: 0.2403, loss: 3.7093 ||:  93%|#########3| 706/759 [03:50<00:13,  4.05it/s]batch_loss: 1.1013, loss: 3.7056 ||:  93%|#########3| 707/759 [03:50<00:11,  4.68it/s]batch_loss: 0.1172, loss: 3.7006 ||:  93%|#########3| 708/759 [03:50<00:13,  3.90it/s]batch_loss: 0.2274, loss: 3.6957 ||:  93%|#########3| 709/759 [03:50<00:11,  4.38it/s]batch_loss: 0.1944, loss: 3.6907 ||:  94%|#########3| 710/759 [03:51<00:12,  3.81it/s]batch_loss: 0.0565, loss: 3.6856 ||:  94%|#########3| 711/759 [03:51<00:18,  2.56it/s]batch_loss: 0.1885, loss: 3.6807 ||:  94%|#########3| 712/759 [03:52<00:17,  2.66it/s]batch_loss: 0.1618, loss: 3.6758 ||:  94%|#########3| 713/759 [03:52<00:17,  2.62it/s]batch_loss: 0.0929, loss: 3.6708 ||:  94%|#########4| 714/759 [03:53<00:26,  1.68it/s]batch_loss: 0.5248, loss: 3.6664 ||:  94%|#########4| 715/759 [03:53<00:20,  2.19it/s]batch_loss: 0.0767, loss: 3.6613 ||:  94%|#########4| 716/759 [03:54<00:24,  1.78it/s]batch_loss: 0.1449, loss: 3.6564 ||:  94%|#########4| 717/759 [03:55<00:20,  2.00it/s]batch_loss: 0.7808, loss: 3.6524 ||:  95%|#########4| 718/759 [03:55<00:16,  2.50it/s]batch_loss: 0.5919, loss: 3.6482 ||:  95%|#########4| 719/759 [03:55<00:13,  3.02it/s]batch_loss: 0.1276, loss: 3.6433 ||:  95%|#########4| 720/759 [03:55<00:13,  2.92it/s]batch_loss: 0.0923, loss: 3.6384 ||:  95%|#########4| 721/759 [03:56<00:14,  2.55it/s]batch_loss: 0.3218, loss: 3.6338 ||:  95%|#########5| 722/759 [03:56<00:11,  3.23it/s]batch_loss: 0.1496, loss: 3.6290 ||:  95%|#########5| 723/759 [03:56<00:11,  3.03it/s]batch_loss: 0.1365, loss: 3.6194 ||:  96%|#########5| 725/759 [03:57<00:09,  3.64it/s]batch_loss: 0.1414, loss: 3.6146 ||:  96%|#########5| 726/759 [03:57<00:09,  3.49it/s]batch_loss: 0.0886, loss: 3.6098 ||:  96%|#########5| 727/759 [03:58<00:10,  2.93it/s]batch_loss: 0.1109, loss: 3.6006 ||:  96%|#########6| 729/759 [03:58<00:07,  4.17it/s]batch_loss: 0.0884, loss: 3.5916 ||:  96%|#########6| 731/759 [03:58<00:06,  4.24it/s]batch_loss: 0.6108, loss: 3.5835 ||:  97%|#########6| 733/759 [03:58<00:04,  5.41it/s]batch_loss: 0.2368, loss: 3.5790 ||:  97%|#########6| 734/759 [03:59<00:04,  5.29it/s]batch_loss: 0.0901, loss: 3.5742 ||:  97%|#########6| 735/759 [03:59<00:05,  4.39it/s]batch_loss: 0.2562, loss: 3.5697 ||:  97%|#########6| 736/759 [03:59<00:04,  4.94it/s]batch_loss: 0.0693, loss: 3.5650 ||:  97%|#########7| 737/759 [04:00<00:07,  3.06it/s]batch_loss: 0.1060, loss: 3.5603 ||:  97%|#########7| 738/759 [04:01<00:11,  1.87it/s]batch_loss: 0.1162, loss: 3.5556 ||:  97%|#########7| 739/759 [04:01<00:09,  2.10it/s]batch_loss: 0.1413, loss: 3.5510 ||:  97%|#########7| 740/759 [04:02<00:09,  1.92it/s]batch_loss: 0.1760, loss: 3.5425 ||:  98%|#########7| 742/759 [04:02<00:05,  2.93it/s]batch_loss: 0.2128, loss: 3.5380 ||:  98%|#########7| 743/759 [04:02<00:04,  3.45it/s]batch_loss: 0.4338, loss: 3.5338 ||:  98%|#########8| 744/759 [04:02<00:03,  4.03it/s]batch_loss: 0.1450, loss: 3.5293 ||:  98%|#########8| 745/759 [04:03<00:04,  3.23it/s]batch_loss: 0.4499, loss: 3.5251 ||:  98%|#########8| 746/759 [04:03<00:03,  3.78it/s]batch_loss: 0.1672, loss: 3.5206 ||:  98%|#########8| 747/759 [04:03<00:03,  3.61it/s]batch_loss: 0.2966, loss: 3.5163 ||:  99%|#########8| 748/759 [04:03<00:02,  3.99it/s]batch_loss: 0.3527, loss: 3.5121 ||:  99%|#########8| 749/759 [04:04<00:02,  4.48it/s]batch_loss: 0.2203, loss: 3.5077 ||:  99%|#########8| 750/759 [04:04<00:02,  3.95it/s]batch_loss: 0.1412, loss: 3.5032 ||:  99%|#########8| 751/759 [04:04<00:02,  3.07it/s]batch_loss: 0.1630, loss: 3.4988 ||:  99%|#########9| 752/759 [04:05<00:02,  3.05it/s]batch_loss: 0.1452, loss: 3.4943 ||:  99%|#########9| 753/759 [04:05<00:01,  3.07it/s]batch_loss: 0.4508, loss: 3.4903 ||:  99%|#########9| 754/759 [04:05<00:01,  3.86it/s]batch_loss: 0.0856, loss: 3.4858 ||:  99%|#########9| 755/759 [04:06<00:01,  2.69it/s]batch_loss: 0.2598, loss: 3.4815 ||: 100%|#########9| 756/759 [04:06<00:00,  3.26it/s]batch_loss: 0.1073, loss: 3.4732 ||: 100%|#########9| 758/759 [04:07<00:00,  3.35it/s]batch_loss: 0.1881, loss: 3.4688 ||: 100%|##########| 759/759 [04:07<00:00,  2.91it/s]batch_loss: 0.1881, loss: 3.4688 ||: 100%|##########| 759/759 [04:07<00:00,  3.07it/s]
2023-11-07 18:38:30,848 - INFO - allennlp.training.gradient_descent_trainer - Validating
  0%|          | 0/253 [00:00<?, ?it/s]batch_loss: 0.0000, loss: 0.0000 ||:   0%|          | 1/253 [00:00<01:08,  3.70it/s]batch_loss: 0.0000, loss: 0.0000 ||:   1%|1         | 3/253 [00:01<01:28,  2.82it/s]batch_loss: 0.0000, loss: 0.0000 ||:   2%|1         | 4/253 [00:01<01:35,  2.60it/s]batch_loss: 0.0000, loss: 0.0000 ||:   2%|1         | 5/253 [00:02<01:48,  2.28it/s]batch_loss: 0.0000, loss: 0.0000 ||:   2%|2         | 6/253 [00:02<01:47,  2.31it/s]batch_loss: 0.0000, loss: 0.0000 ||:   3%|2         | 7/253 [00:03<02:06,  1.94it/s]batch_loss: 0.0000, loss: 0.0000 ||:   3%|3         | 8/253 [00:03<01:47,  2.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:   4%|3         | 9/253 [00:04<01:59,  2.04it/s]batch_loss: 0.0000, loss: 0.0000 ||:   4%|3         | 10/253 [00:04<01:54,  2.12it/s]batch_loss: 0.0000, loss: 0.0000 ||:   4%|4         | 11/253 [00:05<02:32,  1.59it/s]batch_loss: 0.0000, loss: 0.0000 ||:   5%|4         | 12/253 [00:05<01:55,  2.09it/s]batch_loss: 0.0000, loss: 0.0000 ||:   5%|5         | 13/253 [00:05<01:49,  2.19it/s]batch_loss: 0.0000, loss: 0.0000 ||:   6%|5         | 14/253 [00:06<01:36,  2.48it/s]batch_loss: 0.0000, loss: 0.0000 ||:   6%|5         | 15/253 [00:06<01:26,  2.75it/s]batch_loss: 0.0000, loss: 0.0000 ||:   6%|6         | 16/253 [00:06<01:20,  2.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:   7%|6         | 17/253 [00:07<01:35,  2.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:   7%|7         | 18/253 [00:07<01:48,  2.17it/s]batch_loss: 0.0000, loss: 0.0000 ||:   8%|7         | 19/253 [00:08<01:34,  2.48it/s]batch_loss: 0.0000, loss: 0.0000 ||:   8%|8         | 21/253 [00:09<01:32,  2.50it/s]batch_loss: 0.0000, loss: 0.0000 ||:   9%|8         | 22/253 [00:09<01:43,  2.24it/s]batch_loss: 0.0000, loss: 0.0000 ||:   9%|9         | 23/253 [00:09<01:33,  2.45it/s]batch_loss: 0.0000, loss: 0.0000 ||:   9%|9         | 24/253 [00:10<01:25,  2.68it/s]batch_loss: 0.0000, loss: 0.0000 ||:  10%|9         | 25/253 [00:10<01:47,  2.13it/s]batch_loss: 0.0000, loss: 0.0000 ||:  10%|#         | 26/253 [00:11<01:33,  2.42it/s]batch_loss: 0.0000, loss: 0.0000 ||:  11%|#         | 27/253 [00:11<01:24,  2.69it/s]batch_loss: 0.0000, loss: 0.0000 ||:  11%|#1        | 28/253 [00:11<01:08,  3.30it/s]batch_loss: 0.0000, loss: 0.0000 ||:  11%|#1        | 29/253 [00:12<01:25,  2.62it/s]batch_loss: 0.0000, loss: 0.0000 ||:  12%|#1        | 30/253 [00:12<01:17,  2.87it/s]batch_loss: 0.0000, loss: 0.0000 ||:  12%|#2        | 31/253 [00:12<01:03,  3.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  13%|#3        | 33/253 [00:13<00:59,  3.72it/s]batch_loss: 0.0000, loss: 0.0000 ||:  13%|#3        | 34/253 [00:13<00:51,  4.22it/s]batch_loss: 0.0000, loss: 0.0000 ||:  14%|#3        | 35/253 [00:13<01:00,  3.58it/s]batch_loss: 0.0000, loss: 0.0000 ||:  15%|#4        | 37/253 [00:14<01:08,  3.16it/s]batch_loss: 0.0000, loss: 0.0000 ||:  15%|#5        | 38/253 [00:14<01:14,  2.89it/s]batch_loss: 0.0000, loss: 0.0000 ||:  15%|#5        | 39/253 [00:15<01:31,  2.33it/s]batch_loss: 0.0000, loss: 0.0000 ||:  16%|#6        | 41/253 [00:15<01:01,  3.44it/s]batch_loss: 0.0000, loss: 0.0000 ||:  17%|#6        | 42/253 [00:16<01:20,  2.63it/s]batch_loss: 0.0000, loss: 0.0000 ||:  17%|#6        | 43/253 [00:16<01:14,  2.82it/s]batch_loss: 0.0000, loss: 0.0000 ||:  17%|#7        | 44/253 [00:17<01:16,  2.73it/s]batch_loss: 0.0000, loss: 0.0000 ||:  18%|#8        | 46/253 [00:17<00:52,  3.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  19%|#8        | 48/253 [00:17<00:57,  3.58it/s]batch_loss: 0.0000, loss: 0.0000 ||:  19%|#9        | 49/253 [00:18<00:57,  3.56it/s]batch_loss: 0.0000, loss: 0.0000 ||:  20%|#9        | 50/253 [00:18<00:56,  3.60it/s]batch_loss: 0.0000, loss: 0.0000 ||:  20%|##        | 51/253 [00:18<00:56,  3.59it/s]batch_loss: 0.0000, loss: 0.0000 ||:  21%|##        | 52/253 [00:18<00:47,  4.23it/s]batch_loss: 0.0000, loss: 0.0000 ||:  21%|##        | 53/253 [00:19<01:04,  3.08it/s]batch_loss: 0.0000, loss: 0.0000 ||:  22%|##1       | 55/253 [00:19<00:50,  3.90it/s]batch_loss: 0.0000, loss: 0.0000 ||:  22%|##2       | 56/253 [00:20<01:24,  2.34it/s]batch_loss: 0.0000, loss: 0.0000 ||:  23%|##2       | 57/253 [00:20<01:09,  2.83it/s]batch_loss: 0.0000, loss: 0.0000 ||:  23%|##2       | 58/253 [00:21<01:12,  2.70it/s]batch_loss: 0.0000, loss: 0.0000 ||:  23%|##3       | 59/253 [00:22<01:38,  1.97it/s]batch_loss: 0.0000, loss: 0.0000 ||:  24%|##3       | 60/253 [00:22<01:18,  2.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  24%|##4       | 61/253 [00:23<01:49,  1.75it/s]batch_loss: 0.0000, loss: 0.0000 ||:  25%|##4       | 63/253 [00:23<01:09,  2.74it/s]batch_loss: 0.0000, loss: 0.0000 ||:  25%|##5       | 64/253 [00:24<01:24,  2.23it/s]batch_loss: 0.0000, loss: 0.0000 ||:  26%|##6       | 66/253 [00:24<01:19,  2.36it/s]batch_loss: 0.0000, loss: 0.0000 ||:  26%|##6       | 67/253 [00:25<01:12,  2.58it/s]batch_loss: 0.0000, loss: 0.0000 ||:  27%|##6       | 68/253 [00:25<01:08,  2.71it/s]batch_loss: 0.0000, loss: 0.0000 ||:  27%|##7       | 69/253 [00:25<00:56,  3.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:  28%|##7       | 70/253 [00:25<00:57,  3.21it/s]batch_loss: 0.0000, loss: 0.0000 ||:  28%|##8       | 71/253 [00:26<00:55,  3.29it/s]batch_loss: 0.0000, loss: 0.0000 ||:  28%|##8       | 72/253 [00:26<01:03,  2.86it/s]batch_loss: 0.0000, loss: 0.0000 ||:  29%|##9       | 74/253 [00:27<01:06,  2.70it/s]batch_loss: 0.0000, loss: 0.0000 ||:  30%|##9       | 75/253 [00:27<01:01,  2.89it/s]batch_loss: 0.0000, loss: 0.0000 ||:  30%|###       | 76/253 [00:28<01:04,  2.76it/s]batch_loss: 0.0000, loss: 0.0000 ||:  30%|###       | 77/253 [00:28<01:19,  2.21it/s]batch_loss: 0.0000, loss: 0.0000 ||:  31%|###       | 78/253 [00:29<01:03,  2.76it/s]batch_loss: 0.0000, loss: 0.0000 ||:  31%|###1      | 79/253 [00:29<00:58,  2.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  32%|###1      | 80/253 [00:29<00:55,  3.11it/s]batch_loss: 0.0000, loss: 0.0000 ||:  32%|###2      | 81/253 [00:30<01:13,  2.33it/s]batch_loss: 0.0000, loss: 0.0000 ||:  32%|###2      | 82/253 [00:30<01:05,  2.60it/s]batch_loss: 0.0000, loss: 0.0000 ||:  33%|###2      | 83/253 [00:30<00:52,  3.26it/s]batch_loss: 0.0000, loss: 0.0000 ||:  33%|###3      | 84/253 [00:31<01:12,  2.34it/s]batch_loss: 0.0000, loss: 0.0000 ||:  34%|###3      | 86/253 [00:32<01:07,  2.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  34%|###4      | 87/253 [00:33<01:30,  1.84it/s]batch_loss: 0.0000, loss: 0.0000 ||:  35%|###4      | 88/253 [00:33<01:24,  1.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  35%|###5      | 89/253 [00:34<01:39,  1.64it/s]batch_loss: 0.0000, loss: 0.0000 ||:  36%|###5      | 90/253 [00:34<01:23,  1.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:  36%|###5      | 91/253 [00:35<01:25,  1.89it/s]batch_loss: 0.0000, loss: 0.0000 ||:  36%|###6      | 92/253 [00:35<01:12,  2.21it/s]batch_loss: 0.0000, loss: 0.0000 ||:  37%|###6      | 93/253 [00:36<01:31,  1.75it/s]batch_loss: 0.0000, loss: 0.0000 ||:  37%|###7      | 94/253 [00:37<01:36,  1.65it/s]batch_loss: 0.0000, loss: 0.0000 ||:  38%|###7      | 95/253 [00:37<01:33,  1.68it/s]batch_loss: 0.0000, loss: 0.0000 ||:  38%|###7      | 96/253 [00:37<01:24,  1.86it/s]batch_loss: 0.0000, loss: 0.0000 ||:  38%|###8      | 97/253 [00:38<01:11,  2.18it/s]batch_loss: 0.0000, loss: 0.0000 ||:  39%|###8      | 98/253 [00:38<01:04,  2.42it/s]batch_loss: 0.0000, loss: 0.0000 ||:  39%|###9      | 99/253 [00:38<00:57,  2.69it/s]batch_loss: 0.0000, loss: 0.0000 ||:  40%|###9      | 100/253 [00:39<00:52,  2.92it/s]batch_loss: 0.0000, loss: 0.0000 ||:  40%|###9      | 101/253 [00:39<00:48,  3.11it/s]batch_loss: 0.0000, loss: 0.0000 ||:  41%|####      | 103/253 [00:39<00:31,  4.74it/s]batch_loss: 0.0000, loss: 0.0000 ||:  41%|####1     | 104/253 [00:39<00:38,  3.83it/s]batch_loss: 0.0000, loss: 0.0000 ||:  42%|####1     | 106/253 [00:40<00:32,  4.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  43%|####2     | 108/253 [00:40<00:33,  4.36it/s]batch_loss: 0.0000, loss: 0.0000 ||:  43%|####3     | 109/253 [00:41<00:34,  4.18it/s]batch_loss: 0.0000, loss: 0.0000 ||:  43%|####3     | 110/253 [00:41<00:45,  3.14it/s]batch_loss: 0.0000, loss: 0.0000 ||:  44%|####3     | 111/253 [00:42<00:53,  2.64it/s]batch_loss: 0.0000, loss: 0.0000 ||:  44%|####4     | 112/253 [00:42<00:44,  3.18it/s]batch_loss: 0.0000, loss: 0.0000 ||:  45%|####4     | 113/253 [00:42<00:42,  3.26it/s]batch_loss: 0.0000, loss: 0.0000 ||:  45%|####5     | 114/253 [00:43<00:48,  2.87it/s]batch_loss: 0.0000, loss: 0.0000 ||:  45%|####5     | 115/253 [00:43<00:40,  3.44it/s]batch_loss: 0.0000, loss: 0.0000 ||:  46%|####5     | 116/253 [00:43<00:39,  3.49it/s]batch_loss: 0.0000, loss: 0.0000 ||:  46%|####6     | 117/253 [00:44<00:49,  2.77it/s]batch_loss: 0.0000, loss: 0.0000 ||:  47%|####6     | 118/253 [00:44<00:45,  2.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:  47%|####7     | 119/253 [00:44<00:48,  2.79it/s]batch_loss: 0.0000, loss: 0.0000 ||:  47%|####7     | 120/253 [00:45<00:44,  3.02it/s]batch_loss: 0.0000, loss: 0.0000 ||:  48%|####8     | 122/253 [00:45<00:34,  3.78it/s]batch_loss: 0.0000, loss: 0.0000 ||:  49%|####9     | 124/253 [00:45<00:24,  5.32it/s]batch_loss: 0.0000, loss: 0.0000 ||:  49%|####9     | 125/253 [00:46<00:34,  3.72it/s]batch_loss: 0.0000, loss: 0.0000 ||:  50%|####9     | 126/253 [00:46<00:34,  3.72it/s]batch_loss: 0.0000, loss: 0.0000 ||:  51%|#####     | 128/253 [00:46<00:29,  4.29it/s]batch_loss: 0.0000, loss: 0.0000 ||:  51%|#####     | 129/253 [00:46<00:25,  4.89it/s]batch_loss: 0.0000, loss: 0.0000 ||:  51%|#####1    | 130/253 [00:47<00:40,  3.03it/s]batch_loss: 0.0000, loss: 0.0000 ||:  52%|#####1    | 131/253 [00:47<00:34,  3.59it/s]batch_loss: 0.0000, loss: 0.0000 ||:  52%|#####2    | 132/253 [00:47<00:33,  3.59it/s]batch_loss: 0.0000, loss: 0.0000 ||:  53%|#####2    | 134/253 [00:48<00:21,  5.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  53%|#####3    | 135/253 [00:48<00:39,  3.02it/s]batch_loss: 0.0000, loss: 0.0000 ||:  54%|#####4    | 137/253 [00:49<00:37,  3.11it/s]batch_loss: 0.0000, loss: 0.0000 ||:  55%|#####4    | 138/253 [00:49<00:35,  3.23it/s]batch_loss: 0.0000, loss: 0.0000 ||:  55%|#####4    | 139/253 [00:50<00:34,  3.30it/s]batch_loss: 0.0000, loss: 0.0000 ||:  56%|#####5    | 141/253 [00:50<00:31,  3.55it/s]batch_loss: 0.0000, loss: 0.0000 ||:  56%|#####6    | 142/253 [00:50<00:31,  3.56it/s]batch_loss: 0.0000, loss: 0.0000 ||:  57%|#####6    | 143/253 [00:51<00:41,  2.64it/s]batch_loss: 0.0000, loss: 0.0000 ||:  57%|#####6    | 144/253 [00:51<00:42,  2.59it/s]batch_loss: 0.0000, loss: 0.0000 ||:  57%|#####7    | 145/253 [00:52<00:42,  2.55it/s]batch_loss: 0.0000, loss: 0.0000 ||:  58%|#####7    | 146/253 [00:52<00:38,  2.80it/s]batch_loss: 0.0000, loss: 0.0000 ||:  58%|#####8    | 147/253 [00:52<00:35,  2.99it/s]batch_loss: 0.0000, loss: 0.0000 ||:  58%|#####8    | 148/253 [00:53<00:33,  3.16it/s]batch_loss: 0.0000, loss: 0.0000 ||:  59%|#####8    | 149/253 [00:53<00:31,  3.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:  59%|#####9    | 150/253 [00:53<00:36,  2.86it/s]batch_loss: 0.0000, loss: 0.0000 ||:  60%|#####9    | 151/253 [00:54<00:34,  2.99it/s]batch_loss: 0.0000, loss: 0.0000 ||:  60%|######    | 153/253 [00:54<00:22,  4.39it/s]batch_loss: 0.0000, loss: 0.0000 ||:  61%|######1   | 155/253 [00:54<00:16,  6.11it/s]batch_loss: 0.0000, loss: 0.0000 ||:  62%|######1   | 156/253 [00:54<00:15,  6.29it/s]batch_loss: 0.0000, loss: 0.0000 ||:  62%|######2   | 157/253 [00:54<00:18,  5.19it/s]batch_loss: 0.0000, loss: 0.0000 ||:  62%|######2   | 158/253 [00:55<00:20,  4.68it/s]batch_loss: 0.0000, loss: 0.0000 ||:  63%|######2   | 159/253 [00:55<00:25,  3.69it/s]batch_loss: 0.0000, loss: 0.0000 ||:  63%|######3   | 160/253 [00:56<00:28,  3.23it/s]batch_loss: 0.0000, loss: 0.0000 ||:  64%|######4   | 162/253 [00:56<00:19,  4.76it/s]batch_loss: 0.0000, loss: 0.0000 ||:  65%|######4   | 164/253 [00:56<00:19,  4.58it/s]batch_loss: 0.0000, loss: 0.0000 ||:  65%|######5   | 165/253 [00:56<00:20,  4.36it/s]batch_loss: 0.0000, loss: 0.0000 ||:  66%|######5   | 166/253 [00:57<00:23,  3.65it/s]batch_loss: 0.0000, loss: 0.0000 ||:  66%|######6   | 167/253 [00:57<00:30,  2.86it/s]batch_loss: 0.0000, loss: 0.0000 ||:  66%|######6   | 168/253 [00:58<00:25,  3.38it/s]batch_loss: 0.0000, loss: 0.0000 ||:  67%|######6   | 169/253 [00:58<00:24,  3.41it/s]batch_loss: 0.0000, loss: 0.0000 ||:  67%|######7   | 170/253 [00:58<00:27,  3.03it/s]batch_loss: 0.0000, loss: 0.0000 ||:  68%|######7   | 171/253 [00:59<00:25,  3.17it/s]batch_loss: 0.0000, loss: 0.0000 ||:  68%|######8   | 173/253 [00:59<00:27,  2.91it/s]batch_loss: 0.0000, loss: 0.0000 ||:  69%|######8   | 174/253 [01:00<00:26,  2.98it/s]batch_loss: 0.0000, loss: 0.0000 ||:  70%|######9   | 176/253 [01:00<00:25,  2.98it/s]batch_loss: 0.0000, loss: 0.0000 ||:  70%|######9   | 177/253 [01:01<00:26,  2.82it/s]batch_loss: 0.0000, loss: 0.0000 ||:  70%|#######   | 178/253 [01:01<00:25,  2.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  71%|#######   | 179/253 [01:01<00:21,  3.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  71%|#######1  | 180/253 [01:01<00:20,  3.51it/s]batch_loss: 0.0000, loss: 0.0000 ||:  72%|#######1  | 181/253 [01:02<00:16,  4.25it/s]batch_loss: 0.0000, loss: 0.0000 ||:  72%|#######1  | 182/253 [01:02<00:17,  4.01it/s]batch_loss: 0.0000, loss: 0.0000 ||:  72%|#######2  | 183/253 [01:02<00:24,  2.85it/s]batch_loss: 0.0000, loss: 0.0000 ||:  73%|#######2  | 184/253 [01:03<00:37,  1.86it/s]batch_loss: 0.0000, loss: 0.0000 ||:  73%|#######3  | 185/253 [01:04<00:27,  2.44it/s]batch_loss: 0.0000, loss: 0.0000 ||:  74%|#######3  | 186/253 [01:04<00:22,  3.03it/s]batch_loss: 0.0000, loss: 0.0000 ||:  74%|#######3  | 187/253 [01:04<00:23,  2.81it/s]batch_loss: 0.0000, loss: 0.0000 ||:  74%|#######4  | 188/253 [01:05<00:24,  2.65it/s]batch_loss: 0.0000, loss: 0.0000 ||:  75%|#######4  | 189/253 [01:05<00:19,  3.37it/s]batch_loss: 0.0000, loss: 0.0000 ||:  76%|#######5  | 192/253 [01:05<00:12,  4.97it/s]batch_loss: 0.0000, loss: 0.0000 ||:  76%|#######6  | 193/253 [01:06<00:24,  2.44it/s]batch_loss: 0.0000, loss: 0.0000 ||:  77%|#######6  | 194/253 [01:07<00:24,  2.42it/s]batch_loss: 0.0000, loss: 0.0000 ||:  77%|#######7  | 195/253 [01:07<00:26,  2.20it/s]batch_loss: 0.0000, loss: 0.0000 ||:  77%|#######7  | 196/253 [01:07<00:23,  2.43it/s]batch_loss: 0.0000, loss: 0.0000 ||:  78%|#######7  | 197/253 [01:08<00:18,  2.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  78%|#######8  | 198/253 [01:08<00:17,  3.07it/s]batch_loss: 0.0000, loss: 0.0000 ||:  79%|#######8  | 199/253 [01:08<00:14,  3.73it/s]batch_loss: 0.0000, loss: 0.0000 ||:  79%|#######9  | 200/253 [01:08<00:14,  3.73it/s]batch_loss: 0.0000, loss: 0.0000 ||:  79%|#######9  | 201/253 [01:09<00:14,  3.69it/s]batch_loss: 0.0000, loss: 0.0000 ||:  80%|#######9  | 202/253 [01:09<00:20,  2.48it/s]batch_loss: 0.0000, loss: 0.0000 ||:  80%|########  | 203/253 [01:10<00:18,  2.72it/s]batch_loss: 0.0000, loss: 0.0000 ||:  81%|########  | 204/253 [01:10<00:16,  2.91it/s]batch_loss: 0.0000, loss: 0.0000 ||:  81%|########1 | 205/253 [01:10<00:19,  2.41it/s]batch_loss: 0.0000, loss: 0.0000 ||:  81%|########1 | 206/253 [01:11<00:26,  1.80it/s]batch_loss: 0.0000, loss: 0.0000 ||:  82%|########1 | 207/253 [01:11<00:19,  2.35it/s]batch_loss: 0.0000, loss: 0.0000 ||:  82%|########2 | 208/253 [01:12<00:15,  2.99it/s]batch_loss: 0.0000, loss: 0.0000 ||:  83%|########2 | 209/253 [01:12<00:14,  3.14it/s]batch_loss: 0.0000, loss: 0.0000 ||:  83%|########3 | 210/253 [01:12<00:13,  3.23it/s]batch_loss: 0.0000, loss: 0.0000 ||:  83%|########3 | 211/253 [01:13<00:19,  2.14it/s]batch_loss: 0.0000, loss: 0.0000 ||:  84%|########3 | 212/253 [01:14<00:20,  2.04it/s]batch_loss: 0.0000, loss: 0.0000 ||:  84%|########4 | 213/253 [01:14<00:16,  2.36it/s]batch_loss: 0.0000, loss: 0.0000 ||:  85%|########4 | 214/253 [01:14<00:13,  3.00it/s]batch_loss: 0.0000, loss: 0.0000 ||:  85%|########4 | 215/253 [01:14<00:11,  3.19it/s]batch_loss: 0.0000, loss: 0.0000 ||:  85%|########5 | 216/253 [01:14<00:11,  3.33it/s]batch_loss: 0.0000, loss: 0.0000 ||:  86%|########5 | 217/253 [01:15<00:14,  2.40it/s]batch_loss: 0.0000, loss: 0.0000 ||:  86%|########6 | 218/253 [01:15<00:13,  2.66it/s]batch_loss: 0.0000, loss: 0.0000 ||:  87%|########6 | 219/253 [01:16<00:11,  2.92it/s]batch_loss: 0.0000, loss: 0.0000 ||:  87%|########6 | 220/253 [01:16<00:10,  3.12it/s]batch_loss: 0.0000, loss: 0.0000 ||:  88%|########7 | 222/253 [01:16<00:07,  3.88it/s]batch_loss: 0.0000, loss: 0.0000 ||:  88%|########8 | 223/253 [01:17<00:08,  3.40it/s]batch_loss: 0.0000, loss: 0.0000 ||:  89%|########8 | 224/253 [01:17<00:10,  2.71it/s]batch_loss: 0.0000, loss: 0.0000 ||:  89%|########8 | 225/253 [01:18<00:09,  2.93it/s]batch_loss: 0.0000, loss: 0.0000 ||:  90%|########9 | 227/253 [01:18<00:09,  2.80it/s]batch_loss: 0.0000, loss: 0.0000 ||:  90%|######### | 228/253 [01:19<00:08,  2.97it/s]batch_loss: 0.0000, loss: 0.0000 ||:  91%|######### | 229/253 [01:19<00:09,  2.57it/s]batch_loss: 0.0000, loss: 0.0000 ||:  91%|#########1| 231/253 [01:20<00:07,  3.09it/s]batch_loss: 0.0000, loss: 0.0000 ||:  92%|#########1| 232/253 [01:20<00:07,  2.89it/s]batch_loss: 0.0000, loss: 0.0000 ||:  92%|#########2| 234/253 [01:21<00:06,  2.75it/s]batch_loss: 0.0000, loss: 0.0000 ||:  93%|#########2| 235/253 [01:21<00:05,  3.25it/s]batch_loss: 0.0000, loss: 0.0000 ||:  93%|#########3| 236/253 [01:22<00:07,  2.29it/s]batch_loss: 0.0000, loss: 0.0000 ||:  94%|#########3| 237/253 [01:22<00:06,  2.53it/s]batch_loss: 0.0000, loss: 0.0000 ||:  94%|#########4| 238/253 [01:22<00:05,  2.74it/s]batch_loss: 0.0000, loss: 0.0000 ||:  94%|#########4| 239/253 [01:23<00:05,  2.39it/s]batch_loss: 0.0000, loss: 0.0000 ||:  95%|#########4| 240/253 [01:24<00:07,  1.74it/s]batch_loss: 0.0000, loss: 0.0000 ||:  95%|#########5| 241/253 [01:24<00:05,  2.05it/s]batch_loss: 0.0000, loss: 0.0000 ||:  96%|#########5| 242/253 [01:24<00:04,  2.36it/s]batch_loss: 0.0000, loss: 0.0000 ||:  96%|#########6| 243/253 [01:25<00:04,  2.36it/s]batch_loss: 0.0000, loss: 0.0000 ||:  96%|#########6| 244/253 [01:25<00:03,  2.63it/s]batch_loss: 0.0000, loss: 0.0000 ||:  97%|#########6| 245/253 [01:26<00:03,  2.30it/s]batch_loss: 0.0000, loss: 0.0000 ||:  97%|#########7| 246/253 [01:27<00:03,  1.76it/s]batch_loss: 0.0000, loss: 0.0000 ||:  98%|#########7| 247/253 [01:27<00:03,  1.93it/s]batch_loss: 0.0000, loss: 0.0000 ||:  98%|#########8| 248/253 [01:27<00:02,  2.25it/s]batch_loss: 0.0000, loss: 0.0000 ||:  98%|#########8| 249/253 [01:28<00:02,  1.93it/s]batch_loss: 0.0000, loss: 0.0000 ||:  99%|#########8| 250/253 [01:28<00:01,  2.26it/s]batch_loss: 0.0000, loss: 0.0000 ||:  99%|#########9| 251/253 [01:28<00:00,  2.56it/s]batch_loss: 0.0000, loss: 0.0000 ||: 100%|#########9| 252/253 [01:29<00:00,  2.84it/s]batch_loss: 0.0000, loss: 0.0000 ||: 100%|##########| 253/253 [01:29<00:00,  2.83it/s]
2023-11-07 18:40:00,140 - INFO - allennlp.training.callbacks.console_logger -                         Training |  Validation
2023-11-07 18:40:00,140 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB     |  1451.410  |       N/A
2023-11-07 18:40:00,140 - INFO - allennlp.training.callbacks.console_logger - iterx_famus_slot_f1 |     0.000  |     0.000
2023-11-07 18:40:00,140 - INFO - allennlp.training.callbacks.console_logger - iterx_famus_slot_p  |     0.000  |     0.000
2023-11-07 18:40:00,140 - INFO - allennlp.training.callbacks.console_logger - iterx_famus_slot_r  |     0.000  |     0.000
2023-11-07 18:40:00,140 - INFO - allennlp.training.callbacks.console_logger - loss                |     3.469  |     0.000
2023-11-07 18:40:00,140 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB  |  8647.445  |       N/A
2023-11-07 18:40:05,352 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:05:42.007486
2023-11-07 18:40:05,353 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 13:56:22
2023-11-07 18:40:05,353 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/149
2023-11-07 18:40:05,353 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 8.4G
2023-11-07 18:40:05,354 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 13G
2023-11-07 18:40:05,356 - INFO - allennlp.training.gradient_descent_trainer - Training
  0%|          | 0/759 [00:00<?, ?it/s]batch_loss: 0.5418, loss: 0.5418 ||:   0%|          | 1/759 [00:00<01:23,  9.04it/s]batch_loss: 0.3659, loss: 0.4538 ||:   0%|          | 2/759 [00:00<03:10,  3.96it/s]batch_loss: 0.2207, loss: 0.3761 ||:   0%|          | 3/759 [00:00<02:44,  4.58it/s]batch_loss: 0.4368, loss: 0.3738 ||:   1%|          | 5/759 [00:00<01:53,  6.62it/s]batch_loss: 0.2614, loss: 0.3550 ||:   1%|          | 6/759 [00:00<01:48,  6.95it/s]batch_loss: 0.3156, loss: 0.3494 ||:   1%|          | 7/759 [00:01<02:31,  4.98it/s]batch_loss: 0.1904, loss: 0.3295 ||:   1%|1         | 8/759 [00:01<02:34,  4.85it/s]batch_loss: 0.0834, loss: 0.3022 ||:   1%|1         | 9/759 [00:02<04:19,  2.89it/s]batch_loss: 0.0873, loss: 0.2807 ||:   1%|1         | 10/759 [00:02<04:58,  2.51it/s]batch_loss: 0.2158, loss: 0.2748 ||:   1%|1         | 11/759 [00:03<04:38,  2.69it/s]batch_loss: 0.1627, loss: 0.2655 ||:   2%|1         | 12/759 [00:03<04:34,  2.72it/s]batch_loss: 0.0960, loss: 0.2826 ||:   2%|1         | 14/759 [00:04<05:51,  2.12it/s]batch_loss: 0.1704, loss: 0.2752 ||:   2%|1         | 15/759 [00:04<05:24,  2.30it/s]batch_loss: 0.1958, loss: 0.2702 ||:   2%|2         | 16/759 [00:05<04:59,  2.48it/s]batch_loss: 0.0648, loss: 0.2581 ||:   2%|2         | 17/759 [00:05<04:16,  2.89it/s]batch_loss: 0.1104, loss: 0.2499 ||:   2%|2         | 18/759 [00:05<04:47,  2.58it/s]batch_loss: 0.1913, loss: 0.2468 ||:   3%|2         | 19/759 [00:06<03:58,  3.10it/s]batch_loss: 0.2981, loss: 0.2494 ||:   3%|2         | 20/759 [00:06<03:54,  3.15it/s]batch_loss: 0.2234, loss: 0.2482 ||:   3%|2         | 21/759 [00:06<04:27,  2.75it/s]batch_loss: 0.1731, loss: 0.2447 ||:   3%|2         | 22/759 [00:07<04:18,  2.85it/s]batch_loss: 0.0963, loss: 0.2383 ||:   3%|3         | 23/759 [00:07<04:18,  2.84it/s]batch_loss: 0.5239, loss: 0.2502 ||:   3%|3         | 24/759 [00:07<03:33,  3.45it/s]batch_loss: 0.3279, loss: 0.2595 ||:   3%|3         | 26/759 [00:07<02:24,  5.06it/s]batch_loss: 0.1131, loss: 0.2541 ||:   4%|3         | 27/759 [00:08<02:46,  4.38it/s]batch_loss: 0.0987, loss: 0.2485 ||:   4%|3         | 28/759 [00:08<02:34,  4.74it/s]batch_loss: 0.2383, loss: 0.2482 ||:   4%|3         | 29/759 [00:08<02:16,  5.33it/s]batch_loss: 0.2356, loss: 0.2478 ||:   4%|3         | 30/759 [00:08<02:44,  4.43it/s]batch_loss: 0.1268, loss: 0.2439 ||:   4%|4         | 31/759 [00:09<03:07,  3.87it/s]batch_loss: 0.5873, loss: 0.2546 ||:   4%|4         | 32/759 [00:09<02:36,  4.65it/s]batch_loss: 0.2724, loss: 0.2551 ||:   4%|4         | 33/759 [00:09<02:16,  5.33it/s]batch_loss: 0.1534, loss: 0.2522 ||:   4%|4         | 34/759 [00:09<03:50,  3.14it/s]batch_loss: 0.2702, loss: 0.2527 ||:   5%|4         | 35/759 [00:10<03:18,  3.64it/s]batch_loss: 0.3582, loss: 0.2556 ||:   5%|4         | 36/759 [00:10<02:46,  4.34it/s]batch_loss: 0.2408, loss: 0.2597 ||:   5%|5         | 38/759 [00:10<02:05,  5.73it/s]batch_loss: 0.1981, loss: 0.2692 ||:   5%|5         | 40/759 [00:10<01:48,  6.62it/s]batch_loss: 0.1396, loss: 0.2660 ||:   5%|5         | 41/759 [00:11<02:18,  5.18it/s]batch_loss: 0.4620, loss: 0.2707 ||:   6%|5         | 42/759 [00:11<02:39,  4.49it/s]batch_loss: 0.3478, loss: 0.2725 ||:   6%|5         | 43/759 [00:11<02:21,  5.05it/s]batch_loss: 0.5740, loss: 0.2793 ||:   6%|5         | 44/759 [00:11<02:03,  5.79it/s]batch_loss: 0.1335, loss: 0.2761 ||:   6%|5         | 45/759 [00:12<03:06,  3.84it/s]batch_loss: 0.0577, loss: 0.2713 ||:   6%|6         | 46/759 [00:12<04:04,  2.91it/s]batch_loss: 0.1620, loss: 0.2690 ||:   6%|6         | 47/759 [00:12<03:28,  3.41it/s]batch_loss: 0.3155, loss: 0.2753 ||:   6%|6         | 49/759 [00:13<02:32,  4.66it/s]batch_loss: 0.1583, loss: 0.2730 ||:   7%|6         | 50/759 [00:13<02:23,  4.93it/s]batch_loss: 0.1929, loss: 0.2714 ||:   7%|6         | 51/759 [00:13<02:14,  5.28it/s]batch_loss: 0.0760, loss: 0.2798 ||:   7%|6         | 53/759 [00:13<02:18,  5.09it/s]batch_loss: 0.2543, loss: 0.2793 ||:   7%|7         | 54/759 [00:14<02:39,  4.43it/s]batch_loss: 0.2349, loss: 0.2785 ||:   7%|7         | 55/759 [00:14<02:57,  3.97it/s]batch_loss: 0.1995, loss: 0.2771 ||:   7%|7         | 56/759 [00:14<03:08,  3.73it/s]batch_loss: 0.3074, loss: 0.2776 ||:   8%|7         | 57/759 [00:14<02:49,  4.15it/s]batch_loss: 0.2377, loss: 0.2769 ||:   8%|7         | 58/759 [00:15<02:24,  4.85it/s]batch_loss: 0.2711, loss: 0.2776 ||:   8%|7         | 60/759 [00:15<02:24,  4.83it/s]batch_loss: 0.1883, loss: 0.2761 ||:   8%|8         | 61/759 [00:15<02:43,  4.26it/s]batch_loss: 0.1594, loss: 0.2742 ||:   8%|8         | 62/759 [00:16<02:56,  3.95it/s]batch_loss: 0.1797, loss: 0.2727 ||:   8%|8         | 63/759 [00:16<03:06,  3.73it/s]batch_loss: 0.0661, loss: 0.2695 ||:   8%|8         | 64/759 [00:16<04:04,  2.84it/s]batch_loss: 0.2661, loss: 0.2694 ||:   9%|8         | 65/759 [00:17<03:33,  3.25it/s]batch_loss: 0.0836, loss: 0.2666 ||:   9%|8         | 66/759 [00:17<04:49,  2.39it/s]batch_loss: 0.2753, loss: 0.2668 ||:   9%|8         | 67/759 [00:17<03:50,  3.01it/s]batch_loss: 0.2879, loss: 0.2671 ||:   9%|8         | 68/759 [00:18<03:46,  3.05it/s]batch_loss: 0.0807, loss: 0.2644 ||:   9%|9         | 69/759 [00:18<04:23,  2.62it/s]batch_loss: 0.0894, loss: 0.2619 ||:   9%|9         | 70/759 [00:19<06:03,  1.90it/s]batch_loss: 0.1836, loss: 0.2608 ||:   9%|9         | 71/759 [00:20<05:29,  2.08it/s]batch_loss: 0.0909, loss: 0.2584 ||:   9%|9         | 72/759 [00:20<06:16,  1.82it/s]batch_loss: 0.3633, loss: 0.2598 ||:  10%|9         | 73/759 [00:20<04:54,  2.33it/s]batch_loss: 0.1056, loss: 0.2578 ||:  10%|9         | 74/759 [00:21<04:43,  2.42it/s]batch_loss: 0.1677, loss: 0.2566 ||:  10%|9         | 75/759 [00:21<03:54,  2.92it/s]batch_loss: 0.1534, loss: 0.2552 ||:  10%|#         | 76/759 [00:21<03:56,  2.88it/s]batch_loss: 0.2312, loss: 0.2549 ||:  10%|#         | 77/759 [00:22<03:51,  2.95it/s]batch_loss: 0.3976, loss: 0.2625 ||:  10%|#         | 79/759 [00:22<02:26,  4.65it/s]batch_loss: 0.1712, loss: 0.2613 ||:  11%|#         | 80/759 [00:22<02:45,  4.10it/s]batch_loss: 0.0729, loss: 0.2590 ||:  11%|#         | 81/759 [00:23<05:05,  2.22it/s]batch_loss: 0.0982, loss: 0.2570 ||:  11%|#         | 82/759 [00:23<04:43,  2.38it/s]batch_loss: 0.2900, loss: 0.2574 ||:  11%|#         | 83/759 [00:24<03:48,  2.96it/s]batch_loss: 0.1823, loss: 0.2566 ||:  11%|#1        | 84/759 [00:24<03:43,  3.02it/s]batch_loss: 0.1418, loss: 0.2552 ||:  11%|#1        | 85/759 [00:24<04:14,  2.65it/s]batch_loss: 0.0604, loss: 0.2535 ||:  11%|#1        | 87/759 [00:25<04:13,  2.65it/s]batch_loss: 0.1071, loss: 0.2519 ||:  12%|#1        | 88/759 [00:26<04:29,  2.49it/s]batch_loss: 0.0583, loss: 0.2519 ||:  12%|#1        | 90/759 [00:27<05:06,  2.18it/s]batch_loss: 0.0640, loss: 0.2498 ||:  12%|#1        | 91/759 [00:27<05:11,  2.14it/s]batch_loss: 0.0589, loss: 0.2477 ||:  12%|#2        | 92/759 [00:28<05:15,  2.11it/s]batch_loss: 0.6065, loss: 0.2538 ||:  12%|#2        | 94/759 [00:28<03:33,  3.12it/s]batch_loss: 0.2316, loss: 0.2536 ||:  13%|#2        | 95/759 [00:28<03:08,  3.52it/s]batch_loss: 0.3329, loss: 0.2579 ||:  13%|#2        | 97/759 [00:28<02:21,  4.69it/s]batch_loss: 0.2959, loss: 0.2583 ||:  13%|#2        | 98/759 [00:29<02:39,  4.14it/s]batch_loss: 0.3488, loss: 0.2593 ||:  13%|#3        | 100/759 [00:29<02:01,  5.43it/s]batch_loss: 0.1793, loss: 0.2642 ||:  13%|#3        | 102/759 [00:29<01:38,  6.66it/s]batch_loss: 0.0521, loss: 0.2622 ||:  14%|#3        | 103/759 [00:30<03:26,  3.17it/s]batch_loss: 0.4196, loss: 0.2637 ||:  14%|#3        | 104/759 [00:30<03:28,  3.14it/s]batch_loss: 0.1820, loss: 0.2629 ||:  14%|#3        | 105/759 [00:31<03:30,  3.11it/s]batch_loss: 0.0918, loss: 0.2613 ||:  14%|#3        | 106/759 [00:31<03:30,  3.10it/s]batch_loss: 0.0919, loss: 0.2597 ||:  14%|#4        | 107/759 [00:31<03:33,  3.06it/s]batch_loss: 0.1593, loss: 0.2588 ||:  14%|#4        | 108/759 [00:31<02:57,  3.67it/s]batch_loss: 0.1877, loss: 0.2581 ||:  14%|#4        | 109/759 [00:32<03:38,  2.97it/s]batch_loss: 0.1293, loss: 0.2570 ||:  14%|#4        | 110/759 [00:32<04:08,  2.61it/s]batch_loss: 0.9002, loss: 0.2628 ||:  15%|#4        | 111/759 [00:33<03:16,  3.30it/s]batch_loss: 0.0740, loss: 0.2611 ||:  15%|#4        | 112/759 [00:33<04:26,  2.43it/s]batch_loss: 0.2171, loss: 0.2604 ||:  15%|#5        | 114/759 [00:34<03:18,  3.26it/s]batch_loss: 0.0958, loss: 0.2590 ||:  15%|#5        | 115/759 [00:35<05:19,  2.02it/s]batch_loss: 0.1802, loss: 0.2583 ||:  15%|#5        | 116/759 [00:35<04:48,  2.23it/s]batch_loss: 0.2004, loss: 0.2578 ||:  15%|#5        | 117/759 [00:35<03:57,  2.71it/s]batch_loss: 0.6751, loss: 0.2613 ||:  16%|#5        | 118/759 [00:35<03:16,  3.26it/s]batch_loss: 0.0708, loss: 0.2597 ||:  16%|#5        | 119/759 [00:36<05:33,  1.92it/s]batch_loss: 0.0655, loss: 0.2581 ||:  16%|#5        | 120/759 [00:37<05:33,  1.91it/s]batch_loss: 0.2294, loss: 0.2579 ||:  16%|#5        | 121/759 [00:37<04:54,  2.17it/s]batch_loss: 0.1335, loss: 0.2569 ||:  16%|#6        | 122/759 [00:37<03:59,  2.66it/s]batch_loss: 0.1181, loss: 0.2557 ||:  16%|#6        | 123/759 [00:38<04:16,  2.48it/s]batch_loss: 0.1401, loss: 0.2548 ||:  16%|#6        | 124/759 [00:38<04:28,  2.36it/s]batch_loss: 0.1493, loss: 0.2540 ||:  16%|#6        | 125/759 [00:39<04:41,  2.25it/s]batch_loss: 0.1872, loss: 0.2534 ||:  17%|#6        | 126/759 [00:39<04:14,  2.49it/s]batch_loss: 0.3242, loss: 0.2540 ||:  17%|#6        | 127/759 [00:39<03:59,  2.63it/s]batch_loss: 0.0594, loss: 0.2525 ||:  17%|#6        | 128/759 [00:40<05:03,  2.08it/s]batch_loss: 0.0889, loss: 0.2512 ||:  17%|#6        | 129/759 [00:40<04:33,  2.31it/s]batch_loss: 0.0612, loss: 0.2497 ||:  17%|#7        | 130/759 [00:41<04:51,  2.16it/s]batch_loss: 0.1975, loss: 0.2493 ||:  17%|#7        | 131/759 [00:41<04:27,  2.35it/s]batch_loss: 0.1386, loss: 0.2485 ||:  17%|#7        | 132/759 [00:42<04:08,  2.52it/s]batch_loss: 0.0844, loss: 0.2473 ||:  18%|#7        | 133/759 [00:42<03:59,  2.61it/s]batch_loss: 0.4703, loss: 0.2544 ||:  18%|#7        | 135/759 [00:42<02:29,  4.18it/s]batch_loss: 0.0505, loss: 0.2529 ||:  18%|#7        | 136/759 [00:43<04:03,  2.56it/s]batch_loss: 0.0956, loss: 0.2558 ||:  18%|#8        | 138/759 [00:43<03:12,  3.22it/s]batch_loss: 0.1211, loss: 0.2549 ||:  18%|#8        | 139/759 [00:44<03:11,  3.24it/s]batch_loss: 0.2886, loss: 0.2551 ||:  18%|#8        | 140/759 [00:44<03:09,  3.26it/s]batch_loss: 0.3190, loss: 0.2556 ||:  19%|#8        | 141/759 [00:44<02:37,  3.92it/s]batch_loss: 0.0994, loss: 0.2545 ||:  19%|#8        | 142/759 [00:44<02:45,  3.73it/s]batch_loss: 0.2532, loss: 0.2545 ||:  19%|#8        | 143/759 [00:45<02:28,  4.16it/s]batch_loss: 0.3501, loss: 0.2562 ||:  19%|#9        | 145/759 [00:45<02:19,  4.40it/s]batch_loss: 0.2799, loss: 0.2563 ||:  19%|#9        | 146/759 [00:45<02:02,  5.01it/s]batch_loss: 0.0626, loss: 0.2550 ||:  19%|#9        | 147/759 [00:46<04:22,  2.33it/s]batch_loss: 0.0572, loss: 0.2537 ||:  19%|#9        | 148/759 [00:47<06:50,  1.49it/s]batch_loss: 0.0837, loss: 0.2585 ||:  20%|#9        | 150/759 [00:48<05:21,  1.89it/s]batch_loss: 0.0971, loss: 0.2574 ||:  20%|#9        | 151/759 [00:49<04:52,  2.08it/s]batch_loss: 0.2414, loss: 0.2591 ||:  20%|##        | 153/759 [00:49<03:42,  2.73it/s]batch_loss: 0.2380, loss: 0.2590 ||:  20%|##        | 154/759 [00:49<03:33,  2.83it/s]batch_loss: 0.2820, loss: 0.2591 ||:  20%|##        | 155/759 [00:49<03:08,  3.21it/s]batch_loss: 0.0965, loss: 0.2581 ||:  21%|##        | 156/759 [00:50<03:36,  2.79it/s]batch_loss: 0.1700, loss: 0.2575 ||:  21%|##        | 157/759 [00:50<02:59,  3.36it/s]batch_loss: 0.1579, loss: 0.2569 ||:  21%|##        | 158/759 [00:50<02:37,  3.81it/s]batch_loss: 0.2090, loss: 0.2566 ||:  21%|##        | 159/759 [00:51<02:49,  3.53it/s]batch_loss: 0.1365, loss: 0.2558 ||:  21%|##1       | 160/759 [00:51<03:29,  2.86it/s]batch_loss: 0.1827, loss: 0.2554 ||:  21%|##1       | 161/759 [00:51<02:55,  3.40it/s]batch_loss: 0.1196, loss: 0.2545 ||:  21%|##1       | 162/759 [00:52<03:07,  3.18it/s]batch_loss: 0.0780, loss: 0.2535 ||:  21%|##1       | 163/759 [00:52<02:42,  3.66it/s]batch_loss: 0.0569, loss: 0.2523 ||:  22%|##1       | 164/759 [00:53<04:34,  2.17it/s]batch_loss: 0.6293, loss: 0.2553 ||:  22%|##1       | 166/759 [00:53<03:02,  3.25it/s]batch_loss: 0.0477, loss: 0.2540 ||:  22%|##2       | 167/759 [00:54<04:57,  1.99it/s]batch_loss: 0.2524, loss: 0.2540 ||:  22%|##2       | 168/759 [00:54<04:12,  2.34it/s]batch_loss: 0.1158, loss: 0.2532 ||:  22%|##2       | 169/759 [00:55<05:04,  1.94it/s]batch_loss: 0.1044, loss: 0.2523 ||:  22%|##2       | 170/759 [00:55<04:42,  2.09it/s]batch_loss: 0.2226, loss: 0.2522 ||:  23%|##2       | 171/759 [00:56<03:49,  2.56it/s]batch_loss: 0.1266, loss: 0.2514 ||:  23%|##2       | 172/759 [00:56<03:44,  2.61it/s]batch_loss: 0.0700, loss: 0.2504 ||:  23%|##2       | 173/759 [00:56<03:35,  2.72it/s]batch_loss: 0.1230, loss: 0.2496 ||:  23%|##2       | 174/759 [00:57<03:34,  2.73it/s]batch_loss: 0.0915, loss: 0.2487 ||:  23%|##3       | 175/759 [00:57<03:57,  2.46it/s]batch_loss: 0.0830, loss: 0.2478 ||:  23%|##3       | 176/759 [00:57<03:40,  2.65it/s]batch_loss: 0.1459, loss: 0.2472 ||:  23%|##3       | 177/759 [00:58<03:31,  2.75it/s]batch_loss: 0.0769, loss: 0.2463 ||:  23%|##3       | 178/759 [00:58<03:52,  2.49it/s]batch_loss: 0.8741, loss: 0.2498 ||:  24%|##3       | 179/759 [00:59<03:36,  2.68it/s]batch_loss: 0.1850, loss: 0.2494 ||:  24%|##3       | 180/759 [00:59<03:23,  2.84it/s]batch_loss: 0.1668, loss: 0.2490 ||:  24%|##3       | 181/759 [00:59<03:23,  2.84it/s]batch_loss: 0.0891, loss: 0.2481 ||:  24%|##3       | 182/759 [00:59<03:20,  2.88it/s]batch_loss: 0.0594, loss: 0.2470 ||:  24%|##4       | 183/759 [01:00<03:17,  2.91it/s]batch_loss: 0.3260, loss: 0.2475 ||:  24%|##4       | 184/759 [01:00<02:48,  3.41it/s]batch_loss: 0.2300, loss: 0.2474 ||:  24%|##4       | 185/759 [01:00<02:56,  3.26it/s]batch_loss: 0.0718, loss: 0.2464 ||:  25%|##4       | 186/759 [01:01<04:00,  2.39it/s]batch_loss: 0.1269, loss: 0.2458 ||:  25%|##4       | 187/759 [01:01<03:09,  3.02it/s]batch_loss: 0.0754, loss: 0.2449 ||:  25%|##4       | 188/759 [01:02<03:24,  2.80it/s]batch_loss: 0.1548, loss: 0.2444 ||:  25%|##4       | 189/759 [01:02<02:59,  3.17it/s]batch_loss: 0.3149, loss: 0.2448 ||:  25%|##5       | 190/759 [01:02<02:32,  3.73it/s]batch_loss: 0.1002, loss: 0.2440 ||:  25%|##5       | 191/759 [01:02<03:19,  2.85it/s]batch_loss: 0.3055, loss: 0.2443 ||:  25%|##5       | 192/759 [01:03<02:50,  3.33it/s]batch_loss: 0.2026, loss: 0.2441 ||:  25%|##5       | 193/759 [01:03<02:20,  4.04it/s]batch_loss: 0.1397, loss: 0.2436 ||:  26%|##5       | 194/759 [01:03<02:32,  3.70it/s]batch_loss: 0.4522, loss: 0.2447 ||:  26%|##5       | 195/759 [01:03<02:11,  4.29it/s]batch_loss: 0.5931, loss: 0.2464 ||:  26%|##5       | 196/759 [01:03<01:51,  5.03it/s]batch_loss: 0.1394, loss: 0.2459 ||:  26%|##5       | 197/759 [01:04<02:39,  3.53it/s]batch_loss: 0.2265, loss: 0.2458 ||:  26%|##6       | 198/759 [01:04<02:43,  3.42it/s]batch_loss: 0.2330, loss: 0.2457 ||:  26%|##6       | 199/759 [01:04<02:16,  4.11it/s]batch_loss: 0.0589, loss: 0.2448 ||:  26%|##6       | 200/759 [01:05<03:26,  2.71it/s]batch_loss: 0.1279, loss: 0.2442 ||:  26%|##6       | 201/759 [01:05<03:21,  2.77it/s]batch_loss: 0.3045, loss: 0.2483 ||:  27%|##6       | 203/759 [01:06<02:39,  3.50it/s]batch_loss: 0.1064, loss: 0.2498 ||:  27%|##7       | 205/759 [01:06<02:20,  3.95it/s]batch_loss: 0.2026, loss: 0.2495 ||:  27%|##7       | 206/759 [01:06<02:29,  3.71it/s]batch_loss: 0.4264, loss: 0.2504 ||:  27%|##7       | 207/759 [01:07<02:08,  4.30it/s]batch_loss: 0.3460, loss: 0.2508 ||:  27%|##7       | 208/759 [01:07<02:21,  3.90it/s]batch_loss: 0.0810, loss: 0.2500 ||:  28%|##7       | 209/759 [01:07<02:55,  3.13it/s]batch_loss: 0.0686, loss: 0.2492 ||:  28%|##7       | 210/759 [01:08<03:52,  2.36it/s]batch_loss: 0.2140, loss: 0.2490 ||:  28%|##7       | 212/759 [01:08<02:29,  3.66it/s]batch_loss: 0.2493, loss: 0.2490 ||:  28%|##8       | 213/759 [01:08<02:08,  4.24it/s]batch_loss: 0.0950, loss: 0.2482 ||:  28%|##8       | 214/759 [01:09<02:46,  3.28it/s]batch_loss: 0.3805, loss: 0.2510 ||:  28%|##8       | 216/759 [01:09<02:20,  3.85it/s]batch_loss: 0.1698, loss: 0.2507 ||:  29%|##8       | 217/759 [01:10<02:29,  3.63it/s]batch_loss: 0.2709, loss: 0.2508 ||:  29%|##8       | 218/759 [01:10<02:10,  4.15it/s]batch_loss: 0.2459, loss: 0.2507 ||:  29%|##8       | 219/759 [01:10<01:57,  4.58it/s]batch_loss: 0.2259, loss: 0.2506 ||:  29%|##8       | 220/759 [01:10<01:43,  5.22it/s]batch_loss: 0.0811, loss: 0.2499 ||:  29%|##9       | 221/759 [01:11<03:47,  2.37it/s]batch_loss: 0.2251, loss: 0.2497 ||:  29%|##9       | 222/759 [01:11<03:05,  2.89it/s]batch_loss: 0.2154, loss: 0.2496 ||:  29%|##9       | 223/759 [01:11<02:36,  3.43it/s]batch_loss: 0.0812, loss: 0.2488 ||:  30%|##9       | 224/759 [01:12<02:43,  3.27it/s]batch_loss: 0.0619, loss: 0.2480 ||:  30%|##9       | 225/759 [01:12<03:46,  2.36it/s]batch_loss: 0.4280, loss: 0.2488 ||:  30%|##9       | 226/759 [01:13<03:35,  2.47it/s]batch_loss: 0.2232, loss: 0.2487 ||:  30%|##9       | 227/759 [01:13<03:49,  2.32it/s]batch_loss: 0.1024, loss: 0.2480 ||:  30%|###       | 228/759 [01:14<03:33,  2.49it/s]batch_loss: 0.1072, loss: 0.2474 ||:  30%|###       | 229/759 [01:14<03:44,  2.36it/s]batch_loss: 0.3737, loss: 0.2480 ||:  30%|###       | 230/759 [01:14<02:54,  3.04it/s]batch_loss: 0.0542, loss: 0.2471 ||:  30%|###       | 231/759 [01:15<03:20,  2.63it/s]batch_loss: 0.2671, loss: 0.2472 ||:  31%|###       | 232/759 [01:15<02:41,  3.26it/s]batch_loss: 0.1551, loss: 0.2468 ||:  31%|###       | 233/759 [01:15<02:44,  3.19it/s]batch_loss: 0.1065, loss: 0.2480 ||:  31%|###       | 235/759 [01:16<02:17,  3.81it/s]batch_loss: 0.2714, loss: 0.2481 ||:  31%|###1      | 236/759 [01:16<01:59,  4.39it/s]batch_loss: 0.1138, loss: 0.2475 ||:  31%|###1      | 238/759 [01:16<02:09,  4.02it/s]batch_loss: 0.0710, loss: 0.2468 ||:  31%|###1      | 239/759 [01:17<03:19,  2.60it/s]batch_loss: 0.3577, loss: 0.2473 ||:  32%|###1      | 240/759 [01:17<02:53,  2.99it/s]batch_loss: 0.1526, loss: 0.2469 ||:  32%|###1      | 241/759 [01:18<02:56,  2.93it/s]batch_loss: 0.2285, loss: 0.2467 ||:  32%|###2      | 243/759 [01:18<02:01,  4.24it/s]batch_loss: 0.2093, loss: 0.2465 ||:  32%|###2      | 244/759 [01:18<01:54,  4.51it/s]batch_loss: 0.1390, loss: 0.2461 ||:  32%|###2      | 245/759 [01:18<02:10,  3.94it/s]batch_loss: 0.1586, loss: 0.2457 ||:  32%|###2      | 246/759 [01:19<02:24,  3.54it/s]batch_loss: 0.0691, loss: 0.2450 ||:  33%|###2      | 247/759 [01:19<02:50,  3.00it/s]batch_loss: 0.2106, loss: 0.2449 ||:  33%|###2      | 248/759 [01:19<02:49,  3.01it/s]batch_loss: 0.0447, loss: 0.2441 ||:  33%|###2      | 249/759 [01:21<05:08,  1.65it/s]batch_loss: 0.0683, loss: 0.2450 ||:  33%|###3      | 251/759 [01:21<03:36,  2.34it/s]batch_loss: 0.0662, loss: 0.2443 ||:  33%|###3      | 252/759 [01:21<03:25,  2.47it/s]batch_loss: 0.0863, loss: 0.2436 ||:  33%|###3      | 253/759 [01:22<03:16,  2.58it/s]batch_loss: 0.0610, loss: 0.2429 ||:  33%|###3      | 254/759 [01:23<03:56,  2.14it/s]batch_loss: 0.3046, loss: 0.2432 ||:  34%|###3      | 255/759 [01:23<03:07,  2.68it/s]batch_loss: 0.5085, loss: 0.2442 ||:  34%|###3      | 256/759 [01:23<02:38,  3.18it/s]batch_loss: 0.1100, loss: 0.2437 ||:  34%|###3      | 257/759 [01:23<02:44,  3.06it/s]batch_loss: 0.1363, loss: 0.2433 ||:  34%|###3      | 258/759 [01:23<02:11,  3.81it/s]batch_loss: 0.3945, loss: 0.2438 ||:  34%|###4      | 260/759 [01:24<01:40,  4.95it/s]batch_loss: 0.2447, loss: 0.2453 ||:  35%|###4      | 262/759 [01:24<01:17,  6.42it/s]batch_loss: 0.2718, loss: 0.2465 ||:  35%|###4      | 264/759 [01:24<01:13,  6.75it/s]batch_loss: 0.3118, loss: 0.2468 ||:  35%|###4      | 265/759 [01:24<01:13,  6.71it/s]batch_loss: 0.1753, loss: 0.2465 ||:  35%|###5      | 266/759 [01:24<01:34,  5.21it/s]batch_loss: 0.4450, loss: 0.2473 ||:  35%|###5      | 267/759 [01:25<01:24,  5.83it/s]batch_loss: 0.0503, loss: 0.2465 ||:  35%|###5      | 268/759 [01:25<02:55,  2.80it/s]batch_loss: 0.1848, loss: 0.2463 ||:  35%|###5      | 269/759 [01:26<02:30,  3.25it/s]batch_loss: 0.1330, loss: 0.2459 ||:  36%|###5      | 270/759 [01:26<02:36,  3.12it/s]batch_loss: 0.1544, loss: 0.2455 ||:  36%|###5      | 271/759 [01:26<02:13,  3.65it/s]batch_loss: 0.0894, loss: 0.2450 ||:  36%|###5      | 272/759 [01:26<02:19,  3.48it/s]batch_loss: 0.0560, loss: 0.2443 ||:  36%|###5      | 273/759 [01:28<04:17,  1.88it/s]batch_loss: 0.4027, loss: 0.2449 ||:  36%|###6      | 274/759 [01:28<03:46,  2.14it/s]batch_loss: 0.1296, loss: 0.2444 ||:  36%|###6      | 275/759 [01:28<03:27,  2.33it/s]batch_loss: 0.2329, loss: 0.2444 ||:  36%|###6      | 276/759 [01:29<03:16,  2.46it/s]batch_loss: 0.0874, loss: 0.2438 ||:  36%|###6      | 277/759 [01:29<03:14,  2.48it/s]batch_loss: 0.3368, loss: 0.2450 ||:  37%|###6      | 279/759 [01:29<02:07,  3.75it/s]batch_loss: 0.0370, loss: 0.2443 ||:  37%|###6      | 280/759 [01:30<02:41,  2.96it/s]batch_loss: 0.3698, loss: 0.2447 ||:  37%|###7      | 281/759 [01:30<02:18,  3.45it/s]batch_loss: 0.2501, loss: 0.2448 ||:  37%|###7      | 282/759 [01:30<02:10,  3.67it/s]batch_loss: 0.2354, loss: 0.2447 ||:  37%|###7      | 283/759 [01:30<02:16,  3.50it/s]batch_loss: 0.2351, loss: 0.2464 ||:  38%|###7      | 285/759 [01:31<02:17,  3.45it/s]batch_loss: 0.1748, loss: 0.2474 ||:  38%|###7      | 287/759 [01:32<02:13,  3.52it/s]batch_loss: 0.3527, loss: 0.2477 ||:  38%|###7      | 288/759 [01:32<01:56,  4.05it/s]batch_loss: 0.1009, loss: 0.2477 ||:  38%|###8      | 290/759 [01:32<02:19,  3.36it/s]batch_loss: 0.1971, loss: 0.2475 ||:  38%|###8      | 291/759 [01:33<02:22,  3.28it/s]batch_loss: 0.3339, loss: 0.2478 ||:  38%|###8      | 292/759 [01:33<02:04,  3.75it/s]batch_loss: 0.3976, loss: 0.2483 ||:  39%|###8      | 293/759 [01:33<01:45,  4.43it/s]batch_loss: 0.2373, loss: 0.2485 ||:  39%|###8      | 295/759 [01:33<01:24,  5.46it/s]batch_loss: 0.0971, loss: 0.2479 ||:  39%|###8      | 296/759 [01:34<02:09,  3.57it/s]batch_loss: 0.2156, loss: 0.2478 ||:  39%|###9      | 297/759 [01:34<02:13,  3.45it/s]batch_loss: 0.1626, loss: 0.2476 ||:  39%|###9      | 298/759 [01:35<02:15,  3.41it/s]batch_loss: 0.1998, loss: 0.2474 ||:  39%|###9      | 299/759 [01:35<02:16,  3.36it/s]batch_loss: 0.3721, loss: 0.2478 ||:  40%|###9      | 300/759 [01:35<01:57,  3.90it/s]batch_loss: 0.3147, loss: 0.2480 ||:  40%|###9      | 301/759 [01:35<01:48,  4.23it/s]batch_loss: 0.1442, loss: 0.2477 ||:  40%|###9      | 302/759 [01:36<02:06,  3.61it/s]batch_loss: 0.1663, loss: 0.2474 ||:  40%|###9      | 303/759 [01:36<02:10,  3.51it/s]batch_loss: 0.1896, loss: 0.2472 ||:  40%|####      | 304/759 [01:36<02:13,  3.40it/s]batch_loss: 0.1090, loss: 0.2468 ||:  40%|####      | 305/759 [01:37<03:05,  2.44it/s]batch_loss: 0.0614, loss: 0.2462 ||:  40%|####      | 306/759 [01:37<02:56,  2.56it/s]batch_loss: 0.2512, loss: 0.2462 ||:  40%|####      | 307/759 [01:37<02:18,  3.26it/s]batch_loss: 0.0536, loss: 0.2456 ||:  41%|####      | 308/759 [01:38<02:24,  3.12it/s]batch_loss: 0.4354, loss: 0.2462 ||:  41%|####      | 309/759 [01:38<01:56,  3.86it/s]batch_loss: 0.1714, loss: 0.2459 ||:  41%|####      | 310/759 [01:38<02:04,  3.60it/s]batch_loss: 0.2480, loss: 0.2459 ||:  41%|####      | 311/759 [01:38<01:49,  4.09it/s]batch_loss: 0.1340, loss: 0.2456 ||:  41%|####1     | 312/759 [01:39<02:25,  3.07it/s]batch_loss: 0.1258, loss: 0.2452 ||:  41%|####1     | 313/759 [01:39<02:28,  3.00it/s]batch_loss: 0.1666, loss: 0.2449 ||:  41%|####1     | 314/759 [01:39<02:00,  3.69it/s]batch_loss: 0.2175, loss: 0.2449 ||:  42%|####1     | 315/759 [01:40<02:32,  2.92it/s]batch_loss: 0.1217, loss: 0.2445 ||:  42%|####1     | 316/759 [01:40<02:31,  2.92it/s]batch_loss: 0.1292, loss: 0.2452 ||:  42%|####1     | 318/759 [01:41<02:02,  3.60it/s]batch_loss: 0.1015, loss: 0.2448 ||:  42%|####2     | 319/759 [01:41<02:09,  3.39it/s]batch_loss: 0.1951, loss: 0.2446 ||:  42%|####2     | 320/759 [01:41<02:13,  3.29it/s]batch_loss: 0.0498, loss: 0.2440 ||:  42%|####2     | 321/759 [01:43<04:24,  1.66it/s]batch_loss: 0.0793, loss: 0.2435 ||:  42%|####2     | 322/759 [01:43<04:29,  1.62it/s]batch_loss: 0.1636, loss: 0.2432 ||:  43%|####2     | 323/759 [01:44<03:50,  1.89it/s]batch_loss: 0.2957, loss: 0.2434 ||:  43%|####2     | 324/759 [01:44<03:21,  2.15it/s]batch_loss: 0.1465, loss: 0.2431 ||:  43%|####2     | 325/759 [01:44<03:03,  2.36it/s]batch_loss: 0.5656, loss: 0.2455 ||:  43%|####3     | 327/759 [01:44<01:58,  3.65it/s]batch_loss: 0.1430, loss: 0.2451 ||:  43%|####3     | 328/759 [01:45<02:02,  3.53it/s]batch_loss: 0.1479, loss: 0.2448 ||:  43%|####3     | 329/759 [01:45<02:21,  3.04it/s]batch_loss: 0.1283, loss: 0.2445 ||:  43%|####3     | 330/759 [01:46<02:38,  2.71it/s]batch_loss: 0.0398, loss: 0.2439 ||:  44%|####3     | 331/759 [01:47<04:29,  1.59it/s]batch_loss: 0.0456, loss: 0.2433 ||:  44%|####3     | 332/759 [01:48<05:22,  1.33it/s]batch_loss: 0.0620, loss: 0.2428 ||:  44%|####4     | 334/759 [01:49<04:05,  1.73it/s]batch_loss: 0.1150, loss: 0.2425 ||:  44%|####4     | 335/759 [01:49<03:39,  1.93it/s]batch_loss: 0.0388, loss: 0.2419 ||:  44%|####4     | 336/759 [01:50<03:38,  1.93it/s]batch_loss: 0.3353, loss: 0.2421 ||:  44%|####4     | 337/759 [01:50<02:54,  2.42it/s]batch_loss: 0.0769, loss: 0.2416 ||:  45%|####4     | 338/759 [01:50<03:21,  2.09it/s]batch_loss: 0.3553, loss: 0.2427 ||:  45%|####4     | 340/759 [01:50<02:05,  3.34it/s]batch_loss: 0.0927, loss: 0.2423 ||:  45%|####4     | 341/759 [01:51<02:28,  2.82it/s]batch_loss: 0.1010, loss: 0.2419 ||:  45%|####5     | 342/759 [01:51<02:05,  3.32it/s]batch_loss: 0.2880, loss: 0.2420 ||:  45%|####5     | 343/759 [01:51<01:47,  3.87it/s]batch_loss: 0.1961, loss: 0.2423 ||:  45%|####5     | 345/759 [01:52<01:21,  5.08it/s]batch_loss: 0.0688, loss: 0.2418 ||:  46%|####5     | 346/759 [01:52<02:06,  3.27it/s]batch_loss: 0.0609, loss: 0.2413 ||:  46%|####5     | 347/759 [01:53<03:01,  2.27it/s]batch_loss: 0.3789, loss: 0.2425 ||:  46%|####5     | 349/759 [01:53<02:03,  3.31it/s]batch_loss: 0.2336, loss: 0.2425 ||:  46%|####6     | 350/759 [01:53<01:49,  3.73it/s]batch_loss: 0.3569, loss: 0.2441 ||:  46%|####6     | 352/759 [01:54<01:23,  4.86it/s]batch_loss: 0.0684, loss: 0.2436 ||:  47%|####6     | 353/759 [01:54<02:05,  3.24it/s]batch_loss: 0.2581, loss: 0.2446 ||:  47%|####6     | 355/759 [01:55<01:47,  3.76it/s]batch_loss: 0.0824, loss: 0.2442 ||:  47%|####6     | 356/759 [01:55<01:51,  3.62it/s]batch_loss: 0.3316, loss: 0.2444 ||:  47%|####7     | 357/759 [01:55<01:41,  3.98it/s]batch_loss: 0.0887, loss: 0.2440 ||:  47%|####7     | 358/759 [01:56<02:45,  2.43it/s]batch_loss: 0.2394, loss: 0.2440 ||:  47%|####7     | 359/759 [01:56<02:18,  2.88it/s]batch_loss: 0.1853, loss: 0.2438 ||:  47%|####7     | 360/759 [01:57<02:15,  2.94it/s]batch_loss: 0.0619, loss: 0.2433 ||:  48%|####7     | 361/759 [01:57<02:14,  2.95it/s]batch_loss: 0.5132, loss: 0.2448 ||:  48%|####7     | 363/759 [01:57<01:32,  4.29it/s]batch_loss: 0.2253, loss: 0.2448 ||:  48%|####7     | 364/759 [01:57<01:20,  4.93it/s]batch_loss: 0.1687, loss: 0.2446 ||:  48%|####8     | 365/759 [01:58<01:31,  4.32it/s]batch_loss: 0.2902, loss: 0.2446 ||:  48%|####8     | 367/759 [01:58<01:15,  5.20it/s]batch_loss: 0.2980, loss: 0.2448 ||:  48%|####8     | 368/759 [01:58<02:01,  3.21it/s]batch_loss: 0.0644, loss: 0.2443 ||:  49%|####8     | 369/759 [01:59<02:17,  2.85it/s]batch_loss: 0.2736, loss: 0.2444 ||:  49%|####8     | 370/759 [01:59<02:12,  2.94it/s]batch_loss: 0.3534, loss: 0.2446 ||:  49%|####8     | 371/759 [01:59<01:48,  3.58it/s]batch_loss: 0.5289, loss: 0.2454 ||:  49%|####9     | 372/759 [02:00<01:34,  4.11it/s]batch_loss: 0.4285, loss: 0.2459 ||:  49%|####9     | 373/759 [02:00<01:19,  4.83it/s]batch_loss: 0.2784, loss: 0.2460 ||:  49%|####9     | 374/759 [02:00<01:16,  5.03it/s]batch_loss: 0.1293, loss: 0.2457 ||:  49%|####9     | 375/759 [02:00<01:32,  4.16it/s]batch_loss: 0.3589, loss: 0.2481 ||:  50%|####9     | 377/759 [02:00<01:04,  5.88it/s]batch_loss: 0.0727, loss: 0.2476 ||:  50%|####9     | 378/759 [02:02<02:56,  2.16it/s]batch_loss: 0.2723, loss: 0.2477 ||:  50%|####9     | 379/759 [02:02<02:41,  2.36it/s]batch_loss: 0.1175, loss: 0.2473 ||:  50%|#####     | 380/759 [02:03<02:47,  2.26it/s]batch_loss: 0.3732, loss: 0.2477 ||:  50%|#####     | 381/759 [02:03<02:32,  2.48it/s]batch_loss: 0.2612, loss: 0.2477 ||:  50%|#####     | 382/759 [02:03<02:03,  3.06it/s]batch_loss: 0.6045, loss: 0.2486 ||:  50%|#####     | 383/759 [02:03<01:38,  3.83it/s]batch_loss: 0.1122, loss: 0.2483 ||:  51%|#####     | 384/759 [02:03<01:45,  3.55it/s]batch_loss: 0.0849, loss: 0.2479 ||:  51%|#####     | 385/759 [02:04<02:56,  2.12it/s]batch_loss: 0.0306, loss: 0.2473 ||:  51%|#####     | 386/759 [02:05<04:00,  1.55it/s]batch_loss: 0.5996, loss: 0.2482 ||:  51%|#####     | 387/759 [02:05<03:00,  2.07it/s]batch_loss: 0.0640, loss: 0.2477 ||:  51%|#####1    | 388/759 [02:06<03:26,  1.80it/s]batch_loss: 0.1427, loss: 0.2475 ||:  51%|#####1    | 389/759 [02:07<03:03,  2.02it/s]batch_loss: 0.1743, loss: 0.2473 ||:  51%|#####1    | 390/759 [02:07<02:47,  2.20it/s]batch_loss: 0.1261, loss: 0.2470 ||:  52%|#####1    | 391/759 [02:07<02:37,  2.34it/s]batch_loss: 0.4651, loss: 0.2475 ||:  52%|#####1    | 392/759 [02:07<02:05,  2.92it/s]batch_loss: 0.2389, loss: 0.2475 ||:  52%|#####1    | 393/759 [02:08<02:04,  2.95it/s]batch_loss: 0.2889, loss: 0.2483 ||:  52%|#####2    | 395/759 [02:08<01:19,  4.57it/s]batch_loss: 0.2222, loss: 0.2485 ||:  52%|#####2    | 397/759 [02:08<00:59,  6.03it/s]batch_loss: 0.1948, loss: 0.2484 ||:  52%|#####2    | 398/759 [02:08<00:58,  6.22it/s]batch_loss: 0.1904, loss: 0.2483 ||:  53%|#####2    | 399/759 [02:08<00:53,  6.69it/s]batch_loss: 0.0597, loss: 0.2478 ||:  53%|#####2    | 400/759 [02:09<01:57,  3.06it/s]batch_loss: 1.9849, loss: 0.2521 ||:  53%|#####2    | 401/759 [02:09<01:40,  3.58it/s]batch_loss: 0.2310, loss: 0.2524 ||:  53%|#####3    | 403/759 [02:10<01:10,  5.02it/s]batch_loss: 0.1293, loss: 0.2521 ||:  53%|#####3    | 404/759 [02:10<01:20,  4.40it/s]batch_loss: 0.2546, loss: 0.2521 ||:  53%|#####3    | 405/759 [02:10<01:28,  3.99it/s]batch_loss: 0.2699, loss: 0.2521 ||:  53%|#####3    | 406/759 [02:10<01:18,  4.50it/s]batch_loss: 0.1455, loss: 0.2519 ||:  54%|#####3    | 407/759 [02:11<01:16,  4.58it/s]batch_loss: 0.1567, loss: 0.2527 ||:  54%|#####3    | 409/759 [02:11<01:13,  4.76it/s]batch_loss: 0.2204, loss: 0.2526 ||:  54%|#####4    | 410/759 [02:11<01:22,  4.24it/s]batch_loss: 0.0521, loss: 0.2521 ||:  54%|#####4    | 411/759 [02:12<02:53,  2.01it/s]batch_loss: 0.2816, loss: 0.2519 ||:  54%|#####4    | 413/759 [02:13<01:59,  2.90it/s]batch_loss: 0.2020, loss: 0.2518 ||:  55%|#####4    | 414/759 [02:13<02:00,  2.85it/s]batch_loss: 0.1276, loss: 0.2515 ||:  55%|#####4    | 415/759 [02:14<02:15,  2.55it/s]batch_loss: 0.2096, loss: 0.2514 ||:  55%|#####4    | 416/759 [02:14<02:08,  2.67it/s]batch_loss: 0.4505, loss: 0.2519 ||:  55%|#####4    | 417/759 [02:14<02:05,  2.72it/s]batch_loss: 0.2328, loss: 0.2531 ||:  55%|#####5    | 419/759 [02:14<01:22,  4.12it/s]batch_loss: 0.1675, loss: 0.2529 ||:  55%|#####5    | 420/759 [02:15<01:29,  3.79it/s]batch_loss: 0.2577, loss: 0.2529 ||:  55%|#####5    | 421/759 [02:15<01:32,  3.64it/s]batch_loss: 0.1556, loss: 0.2527 ||:  56%|#####5    | 422/759 [02:16<01:51,  3.03it/s]batch_loss: 0.0708, loss: 0.2523 ||:  56%|#####5    | 423/759 [02:16<02:05,  2.69it/s]batch_loss: 0.1599, loss: 0.2520 ||:  56%|#####5    | 424/759 [02:16<02:02,  2.74it/s]batch_loss: 0.0814, loss: 0.2516 ||:  56%|#####5    | 425/759 [02:17<01:59,  2.80it/s]batch_loss: 0.1949, loss: 0.2515 ||:  56%|#####6    | 426/759 [02:17<01:56,  2.86it/s]batch_loss: 0.0875, loss: 0.2511 ||:  56%|#####6    | 427/759 [02:18<02:09,  2.57it/s]batch_loss: 0.1566, loss: 0.2509 ||:  56%|#####6    | 428/759 [02:18<02:01,  2.73it/s]batch_loss: 0.2588, loss: 0.2509 ||:  57%|#####6    | 429/759 [02:18<01:42,  3.22it/s]batch_loss: 0.1481, loss: 0.2507 ||:  57%|#####6    | 430/759 [02:18<01:41,  3.23it/s]batch_loss: 0.0618, loss: 0.2502 ||:  57%|#####6    | 431/759 [02:19<02:19,  2.35it/s]batch_loss: 1.9007, loss: 0.2543 ||:  57%|#####7    | 433/759 [02:19<01:27,  3.74it/s]batch_loss: 0.2068, loss: 0.2541 ||:  57%|#####7    | 434/759 [02:19<01:14,  4.36it/s]batch_loss: 0.6211, loss: 0.2565 ||:  57%|#####7    | 436/759 [02:19<00:52,  6.14it/s]batch_loss: 0.3731, loss: 0.2567 ||:  58%|#####7    | 437/759 [02:20<00:47,  6.72it/s]batch_loss: 0.2343, loss: 0.2567 ||:  58%|#####7    | 438/759 [02:20<00:47,  6.70it/s]batch_loss: 0.2285, loss: 0.2566 ||:  58%|#####7    | 439/759 [02:20<00:47,  6.70it/s]batch_loss: 0.1900, loss: 0.2565 ||:  58%|#####7    | 440/759 [02:20<01:04,  4.93it/s]batch_loss: 0.7326, loss: 0.2582 ||:  58%|#####8    | 442/759 [02:20<00:45,  6.92it/s]batch_loss: 0.2119, loss: 0.2581 ||:  58%|#####8    | 443/759 [02:21<01:10,  4.47it/s]batch_loss: 0.0903, loss: 0.2580 ||:  59%|#####8    | 445/759 [02:21<01:17,  4.04it/s]batch_loss: 0.1415, loss: 0.2578 ||:  59%|#####8    | 446/759 [02:22<01:26,  3.62it/s]batch_loss: 0.2008, loss: 0.2576 ||:  59%|#####8    | 447/759 [02:22<01:17,  4.01it/s]batch_loss: 0.0721, loss: 0.2572 ||:  59%|#####9    | 448/759 [02:22<01:26,  3.58it/s]batch_loss: 0.0744, loss: 0.2568 ||:  59%|#####9    | 449/759 [02:23<02:01,  2.56it/s]batch_loss: 0.1453, loss: 0.2566 ||:  59%|#####9    | 450/759 [02:23<01:53,  2.72it/s]batch_loss: 0.0887, loss: 0.2562 ||:  59%|#####9    | 451/759 [02:24<02:30,  2.04it/s]batch_loss: 0.2275, loss: 0.2561 ||:  60%|#####9    | 452/759 [02:24<02:01,  2.53it/s]batch_loss: 0.1471, loss: 0.2575 ||:  60%|#####9    | 454/759 [02:25<01:33,  3.27it/s]batch_loss: 0.1078, loss: 0.2577 ||:  60%|######    | 456/759 [02:25<01:21,  3.72it/s]batch_loss: 0.1128, loss: 0.2574 ||:  60%|######    | 457/759 [02:26<01:45,  2.86it/s]batch_loss: 0.1580, loss: 0.2586 ||:  60%|######    | 459/759 [02:26<01:27,  3.43it/s]batch_loss: 0.1418, loss: 0.2583 ||:  61%|######    | 460/759 [02:26<01:30,  3.32it/s]batch_loss: 0.0946, loss: 0.2579 ||:  61%|######    | 461/759 [02:27<01:33,  3.18it/s]batch_loss: 0.3190, loss: 0.2584 ||:  61%|######1   | 463/759 [02:27<01:08,  4.32it/s]batch_loss: 0.1151, loss: 0.2581 ||:  61%|######1   | 464/759 [02:28<01:25,  3.46it/s]batch_loss: 0.4662, loss: 0.2586 ||:  61%|######1   | 465/759 [02:28<01:28,  3.32it/s]batch_loss: 0.3394, loss: 0.2587 ||:  61%|######1   | 466/759 [02:28<01:17,  3.78it/s]batch_loss: 0.2488, loss: 0.2587 ||:  62%|######1   | 467/759 [02:28<01:07,  4.30it/s]batch_loss: 0.0329, loss: 0.2582 ||:  62%|######1   | 468/759 [02:29<02:03,  2.35it/s]batch_loss: 0.0682, loss: 0.2578 ||:  62%|######1   | 469/759 [02:30<02:15,  2.14it/s]batch_loss: 0.5655, loss: 0.2585 ||:  62%|######1   | 470/759 [02:30<01:44,  2.77it/s]batch_loss: 0.3678, loss: 0.2587 ||:  62%|######2   | 471/759 [02:30<01:41,  2.83it/s]batch_loss: 0.1311, loss: 0.2585 ||:  62%|######2   | 472/759 [02:30<01:27,  3.29it/s]batch_loss: 0.4048, loss: 0.2588 ||:  62%|######2   | 473/759 [02:30<01:11,  3.98it/s]batch_loss: 0.0674, loss: 0.2596 ||:  63%|######2   | 475/759 [02:31<01:16,  3.72it/s]batch_loss: 0.4398, loss: 0.2599 ||:  63%|######2   | 476/759 [02:31<01:08,  4.11it/s]batch_loss: 0.2236, loss: 0.2599 ||:  63%|######2   | 477/759 [02:31<01:01,  4.58it/s]batch_loss: 0.1424, loss: 0.2596 ||:  63%|######2   | 478/759 [02:32<01:16,  3.68it/s]batch_loss: 0.3114, loss: 0.2597 ||:  63%|######3   | 479/759 [02:32<01:19,  3.53it/s]batch_loss: 0.1495, loss: 0.2602 ||:  63%|######3   | 481/759 [02:33<01:18,  3.55it/s]batch_loss: 0.2024, loss: 0.2608 ||:  64%|######3   | 483/759 [02:33<01:08,  4.01it/s]batch_loss: 0.1637, loss: 0.2606 ||:  64%|######3   | 484/759 [02:33<01:12,  3.78it/s]batch_loss: 0.0274, loss: 0.2601 ||:  64%|######3   | 485/759 [02:34<01:55,  2.38it/s]batch_loss: 0.2717, loss: 0.2601 ||:  64%|######4   | 486/759 [02:34<01:36,  2.82it/s]batch_loss: 0.1635, loss: 0.2599 ||:  64%|######4   | 487/759 [02:35<01:33,  2.90it/s]batch_loss: 0.3388, loss: 0.2601 ||:  64%|######4   | 488/759 [02:35<01:17,  3.48it/s]batch_loss: 0.2510, loss: 0.2601 ||:  64%|######4   | 489/759 [02:35<01:20,  3.37it/s]batch_loss: 0.0653, loss: 0.2597 ||:  65%|######4   | 490/759 [02:36<01:53,  2.37it/s]batch_loss: 0.2593, loss: 0.2597 ||:  65%|######4   | 491/759 [02:36<01:46,  2.52it/s]batch_loss: 0.1494, loss: 0.2598 ||:  65%|######4   | 493/759 [02:37<01:23,  3.19it/s]batch_loss: 0.1459, loss: 0.2595 ||:  65%|######5   | 494/759 [02:37<01:25,  3.11it/s]batch_loss: 0.2579, loss: 0.2595 ||:  65%|######5   | 496/759 [02:37<01:12,  3.63it/s]batch_loss: 0.9812, loss: 0.2611 ||:  66%|######5   | 498/759 [02:38<00:54,  4.82it/s]batch_loss: 0.3179, loss: 0.2612 ||:  66%|######5   | 499/759 [02:38<00:59,  4.36it/s]batch_loss: 0.3353, loss: 0.2613 ||:  66%|######5   | 500/759 [02:38<00:51,  4.99it/s]batch_loss: 0.0425, loss: 0.2609 ||:  66%|######6   | 501/759 [02:39<01:35,  2.69it/s]batch_loss: 0.1110, loss: 0.2608 ||:  66%|######6   | 503/759 [02:40<01:27,  2.93it/s]batch_loss: 0.2044, loss: 0.2607 ||:  66%|######6   | 504/759 [02:40<01:26,  2.96it/s]batch_loss: 0.0816, loss: 0.2603 ||:  67%|######6   | 505/759 [02:40<01:36,  2.63it/s]batch_loss: 0.1032, loss: 0.2600 ||:  67%|######6   | 506/759 [02:41<01:46,  2.37it/s]batch_loss: 0.0553, loss: 0.2596 ||:  67%|######6   | 507/759 [02:41<01:57,  2.15it/s]batch_loss: 0.2630, loss: 0.2596 ||:  67%|######6   | 508/759 [02:42<01:36,  2.60it/s]batch_loss: 0.1018, loss: 0.2593 ||:  67%|######7   | 509/759 [02:42<01:35,  2.62it/s]batch_loss: 0.3088, loss: 0.2594 ||:  67%|######7   | 510/759 [02:42<01:19,  3.14it/s]batch_loss: 0.1238, loss: 0.2592 ||:  67%|######7   | 511/759 [02:43<01:22,  3.02it/s]batch_loss: 0.0957, loss: 0.2588 ||:  67%|######7   | 512/759 [02:43<01:33,  2.64it/s]batch_loss: 0.1205, loss: 0.2592 ||:  68%|######7   | 514/759 [02:43<01:13,  3.34it/s]batch_loss: 0.2213, loss: 0.2591 ||:  68%|######7   | 515/759 [02:44<01:01,  3.99it/s]batch_loss: 0.1412, loss: 0.2589 ||:  68%|######7   | 516/759 [02:44<01:05,  3.69it/s]batch_loss: 0.0526, loss: 0.2585 ||:  68%|######8   | 517/759 [02:45<01:57,  2.06it/s]batch_loss: 0.4240, loss: 0.2594 ||:  68%|######8   | 519/759 [02:45<01:16,  3.13it/s]batch_loss: 0.3531, loss: 0.2596 ||:  69%|######8   | 520/759 [02:45<01:03,  3.74it/s]batch_loss: 0.2236, loss: 0.2595 ||:  69%|######8   | 521/759 [02:45<00:56,  4.19it/s]batch_loss: 0.1026, loss: 0.2599 ||:  69%|######8   | 523/759 [02:46<00:51,  4.60it/s]batch_loss: 0.6613, loss: 0.2607 ||:  69%|######9   | 524/759 [02:46<00:47,  4.99it/s]batch_loss: 0.2693, loss: 0.2607 ||:  69%|######9   | 525/759 [02:46<00:43,  5.42it/s]batch_loss: 0.1889, loss: 0.2606 ||:  69%|######9   | 526/759 [02:46<00:42,  5.54it/s]batch_loss: 0.1977, loss: 0.2605 ||:  69%|######9   | 527/759 [02:46<00:42,  5.44it/s]batch_loss: 0.1113, loss: 0.2602 ||:  70%|######9   | 528/759 [02:47<01:13,  3.13it/s]batch_loss: 0.2139, loss: 0.2601 ||:  70%|######9   | 529/759 [02:47<00:59,  3.88it/s]batch_loss: 0.3863, loss: 0.2603 ||:  70%|######9   | 530/759 [02:47<00:51,  4.42it/s]batch_loss: 0.0817, loss: 0.2600 ||:  70%|######9   | 531/759 [02:48<01:31,  2.50it/s]batch_loss: 0.2528, loss: 0.2600 ||:  70%|#######   | 532/759 [02:48<01:24,  2.68it/s]batch_loss: 0.0601, loss: 0.2596 ||:  70%|#######   | 533/759 [02:49<01:57,  1.93it/s]batch_loss: 0.1138, loss: 0.2593 ||:  70%|#######   | 534/759 [02:50<01:44,  2.15it/s]batch_loss: 0.0760, loss: 0.2590 ||:  70%|#######   | 535/759 [02:50<01:59,  1.88it/s]batch_loss: 0.1807, loss: 0.2588 ||:  71%|#######   | 536/759 [02:51<01:43,  2.15it/s]batch_loss: 0.7872, loss: 0.2598 ||:  71%|#######   | 537/759 [02:51<01:19,  2.78it/s]batch_loss: 0.3994, loss: 0.2601 ||:  71%|#######   | 538/759 [02:51<01:03,  3.50it/s]batch_loss: 0.2047, loss: 0.2602 ||:  71%|#######1  | 540/759 [02:51<00:52,  4.13it/s]batch_loss: 0.1438, loss: 0.2600 ||:  71%|#######1  | 541/759 [02:52<00:57,  3.78it/s]batch_loss: 0.2848, loss: 0.2601 ||:  71%|#######1  | 542/759 [02:52<01:08,  3.19it/s]batch_loss: 0.3959, loss: 0.2603 ||:  72%|#######1  | 543/759 [02:52<01:08,  3.17it/s]batch_loss: 0.2563, loss: 0.2603 ||:  72%|#######1  | 544/759 [02:53<01:08,  3.16it/s]batch_loss: 0.0978, loss: 0.2600 ||:  72%|#######1  | 545/759 [02:54<01:39,  2.14it/s]batch_loss: 0.1694, loss: 0.2598 ||:  72%|#######1  | 546/759 [02:54<01:29,  2.37it/s]batch_loss: 0.1855, loss: 0.2597 ||:  72%|#######2  | 547/759 [02:54<01:11,  2.96it/s]batch_loss: 0.0440, loss: 0.2593 ||:  72%|#######2  | 548/759 [02:55<01:31,  2.30it/s]batch_loss: 0.2150, loss: 0.2592 ||:  72%|#######2  | 549/759 [02:55<01:26,  2.43it/s]batch_loss: 0.0766, loss: 0.2589 ||:  72%|#######2  | 550/759 [02:56<01:42,  2.04it/s]batch_loss: 0.1212, loss: 0.2603 ||:  73%|#######2  | 552/759 [02:56<01:22,  2.51it/s]batch_loss: 0.1861, loss: 0.2606 ||:  73%|#######2  | 554/759 [02:57<01:04,  3.16it/s]batch_loss: 0.3039, loss: 0.2606 ||:  73%|#######3  | 555/759 [02:57<01:05,  3.11it/s]batch_loss: 0.3289, loss: 0.2608 ||:  73%|#######3  | 556/759 [02:57<00:54,  3.69it/s]batch_loss: 0.1003, loss: 0.2609 ||:  74%|#######3  | 558/759 [02:58<00:50,  4.00it/s]batch_loss: 0.1359, loss: 0.2607 ||:  74%|#######3  | 559/759 [02:58<01:00,  3.28it/s]batch_loss: 0.1913, loss: 0.2606 ||:  74%|#######3  | 560/759 [02:58<00:53,  3.75it/s]batch_loss: 0.1415, loss: 0.2604 ||:  74%|#######3  | 561/759 [02:58<00:48,  4.06it/s]batch_loss: 0.1164, loss: 0.2601 ||:  74%|#######4  | 562/759 [02:59<00:57,  3.43it/s]batch_loss: 0.1186, loss: 0.2599 ||:  74%|#######4  | 563/759 [02:59<01:00,  3.25it/s]batch_loss: 0.0715, loss: 0.2595 ||:  74%|#######4  | 564/759 [03:00<01:27,  2.24it/s]batch_loss: 0.2219, loss: 0.2595 ||:  74%|#######4  | 565/759 [03:00<01:19,  2.45it/s]batch_loss: 0.2516, loss: 0.2594 ||:  75%|#######4  | 566/759 [03:00<01:03,  3.05it/s]batch_loss: 0.1238, loss: 0.2592 ||:  75%|#######4  | 567/759 [03:01<01:03,  3.04it/s]batch_loss: 0.1361, loss: 0.2590 ||:  75%|#######4  | 568/759 [03:02<01:42,  1.86it/s]batch_loss: 0.1735, loss: 0.2588 ||:  75%|#######4  | 569/759 [03:02<01:31,  2.09it/s]batch_loss: 0.0761, loss: 0.2585 ||:  75%|#######5  | 570/759 [03:03<01:31,  2.07it/s]batch_loss: 0.1373, loss: 0.2594 ||:  75%|#######5  | 572/759 [03:03<00:56,  3.30it/s]batch_loss: 0.3850, loss: 0.2596 ||:  75%|#######5  | 573/759 [03:03<00:51,  3.64it/s]batch_loss: 0.2254, loss: 0.2595 ||:  76%|#######5  | 574/759 [03:03<00:44,  4.16it/s]batch_loss: 0.4337, loss: 0.2598 ||:  76%|#######5  | 575/759 [03:03<00:50,  3.64it/s]batch_loss: 0.3301, loss: 0.2600 ||:  76%|#######5  | 576/759 [03:04<00:49,  3.73it/s]batch_loss: 0.1204, loss: 0.2597 ||:  76%|#######6  | 577/759 [03:04<00:53,  3.37it/s]batch_loss: 0.2292, loss: 0.2597 ||:  76%|#######6  | 578/759 [03:04<00:55,  3.26it/s]batch_loss: 0.3647, loss: 0.2598 ||:  76%|#######6  | 579/759 [03:05<00:44,  4.05it/s]batch_loss: 0.0589, loss: 0.2595 ||:  76%|#######6  | 580/759 [03:05<01:05,  2.73it/s]batch_loss: 0.1768, loss: 0.2594 ||:  77%|#######6  | 581/759 [03:06<01:03,  2.82it/s]batch_loss: 0.1053, loss: 0.2591 ||:  77%|#######6  | 582/759 [03:06<01:01,  2.89it/s]batch_loss: 0.1226, loss: 0.2589 ||:  77%|#######6  | 583/759 [03:06<01:01,  2.86it/s]batch_loss: 0.1050, loss: 0.2588 ||:  77%|#######7  | 585/759 [03:07<00:50,  3.43it/s]batch_loss: 0.1753, loss: 0.2587 ||:  77%|#######7  | 586/759 [03:07<00:43,  3.94it/s]batch_loss: 0.1157, loss: 0.2585 ||:  77%|#######7  | 587/759 [03:07<00:49,  3.46it/s]batch_loss: 0.2815, loss: 0.2585 ||:  77%|#######7  | 588/759 [03:08<00:51,  3.31it/s]batch_loss: 0.0500, loss: 0.2581 ||:  78%|#######7  | 589/759 [03:08<00:53,  3.20it/s]batch_loss: 0.2151, loss: 0.2581 ||:  78%|#######7  | 590/759 [03:08<00:55,  3.07it/s]batch_loss: 0.4371, loss: 0.2584 ||:  78%|#######7  | 591/759 [03:09<00:54,  3.10it/s]batch_loss: 0.1598, loss: 0.2582 ||:  78%|#######7  | 592/759 [03:09<00:47,  3.54it/s]batch_loss: 0.1117, loss: 0.2580 ||:  78%|#######8  | 593/759 [03:09<00:51,  3.21it/s]batch_loss: 0.0527, loss: 0.2576 ||:  78%|#######8  | 594/759 [03:09<00:51,  3.19it/s]batch_loss: 0.0314, loss: 0.2572 ||:  78%|#######8  | 595/759 [03:10<01:27,  1.86it/s]batch_loss: 0.0534, loss: 0.2569 ||:  79%|#######8  | 596/759 [03:11<01:36,  1.68it/s]batch_loss: 0.3624, loss: 0.2571 ||:  79%|#######8  | 597/759 [03:11<01:15,  2.14it/s]batch_loss: 0.0505, loss: 0.2567 ||:  79%|#######8  | 598/759 [03:12<01:25,  1.88it/s]batch_loss: 0.0572, loss: 0.2576 ||:  79%|#######9  | 600/759 [03:13<01:12,  2.19it/s]batch_loss: 0.0955, loss: 0.2574 ||:  79%|#######9  | 601/759 [03:13<01:19,  1.98it/s]batch_loss: 0.0650, loss: 0.2571 ||:  79%|#######9  | 602/759 [03:14<01:11,  2.18it/s]batch_loss: 0.2957, loss: 0.2571 ||:  79%|#######9  | 603/759 [03:14<00:57,  2.71it/s]batch_loss: 0.1248, loss: 0.2569 ||:  80%|#######9  | 604/759 [03:14<00:55,  2.79it/s]batch_loss: 0.1595, loss: 0.2567 ||:  80%|#######9  | 605/759 [03:15<01:01,  2.49it/s]batch_loss: 0.1753, loss: 0.2566 ||:  80%|#######9  | 607/759 [03:15<00:47,  3.19it/s]batch_loss: 0.0928, loss: 0.2563 ||:  80%|########  | 608/759 [03:16<00:48,  3.10it/s]batch_loss: 0.1035, loss: 0.2561 ||:  80%|########  | 609/759 [03:16<01:02,  2.42it/s]batch_loss: 0.3328, loss: 0.2562 ||:  80%|########  | 610/759 [03:16<00:57,  2.58it/s]batch_loss: 0.0519, loss: 0.2565 ||:  81%|########  | 612/759 [03:17<01:02,  2.33it/s]batch_loss: 0.3026, loss: 0.2566 ||:  81%|########  | 613/759 [03:18<00:58,  2.48it/s]batch_loss: 0.2774, loss: 0.2570 ||:  81%|########1 | 615/759 [03:18<00:39,  3.64it/s]batch_loss: 0.0506, loss: 0.2567 ||:  81%|########1 | 616/759 [03:19<01:13,  1.95it/s]batch_loss: 0.1694, loss: 0.2565 ||:  81%|########1 | 617/759 [03:20<01:06,  2.13it/s]batch_loss: 0.4127, loss: 0.2568 ||:  81%|########1 | 618/759 [03:20<01:00,  2.31it/s]batch_loss: 0.0977, loss: 0.2565 ||:  82%|########1 | 619/759 [03:20<00:49,  2.84it/s]batch_loss: 0.0357, loss: 0.2562 ||:  82%|########1 | 620/759 [03:21<00:56,  2.47it/s]batch_loss: 0.1274, loss: 0.2559 ||:  82%|########1 | 621/759 [03:21<00:52,  2.60it/s]batch_loss: 0.3394, loss: 0.2561 ||:  82%|########1 | 622/759 [03:21<00:49,  2.75it/s]batch_loss: 0.2494, loss: 0.2561 ||:  82%|########2 | 623/759 [03:22<00:49,  2.74it/s]batch_loss: 0.0879, loss: 0.2558 ||:  82%|########2 | 624/759 [03:22<00:41,  3.21it/s]batch_loss: 0.0529, loss: 0.2555 ||:  82%|########2 | 625/759 [03:22<00:43,  3.10it/s]batch_loss: 0.2876, loss: 0.2555 ||:  82%|########2 | 626/759 [03:22<00:34,  3.87it/s]batch_loss: 0.2038, loss: 0.2554 ||:  83%|########2 | 627/759 [03:22<00:30,  4.28it/s]batch_loss: 0.3076, loss: 0.2555 ||:  83%|########2 | 628/759 [03:23<00:26,  4.93it/s]batch_loss: 0.1847, loss: 0.2554 ||:  83%|########2 | 629/759 [03:23<00:31,  4.17it/s]batch_loss: 0.3864, loss: 0.2556 ||:  83%|########3 | 630/759 [03:23<00:27,  4.70it/s]batch_loss: 0.3184, loss: 0.2557 ||:  83%|########3 | 631/759 [03:23<00:23,  5.34it/s]batch_loss: 0.0910, loss: 0.2555 ||:  83%|########3 | 632/759 [03:24<00:31,  4.04it/s]batch_loss: 0.2370, loss: 0.2562 ||:  84%|########3 | 634/759 [03:24<00:29,  4.28it/s]batch_loss: 0.1751, loss: 0.2565 ||:  84%|########3 | 636/759 [03:24<00:27,  4.52it/s]batch_loss: 0.1481, loss: 0.2563 ||:  84%|########3 | 637/759 [03:25<00:29,  4.08it/s]batch_loss: 0.4576, loss: 0.2566 ||:  84%|########4 | 638/759 [03:25<00:25,  4.66it/s]batch_loss: 0.1832, loss: 0.2565 ||:  84%|########4 | 639/759 [03:25<00:22,  5.29it/s]batch_loss: 0.2941, loss: 0.2566 ||:  84%|########4 | 640/759 [03:25<00:21,  5.43it/s]batch_loss: 0.0233, loss: 0.2562 ||:  84%|########4 | 641/759 [03:26<00:34,  3.39it/s]batch_loss: 0.2318, loss: 0.2562 ||:  85%|########4 | 642/759 [03:26<00:30,  3.80it/s]batch_loss: 0.1611, loss: 0.2560 ||:  85%|########4 | 643/759 [03:26<00:32,  3.54it/s]batch_loss: 0.1418, loss: 0.2563 ||:  85%|########4 | 645/759 [03:27<00:27,  4.07it/s]batch_loss: 0.0670, loss: 0.2560 ||:  85%|########5 | 646/759 [03:27<00:34,  3.24it/s]batch_loss: 0.3323, loss: 0.2561 ||:  85%|########5 | 647/759 [03:27<00:28,  3.87it/s]batch_loss: 0.1024, loss: 0.2559 ||:  85%|########5 | 648/759 [03:28<00:31,  3.57it/s]batch_loss: 0.1072, loss: 0.2557 ||:  86%|########5 | 649/759 [03:28<00:36,  2.98it/s]batch_loss: 0.3715, loss: 0.2558 ||:  86%|########5 | 650/759 [03:28<00:30,  3.57it/s]batch_loss: 0.0422, loss: 0.2563 ||:  86%|########5 | 652/759 [03:29<00:38,  2.80it/s]batch_loss: 0.1135, loss: 0.2561 ||:  86%|########6 | 653/759 [03:30<00:41,  2.52it/s]batch_loss: 0.0537, loss: 0.2558 ||:  86%|########6 | 654/759 [03:30<00:52,  2.01it/s]batch_loss: 0.4240, loss: 0.2560 ||:  86%|########6 | 655/759 [03:31<00:41,  2.48it/s]batch_loss: 0.7375, loss: 0.2568 ||:  86%|########6 | 656/759 [03:31<00:33,  3.08it/s]batch_loss: 1.4144, loss: 0.2585 ||:  87%|########6 | 657/759 [03:31<00:26,  3.83it/s]batch_loss: 0.1160, loss: 0.2583 ||:  87%|########6 | 658/759 [03:31<00:29,  3.42it/s]batch_loss: 0.2849, loss: 0.2584 ||:  87%|########6 | 659/759 [03:32<00:36,  2.73it/s]batch_loss: 0.1677, loss: 0.2584 ||:  87%|########7 | 661/759 [03:32<00:28,  3.43it/s]batch_loss: 0.1977, loss: 0.2583 ||:  87%|########7 | 662/759 [03:32<00:25,  3.83it/s]batch_loss: 0.0824, loss: 0.2580 ||:  87%|########7 | 663/759 [03:33<00:31,  3.04it/s]batch_loss: 0.2765, loss: 0.2580 ||:  87%|########7 | 664/759 [03:33<00:27,  3.49it/s]batch_loss: 0.1333, loss: 0.2579 ||:  88%|########7 | 665/759 [03:33<00:24,  3.91it/s]batch_loss: 0.0729, loss: 0.2576 ||:  88%|########7 | 666/759 [03:34<00:30,  3.01it/s]batch_loss: 0.2801, loss: 0.2576 ||:  88%|########7 | 667/759 [03:34<00:26,  3.52it/s]batch_loss: 0.3177, loss: 0.2577 ||:  88%|########8 | 668/759 [03:34<00:21,  4.21it/s]batch_loss: 0.0707, loss: 0.2574 ||:  88%|########8 | 669/759 [03:34<00:27,  3.23it/s]batch_loss: 0.1389, loss: 0.2572 ||:  88%|########8 | 670/759 [03:35<00:31,  2.78it/s]batch_loss: 0.1534, loss: 0.2571 ||:  88%|########8 | 671/759 [03:35<00:31,  2.79it/s]batch_loss: 0.1445, loss: 0.2569 ||:  89%|########8 | 672/759 [03:36<00:31,  2.76it/s]batch_loss: 0.2300, loss: 0.2569 ||:  89%|########8 | 673/759 [03:36<00:30,  2.84it/s]batch_loss: 0.1935, loss: 0.2577 ||:  89%|########8 | 675/759 [03:36<00:19,  4.30it/s]batch_loss: 0.0768, loss: 0.2574 ||:  89%|########9 | 676/759 [03:37<00:30,  2.71it/s]batch_loss: 0.1325, loss: 0.2572 ||:  89%|########9 | 677/759 [03:37<00:25,  3.16it/s]batch_loss: 0.0558, loss: 0.2569 ||:  89%|########9 | 678/759 [03:38<00:41,  1.97it/s]batch_loss: 0.2992, loss: 0.2570 ||:  89%|########9 | 679/759 [03:38<00:36,  2.21it/s]batch_loss: 0.0845, loss: 0.2568 ||:  90%|########9 | 680/759 [03:39<00:40,  1.96it/s]batch_loss: 0.1021, loss: 0.2565 ||:  90%|########9 | 681/759 [03:39<00:35,  2.18it/s]batch_loss: 0.1481, loss: 0.2564 ||:  90%|########9 | 682/759 [03:40<00:39,  1.94it/s]batch_loss: 0.3486, loss: 0.2565 ||:  90%|########9 | 683/759 [03:40<00:31,  2.41it/s]batch_loss: 0.0832, loss: 0.2562 ||:  90%|######### | 684/759 [03:41<00:32,  2.31it/s]batch_loss: 0.0957, loss: 0.2560 ||:  90%|######### | 685/759 [03:41<00:29,  2.48it/s]batch_loss: 0.0472, loss: 0.2563 ||:  91%|######### | 687/759 [03:42<00:30,  2.34it/s]batch_loss: 0.2069, loss: 0.2562 ||:  91%|######### | 688/759 [03:42<00:28,  2.51it/s]batch_loss: 0.1828, loss: 0.2561 ||:  91%|######### | 689/759 [03:42<00:23,  2.93it/s]batch_loss: 0.1002, loss: 0.2559 ||:  91%|######### | 690/759 [03:43<00:22,  3.00it/s]batch_loss: 0.1801, loss: 0.2557 ||:  91%|#########1| 692/759 [03:43<00:19,  3.51it/s]batch_loss: 0.1797, loss: 0.2555 ||:  91%|#########1| 693/759 [03:43<00:17,  3.86it/s]batch_loss: 0.2536, loss: 0.2555 ||:  91%|#########1| 694/759 [03:44<00:15,  4.30it/s]batch_loss: 0.0621, loss: 0.2553 ||:  92%|#########1| 695/759 [03:45<00:27,  2.36it/s]batch_loss: 0.0455, loss: 0.2550 ||:  92%|#########1| 696/759 [03:46<00:38,  1.63it/s]batch_loss: 0.0465, loss: 0.2547 ||:  92%|#########1| 697/759 [03:46<00:35,  1.75it/s]batch_loss: 0.5072, loss: 0.2551 ||:  92%|#########2| 699/759 [03:46<00:22,  2.70it/s]batch_loss: 0.1036, loss: 0.2549 ||:  92%|#########2| 700/759 [03:46<00:18,  3.20it/s]batch_loss: 0.0569, loss: 0.2546 ||:  92%|#########2| 701/759 [03:47<00:26,  2.16it/s]batch_loss: 0.1660, loss: 0.2545 ||:  92%|#########2| 702/759 [03:47<00:20,  2.72it/s]batch_loss: 0.3291, loss: 0.2546 ||:  93%|#########2| 703/759 [03:48<00:16,  3.31it/s]batch_loss: 0.1348, loss: 0.2544 ||:  93%|#########2| 704/759 [03:48<00:14,  3.87it/s]batch_loss: 0.1499, loss: 0.2543 ||:  93%|#########2| 705/759 [03:48<00:15,  3.60it/s]batch_loss: 0.2586, loss: 0.2549 ||:  93%|#########3| 707/759 [03:48<00:10,  4.89it/s]batch_loss: 0.1464, loss: 0.2547 ||:  93%|#########3| 708/759 [03:49<00:14,  3.55it/s]batch_loss: 0.1866, loss: 0.2546 ||:  93%|#########3| 709/759 [03:49<00:12,  3.90it/s]batch_loss: 0.1262, loss: 0.2545 ||:  94%|#########3| 710/759 [03:49<00:14,  3.41it/s]batch_loss: 0.0708, loss: 0.2542 ||:  94%|#########3| 711/759 [03:50<00:14,  3.24it/s]batch_loss: 0.0498, loss: 0.2539 ||:  94%|#########3| 712/759 [03:50<00:19,  2.38it/s]batch_loss: 0.1865, loss: 0.2538 ||:  94%|#########3| 713/759 [03:51<00:15,  3.02it/s]batch_loss: 0.0915, loss: 0.2536 ||:  94%|#########4| 714/759 [03:51<00:15,  2.91it/s]batch_loss: 0.2825, loss: 0.2536 ||:  94%|#########4| 715/759 [03:51<00:12,  3.47it/s]batch_loss: 0.1347, loss: 0.2535 ||:  94%|#########4| 716/759 [03:51<00:12,  3.41it/s]batch_loss: 0.0382, loss: 0.2532 ||:  94%|#########4| 717/759 [03:52<00:17,  2.39it/s]batch_loss: 0.3789, loss: 0.2533 ||:  95%|#########4| 718/759 [03:52<00:14,  2.89it/s]batch_loss: 0.6273, loss: 0.2539 ||:  95%|#########4| 719/759 [03:52<00:11,  3.44it/s]batch_loss: 0.3497, loss: 0.2540 ||:  95%|#########4| 720/759 [03:53<00:09,  3.96it/s]batch_loss: 0.0954, loss: 0.2538 ||:  95%|#########4| 721/759 [03:53<00:10,  3.49it/s]batch_loss: 0.0653, loss: 0.2535 ||:  95%|#########5| 722/759 [03:54<00:16,  2.22it/s]batch_loss: 0.3537, loss: 0.2537 ||:  95%|#########5| 723/759 [03:54<00:12,  2.85it/s]batch_loss: 0.1719, loss: 0.2535 ||:  95%|#########5| 724/759 [03:54<00:10,  3.39it/s]batch_loss: 0.3538, loss: 0.2537 ||:  96%|#########5| 725/759 [03:54<00:08,  4.13it/s]batch_loss: 0.1368, loss: 0.2535 ||:  96%|#########5| 726/759 [03:55<00:08,  3.78it/s]batch_loss: 0.1289, loss: 0.2534 ||:  96%|#########5| 727/759 [03:55<00:08,  3.61it/s]batch_loss: 0.5493, loss: 0.2538 ||:  96%|#########6| 729/759 [03:55<00:05,  5.36it/s]batch_loss: 0.3205, loss: 0.2539 ||:  96%|#########6| 730/759 [03:55<00:05,  5.48it/s]batch_loss: 0.2331, loss: 0.2539 ||:  96%|#########6| 731/759 [03:56<00:06,  4.47it/s]batch_loss: 0.0799, loss: 0.2537 ||:  96%|#########6| 732/759 [03:56<00:07,  3.82it/s]batch_loss: 0.4346, loss: 0.2539 ||:  97%|#########6| 733/759 [03:56<00:05,  4.49it/s]batch_loss: 0.2976, loss: 0.2540 ||:  97%|#########6| 734/759 [03:56<00:04,  5.08it/s]batch_loss: 0.0883, loss: 0.2537 ||:  97%|#########6| 735/759 [03:56<00:05,  4.30it/s]batch_loss: 0.0595, loss: 0.2539 ||:  97%|#########7| 737/759 [03:57<00:05,  3.84it/s]batch_loss: 0.1471, loss: 0.2537 ||:  97%|#########7| 738/759 [03:57<00:05,  3.60it/s]batch_loss: 0.1927, loss: 0.2536 ||:  97%|#########7| 739/759 [03:58<00:06,  3.01it/s]batch_loss: 0.1392, loss: 0.2535 ||:  97%|#########7| 740/759 [03:58<00:06,  2.96it/s]batch_loss: 0.1149, loss: 0.2533 ||:  98%|#########7| 741/759 [03:59<00:06,  2.88it/s]batch_loss: 0.1332, loss: 0.2531 ||:  98%|#########7| 742/759 [03:59<00:05,  2.90it/s]batch_loss: 0.0602, loss: 0.2529 ||:  98%|#########7| 743/759 [04:00<00:08,  1.91it/s]batch_loss: 0.4671, loss: 0.2532 ||:  98%|#########8| 744/759 [04:00<00:06,  2.38it/s]batch_loss: 0.0768, loss: 0.2529 ||:  98%|#########8| 745/759 [04:01<00:06,  2.03it/s]batch_loss: 0.3798, loss: 0.2531 ||:  98%|#########8| 746/759 [04:01<00:05,  2.21it/s]batch_loss: 0.0315, loss: 0.2528 ||:  98%|#########8| 747/759 [04:02<00:05,  2.12it/s]batch_loss: 0.1251, loss: 0.2527 ||:  99%|#########8| 749/759 [04:02<00:03,  2.82it/s]batch_loss: 0.3492, loss: 0.2528 ||:  99%|#########8| 750/759 [04:02<00:02,  3.22it/s]batch_loss: 0.0525, loss: 0.2525 ||:  99%|#########8| 751/759 [04:03<00:02,  2.75it/s]batch_loss: 0.0526, loss: 0.2523 ||:  99%|#########9| 752/759 [04:03<00:02,  2.81it/s]batch_loss: 0.2183, loss: 0.2522 ||:  99%|#########9| 753/759 [04:03<00:02,  2.83it/s]batch_loss: 0.2823, loss: 0.2523 ||:  99%|#########9| 754/759 [04:04<00:01,  3.54it/s]batch_loss: 0.1860, loss: 0.2522 ||:  99%|#########9| 755/759 [04:04<00:00,  4.09it/s]batch_loss: 0.0509, loss: 0.2519 ||: 100%|#########9| 756/759 [04:04<00:00,  3.03it/s]batch_loss: 0.1144, loss: 0.2517 ||: 100%|#########9| 757/759 [04:05<00:00,  2.87it/s]batch_loss: 0.1888, loss: 0.2517 ||: 100%|#########9| 758/759 [04:05<00:00,  3.34it/s]batch_loss: 0.2381, loss: 0.2516 ||: 100%|##########| 759/759 [04:05<00:00,  3.24it/s]batch_loss: 0.2381, loss: 0.2516 ||: 100%|##########| 759/759 [04:05<00:00,  3.09it/s]
2023-11-07 18:44:10,987 - INFO - allennlp.training.gradient_descent_trainer - Validating
  0%|          | 0/253 [00:00<?, ?it/s]batch_loss: 0.0000, loss: 0.0000 ||:   0%|          | 1/253 [00:00<01:16,  3.29it/s]batch_loss: 0.0000, loss: 0.0000 ||:   1%|          | 2/253 [00:00<00:54,  4.61it/s]batch_loss: 0.0000, loss: 0.0000 ||:   2%|1         | 4/253 [00:00<00:47,  5.19it/s]batch_loss: 0.0000, loss: 0.0000 ||:   2%|1         | 5/253 [00:00<00:40,  6.09it/s]batch_loss: 0.0000, loss: 0.0000 ||:   2%|2         | 6/253 [00:01<01:01,  4.04it/s]batch_loss: 0.0000, loss: 0.0000 ||:   3%|2         | 7/253 [00:01<00:53,  4.56it/s]batch_loss: 0.0000, loss: 0.0000 ||:   3%|3         | 8/253 [00:01<00:57,  4.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:   4%|3         | 9/253 [00:02<01:30,  2.68it/s]batch_loss: 0.0000, loss: 0.0000 ||:   4%|3         | 10/253 [00:03<01:54,  2.13it/s]batch_loss: 0.0000, loss: 0.0000 ||:   5%|4         | 12/253 [00:03<01:21,  2.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:   5%|5         | 13/253 [00:04<02:00,  1.99it/s]batch_loss: 0.0000, loss: 0.0000 ||:   6%|5         | 14/253 [00:04<01:37,  2.45it/s]batch_loss: 0.0000, loss: 0.0000 ||:   6%|5         | 15/253 [00:04<01:29,  2.66it/s]batch_loss: 0.0000, loss: 0.0000 ||:   7%|6         | 17/253 [00:05<01:09,  3.38it/s]batch_loss: 0.0000, loss: 0.0000 ||:   7%|7         | 18/253 [00:05<01:09,  3.37it/s]batch_loss: 0.0000, loss: 0.0000 ||:   8%|7         | 19/253 [00:05<01:00,  3.87it/s]batch_loss: 0.0000, loss: 0.0000 ||:   8%|7         | 20/253 [00:06<01:20,  2.89it/s]batch_loss: 0.0000, loss: 0.0000 ||:   8%|8         | 21/253 [00:07<01:46,  2.19it/s]batch_loss: 0.0000, loss: 0.0000 ||:   9%|9         | 23/253 [00:07<01:17,  2.98it/s]batch_loss: 0.0000, loss: 0.0000 ||:   9%|9         | 24/253 [00:07<01:20,  2.85it/s]batch_loss: 0.0000, loss: 0.0000 ||:  10%|9         | 25/253 [00:08<01:23,  2.73it/s]batch_loss: 0.0000, loss: 0.0000 ||:  10%|#         | 26/253 [00:08<01:18,  2.88it/s]batch_loss: 0.0000, loss: 0.0000 ||:  11%|#1        | 28/253 [00:08<00:59,  3.78it/s]batch_loss: 0.0000, loss: 0.0000 ||:  11%|#1        | 29/253 [00:09<01:00,  3.73it/s]batch_loss: 0.0000, loss: 0.0000 ||:  12%|#1        | 30/253 [00:09<00:59,  3.74it/s]batch_loss: 0.0000, loss: 0.0000 ||:  13%|#2        | 32/253 [00:10<01:04,  3.45it/s]batch_loss: 0.0000, loss: 0.0000 ||:  13%|#3        | 33/253 [00:10<01:03,  3.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  13%|#3        | 34/253 [00:11<01:25,  2.55it/s]batch_loss: 0.0000, loss: 0.0000 ||:  14%|#3        | 35/253 [00:11<01:18,  2.77it/s]batch_loss: 0.0000, loss: 0.0000 ||:  14%|#4        | 36/253 [00:11<01:13,  2.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  15%|#4        | 37/253 [00:12<01:17,  2.77it/s]batch_loss: 0.0000, loss: 0.0000 ||:  15%|#5        | 38/253 [00:12<01:03,  3.38it/s]batch_loss: 0.0000, loss: 0.0000 ||:  16%|#5        | 40/253 [00:13<01:15,  2.83it/s]batch_loss: 0.0000, loss: 0.0000 ||:  16%|#6        | 41/253 [00:13<01:20,  2.65it/s]batch_loss: 0.0000, loss: 0.0000 ||:  17%|#6        | 42/253 [00:13<01:14,  2.83it/s]batch_loss: 0.0000, loss: 0.0000 ||:  17%|#6        | 43/253 [00:14<01:33,  2.24it/s]batch_loss: 0.0000, loss: 0.0000 ||:  17%|#7        | 44/253 [00:14<01:15,  2.78it/s]batch_loss: 0.0000, loss: 0.0000 ||:  18%|#7        | 45/253 [00:15<01:35,  2.18it/s]batch_loss: 0.0000, loss: 0.0000 ||:  18%|#8        | 46/253 [00:15<01:23,  2.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  19%|#8        | 47/253 [00:15<01:07,  3.06it/s]batch_loss: 0.0000, loss: 0.0000 ||:  19%|#8        | 48/253 [00:16<01:03,  3.22it/s]batch_loss: 0.0000, loss: 0.0000 ||:  19%|#9        | 49/253 [00:16<00:52,  3.87it/s]batch_loss: 0.0000, loss: 0.0000 ||:  20%|#9        | 50/253 [00:16<01:03,  3.21it/s]batch_loss: 0.0000, loss: 0.0000 ||:  20%|##        | 51/253 [00:16<01:09,  2.91it/s]batch_loss: 0.0000, loss: 0.0000 ||:  21%|##        | 53/253 [00:17<00:52,  3.83it/s]batch_loss: 0.0000, loss: 0.0000 ||:  21%|##1       | 54/253 [00:18<01:13,  2.71it/s]batch_loss: 0.0000, loss: 0.0000 ||:  22%|##1       | 55/253 [00:18<01:31,  2.17it/s]batch_loss: 0.0000, loss: 0.0000 ||:  22%|##2       | 56/253 [00:19<01:21,  2.43it/s]batch_loss: 0.0000, loss: 0.0000 ||:  23%|##2       | 57/253 [00:19<01:21,  2.40it/s]batch_loss: 0.0000, loss: 0.0000 ||:  23%|##2       | 58/253 [00:19<01:15,  2.58it/s]batch_loss: 0.0000, loss: 0.0000 ||:  23%|##3       | 59/253 [00:19<00:59,  3.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:  24%|##3       | 60/253 [00:20<00:57,  3.34it/s]batch_loss: 0.0000, loss: 0.0000 ||:  25%|##4       | 62/253 [00:20<00:51,  3.68it/s]batch_loss: 0.0000, loss: 0.0000 ||:  25%|##4       | 63/253 [00:20<00:51,  3.67it/s]batch_loss: 0.0000, loss: 0.0000 ||:  26%|##5       | 65/253 [00:21<00:42,  4.40it/s]batch_loss: 0.0000, loss: 0.0000 ||:  26%|##6       | 66/253 [00:22<01:09,  2.70it/s]batch_loss: 0.0000, loss: 0.0000 ||:  26%|##6       | 67/253 [00:22<01:05,  2.86it/s]batch_loss: 0.0000, loss: 0.0000 ||:  27%|##6       | 68/253 [00:22<01:01,  2.99it/s]batch_loss: 0.0000, loss: 0.0000 ||:  27%|##7       | 69/253 [00:22<00:58,  3.13it/s]batch_loss: 0.0000, loss: 0.0000 ||:  28%|##7       | 70/253 [00:23<01:27,  2.09it/s]batch_loss: 0.0000, loss: 0.0000 ||:  28%|##8       | 71/253 [00:24<01:23,  2.18it/s]batch_loss: 0.0000, loss: 0.0000 ||:  28%|##8       | 72/253 [00:24<01:36,  1.87it/s]batch_loss: 0.0000, loss: 0.0000 ||:  29%|##8       | 73/253 [00:25<01:30,  1.99it/s]batch_loss: 0.0000, loss: 0.0000 ||:  29%|##9       | 74/253 [00:25<01:18,  2.29it/s]batch_loss: 0.0000, loss: 0.0000 ||:  30%|##9       | 75/253 [00:25<01:09,  2.56it/s]batch_loss: 0.0000, loss: 0.0000 ||:  30%|###       | 76/253 [00:26<01:03,  2.77it/s]batch_loss: 0.0000, loss: 0.0000 ||:  30%|###       | 77/253 [00:26<00:59,  2.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:  31%|###       | 78/253 [00:27<01:19,  2.19it/s]batch_loss: 0.0000, loss: 0.0000 ||:  31%|###1      | 79/253 [00:27<01:09,  2.49it/s]batch_loss: 0.0000, loss: 0.0000 ||:  32%|###2      | 81/253 [00:27<00:44,  3.88it/s]batch_loss: 0.0000, loss: 0.0000 ||:  32%|###2      | 82/253 [00:27<00:44,  3.84it/s]batch_loss: 0.0000, loss: 0.0000 ||:  33%|###2      | 83/253 [00:28<00:37,  4.49it/s]batch_loss: 0.0000, loss: 0.0000 ||:  33%|###3      | 84/253 [00:28<01:02,  2.71it/s]batch_loss: 0.0000, loss: 0.0000 ||:  34%|###3      | 85/253 [00:29<00:58,  2.88it/s]batch_loss: 0.0000, loss: 0.0000 ||:  34%|###3      | 86/253 [00:29<00:48,  3.45it/s]batch_loss: 0.0000, loss: 0.0000 ||:  34%|###4      | 87/253 [00:29<00:47,  3.48it/s]batch_loss: 0.0000, loss: 0.0000 ||:  35%|###4      | 88/253 [00:29<00:46,  3.51it/s]batch_loss: 0.0000, loss: 0.0000 ||:  35%|###5      | 89/253 [00:30<01:20,  2.03it/s]batch_loss: 0.0000, loss: 0.0000 ||:  36%|###5      | 90/253 [00:31<01:19,  2.05it/s]batch_loss: 0.0000, loss: 0.0000 ||:  36%|###5      | 91/253 [00:31<01:08,  2.35it/s]batch_loss: 0.0000, loss: 0.0000 ||:  36%|###6      | 92/253 [00:31<01:01,  2.61it/s]batch_loss: 0.0000, loss: 0.0000 ||:  37%|###6      | 93/253 [00:32<00:49,  3.20it/s]batch_loss: 0.0000, loss: 0.0000 ||:  37%|###7      | 94/253 [00:32<00:55,  2.86it/s]batch_loss: 0.0000, loss: 0.0000 ||:  38%|###7      | 95/253 [00:32<00:52,  3.04it/s]batch_loss: 0.0000, loss: 0.0000 ||:  38%|###7      | 96/253 [00:33<00:49,  3.16it/s]batch_loss: 0.0000, loss: 0.0000 ||:  38%|###8      | 97/253 [00:33<00:47,  3.28it/s]batch_loss: 0.0000, loss: 0.0000 ||:  39%|###8      | 98/253 [00:33<00:52,  2.94it/s]batch_loss: 0.0000, loss: 0.0000 ||:  39%|###9      | 99/253 [00:34<01:22,  1.87it/s]batch_loss: 0.0000, loss: 0.0000 ||:  40%|###9      | 100/253 [00:35<01:10,  2.18it/s]batch_loss: 0.0000, loss: 0.0000 ||:  40%|###9      | 101/253 [00:35<01:01,  2.48it/s]batch_loss: 0.0000, loss: 0.0000 ||:  40%|####      | 102/253 [00:35<00:55,  2.71it/s]batch_loss: 0.0000, loss: 0.0000 ||:  41%|####1     | 104/253 [00:35<00:41,  3.57it/s]batch_loss: 0.0000, loss: 0.0000 ||:  42%|####1     | 106/253 [00:36<00:34,  4.22it/s]batch_loss: 0.0000, loss: 0.0000 ||:  42%|####2     | 107/253 [00:36<00:31,  4.60it/s]batch_loss: 0.0000, loss: 0.0000 ||:  43%|####2     | 108/253 [00:36<00:43,  3.30it/s]batch_loss: 0.0000, loss: 0.0000 ||:  43%|####3     | 109/253 [00:37<00:42,  3.39it/s]batch_loss: 0.0000, loss: 0.0000 ||:  44%|####3     | 111/253 [00:37<00:34,  4.12it/s]batch_loss: 0.0000, loss: 0.0000 ||:  44%|####4     | 112/253 [00:37<00:30,  4.56it/s]batch_loss: 0.0000, loss: 0.0000 ||:  45%|####4     | 113/253 [00:38<00:32,  4.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:  45%|####5     | 114/253 [00:38<00:44,  3.14it/s]batch_loss: 0.0000, loss: 0.0000 ||:  45%|####5     | 115/253 [00:39<00:54,  2.55it/s]batch_loss: 0.0000, loss: 0.0000 ||:  46%|####5     | 116/253 [00:39<00:43,  3.14it/s]batch_loss: 0.0000, loss: 0.0000 ||:  46%|####6     | 117/253 [00:39<00:41,  3.28it/s]batch_loss: 0.0000, loss: 0.0000 ||:  47%|####6     | 118/253 [00:39<00:33,  4.01it/s]batch_loss: 0.0000, loss: 0.0000 ||:  47%|####7     | 119/253 [00:39<00:34,  3.88it/s]batch_loss: 0.0000, loss: 0.0000 ||:  47%|####7     | 120/253 [00:40<00:47,  2.82it/s]batch_loss: 0.0000, loss: 0.0000 ||:  48%|####7     | 121/253 [00:40<00:43,  3.05it/s]batch_loss: 0.0000, loss: 0.0000 ||:  48%|####8     | 122/253 [00:41<00:51,  2.54it/s]batch_loss: 0.0000, loss: 0.0000 ||:  49%|####8     | 123/253 [00:41<00:52,  2.50it/s]batch_loss: 0.0000, loss: 0.0000 ||:  49%|####9     | 124/253 [00:42<01:03,  2.04it/s]batch_loss: 0.0000, loss: 0.0000 ||:  49%|####9     | 125/253 [00:42<00:59,  2.14it/s]batch_loss: 0.0000, loss: 0.0000 ||:  50%|####9     | 126/253 [00:43<00:57,  2.20it/s]batch_loss: 0.0000, loss: 0.0000 ||:  50%|#####     | 127/253 [00:43<00:44,  2.85it/s]batch_loss: 0.0000, loss: 0.0000 ||:  51%|#####     | 128/253 [00:43<00:34,  3.59it/s]batch_loss: 0.0000, loss: 0.0000 ||:  51%|#####1    | 130/253 [00:43<00:27,  4.46it/s]batch_loss: 0.0000, loss: 0.0000 ||:  52%|#####1    | 131/253 [00:44<00:37,  3.24it/s]batch_loss: 0.0000, loss: 0.0000 ||:  52%|#####2    | 132/253 [00:45<00:54,  2.23it/s]batch_loss: 0.0000, loss: 0.0000 ||:  53%|#####2    | 133/253 [00:45<00:57,  2.10it/s]batch_loss: 0.0000, loss: 0.0000 ||:  53%|#####2    | 134/253 [00:45<00:44,  2.65it/s]batch_loss: 0.0000, loss: 0.0000 ||:  53%|#####3    | 135/253 [00:46<00:45,  2.57it/s]batch_loss: 0.0000, loss: 0.0000 ||:  54%|#####3    | 136/253 [00:46<00:46,  2.53it/s]batch_loss: 0.0000, loss: 0.0000 ||:  54%|#####4    | 137/253 [00:47<00:41,  2.77it/s]batch_loss: 0.0000, loss: 0.0000 ||:  55%|#####4    | 138/253 [00:47<00:38,  2.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  55%|#####4    | 139/253 [00:48<00:56,  2.02it/s]batch_loss: 0.0000, loss: 0.0000 ||:  55%|#####5    | 140/253 [00:48<00:48,  2.33it/s]batch_loss: 0.0000, loss: 0.0000 ||:  56%|#####5    | 141/253 [00:49<00:56,  1.97it/s]batch_loss: 0.0000, loss: 0.0000 ||:  56%|#####6    | 142/253 [00:49<00:48,  2.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:  57%|#####6    | 143/253 [00:49<00:37,  2.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:  57%|#####6    | 144/253 [00:49<00:29,  3.71it/s]batch_loss: 0.0000, loss: 0.0000 ||:  57%|#####7    | 145/253 [00:49<00:29,  3.69it/s]batch_loss: 0.0000, loss: 0.0000 ||:  58%|#####7    | 146/253 [00:50<00:29,  3.63it/s]batch_loss: 0.0000, loss: 0.0000 ||:  58%|#####8    | 147/253 [00:50<00:38,  2.74it/s]batch_loss: 0.0000, loss: 0.0000 ||:  58%|#####8    | 148/253 [00:51<00:35,  2.97it/s]batch_loss: 0.0000, loss: 0.0000 ||:  59%|#####8    | 149/253 [00:51<00:45,  2.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:  59%|#####9    | 150/253 [00:52<00:40,  2.56it/s]batch_loss: 0.0000, loss: 0.0000 ||:  60%|#####9    | 151/253 [00:52<00:31,  3.19it/s]batch_loss: 0.0000, loss: 0.0000 ||:  60%|######    | 152/253 [00:52<00:35,  2.85it/s]batch_loss: 0.0000, loss: 0.0000 ||:  60%|######    | 153/253 [00:52<00:32,  3.04it/s]batch_loss: 0.0000, loss: 0.0000 ||:  61%|######    | 154/253 [00:53<00:31,  3.16it/s]batch_loss: 0.0000, loss: 0.0000 ||:  61%|######1   | 155/253 [00:53<00:38,  2.57it/s]batch_loss: 0.0000, loss: 0.0000 ||:  62%|######1   | 156/253 [00:54<00:38,  2.49it/s]batch_loss: 0.0000, loss: 0.0000 ||:  62%|######2   | 157/253 [00:54<00:35,  2.71it/s]batch_loss: 0.0000, loss: 0.0000 ||:  62%|######2   | 158/253 [00:54<00:32,  2.90it/s]batch_loss: 0.0000, loss: 0.0000 ||:  63%|######3   | 160/253 [00:55<00:31,  2.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  64%|######3   | 161/253 [00:55<00:29,  3.09it/s]batch_loss: 0.0000, loss: 0.0000 ||:  64%|######4   | 162/253 [00:56<00:38,  2.36it/s]batch_loss: 0.0000, loss: 0.0000 ||:  64%|######4   | 163/253 [00:56<00:37,  2.38it/s]batch_loss: 0.0000, loss: 0.0000 ||:  65%|######4   | 164/253 [00:56<00:29,  3.01it/s]batch_loss: 0.0000, loss: 0.0000 ||:  65%|######5   | 165/253 [00:57<00:31,  2.81it/s]batch_loss: 0.0000, loss: 0.0000 ||:  66%|######5   | 166/253 [00:58<00:47,  1.84it/s]batch_loss: 0.0000, loss: 0.0000 ||:  66%|######6   | 167/253 [00:58<00:40,  2.15it/s]batch_loss: 0.0000, loss: 0.0000 ||:  66%|######6   | 168/253 [00:58<00:31,  2.70it/s]batch_loss: 0.0000, loss: 0.0000 ||:  67%|######6   | 169/253 [00:59<00:39,  2.13it/s]batch_loss: 0.0000, loss: 0.0000 ||:  67%|######7   | 170/253 [00:59<00:41,  2.01it/s]batch_loss: 0.0000, loss: 0.0000 ||:  68%|######7   | 171/253 [01:00<00:39,  2.08it/s]batch_loss: 0.0000, loss: 0.0000 ||:  68%|######7   | 172/253 [01:01<00:44,  1.81it/s]batch_loss: 0.0000, loss: 0.0000 ||:  68%|######8   | 173/253 [01:01<00:37,  2.11it/s]batch_loss: 0.0000, loss: 0.0000 ||:  69%|######9   | 175/253 [01:02<00:31,  2.46it/s]batch_loss: 0.0000, loss: 0.0000 ||:  70%|######9   | 176/253 [01:02<00:39,  1.94it/s]batch_loss: 0.0000, loss: 0.0000 ||:  70%|######9   | 177/253 [01:03<00:37,  2.05it/s]batch_loss: 0.0000, loss: 0.0000 ||:  70%|#######   | 178/253 [01:04<00:46,  1.61it/s]batch_loss: 0.0000, loss: 0.0000 ||:  71%|#######   | 179/253 [01:04<00:41,  1.78it/s]batch_loss: 0.0000, loss: 0.0000 ||:  71%|#######1  | 180/253 [01:04<00:34,  2.09it/s]batch_loss: 0.0000, loss: 0.0000 ||:  72%|#######1  | 181/253 [01:05<00:30,  2.39it/s]batch_loss: 0.0000, loss: 0.0000 ||:  72%|#######1  | 182/253 [01:05<00:29,  2.39it/s]batch_loss: 0.0000, loss: 0.0000 ||:  72%|#######2  | 183/253 [01:05<00:26,  2.67it/s]batch_loss: 0.0000, loss: 0.0000 ||:  73%|#######2  | 184/253 [01:06<00:20,  3.40it/s]batch_loss: 0.0000, loss: 0.0000 ||:  74%|#######3  | 186/253 [01:06<00:13,  4.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:  74%|#######3  | 187/253 [01:06<00:15,  4.38it/s]batch_loss: 0.0000, loss: 0.0000 ||:  74%|#######4  | 188/253 [01:06<00:13,  4.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  75%|#######4  | 189/253 [01:07<00:16,  3.83it/s]batch_loss: 0.0000, loss: 0.0000 ||:  75%|#######5  | 190/253 [01:07<00:19,  3.27it/s]batch_loss: 0.0000, loss: 0.0000 ||:  75%|#######5  | 191/253 [01:07<00:15,  4.04it/s]batch_loss: 0.0000, loss: 0.0000 ||:  76%|#######5  | 192/253 [01:08<00:18,  3.34it/s]batch_loss: 0.0000, loss: 0.0000 ||:  76%|#######6  | 193/253 [01:09<00:30,  1.94it/s]batch_loss: 0.0000, loss: 0.0000 ||:  77%|#######6  | 194/253 [01:09<00:23,  2.52it/s]batch_loss: 0.0000, loss: 0.0000 ||:  77%|#######7  | 195/253 [01:09<00:23,  2.47it/s]batch_loss: 0.0000, loss: 0.0000 ||:  77%|#######7  | 196/253 [01:10<00:23,  2.42it/s]batch_loss: 0.0000, loss: 0.0000 ||:  78%|#######7  | 197/253 [01:10<00:25,  2.17it/s]batch_loss: 0.0000, loss: 0.0000 ||:  78%|#######8  | 198/253 [01:11<00:24,  2.25it/s]batch_loss: 0.0000, loss: 0.0000 ||:  79%|#######8  | 199/253 [01:11<00:21,  2.54it/s]batch_loss: 0.0000, loss: 0.0000 ||:  79%|#######9  | 200/253 [01:11<00:19,  2.77it/s]batch_loss: 0.0000, loss: 0.0000 ||:  79%|#######9  | 201/253 [01:11<00:17,  2.97it/s]batch_loss: 0.0000, loss: 0.0000 ||:  80%|#######9  | 202/253 [01:12<00:27,  1.85it/s]batch_loss: 0.0000, loss: 0.0000 ||:  80%|########  | 203/253 [01:13<00:23,  2.13it/s]batch_loss: 0.0000, loss: 0.0000 ||:  81%|########  | 204/253 [01:13<00:24,  2.02it/s]batch_loss: 0.0000, loss: 0.0000 ||:  81%|########1 | 206/253 [01:13<00:14,  3.35it/s]batch_loss: 0.0000, loss: 0.0000 ||:  82%|########1 | 207/253 [01:14<00:17,  2.56it/s]batch_loss: 0.0000, loss: 0.0000 ||:  82%|########2 | 208/253 [01:14<00:14,  3.17it/s]batch_loss: 0.0000, loss: 0.0000 ||:  83%|########2 | 209/253 [01:15<00:19,  2.30it/s]batch_loss: 0.0000, loss: 0.0000 ||:  83%|########3 | 210/253 [01:15<00:20,  2.12it/s]batch_loss: 0.0000, loss: 0.0000 ||:  83%|########3 | 211/253 [01:16<00:15,  2.70it/s]batch_loss: 0.0000, loss: 0.0000 ||:  84%|########3 | 212/253 [01:16<00:18,  2.16it/s]batch_loss: 0.0000, loss: 0.0000 ||:  84%|########4 | 213/253 [01:17<00:19,  2.01it/s]batch_loss: 0.0000, loss: 0.0000 ||:  85%|########4 | 214/253 [01:17<00:19,  1.96it/s]batch_loss: 0.0000, loss: 0.0000 ||:  85%|########4 | 215/253 [01:18<00:16,  2.28it/s]batch_loss: 0.0000, loss: 0.0000 ||:  85%|########5 | 216/253 [01:18<00:17,  2.11it/s]batch_loss: 0.0000, loss: 0.0000 ||:  86%|########5 | 217/253 [01:19<00:24,  1.50it/s]batch_loss: 0.0000, loss: 0.0000 ||:  87%|########6 | 219/253 [01:19<00:13,  2.56it/s]batch_loss: 0.0000, loss: 0.0000 ||:  87%|########6 | 220/253 [01:20<00:11,  2.77it/s]batch_loss: 0.0000, loss: 0.0000 ||:  87%|########7 | 221/253 [01:20<00:09,  3.38it/s]batch_loss: 0.0000, loss: 0.0000 ||:  88%|########8 | 223/253 [01:20<00:06,  4.63it/s]batch_loss: 0.0000, loss: 0.0000 ||:  89%|########8 | 224/253 [01:20<00:06,  4.19it/s]batch_loss: 0.0000, loss: 0.0000 ||:  89%|########8 | 225/253 [01:21<00:11,  2.51it/s]batch_loss: 0.0000, loss: 0.0000 ||:  90%|########9 | 227/253 [01:22<00:09,  2.75it/s]batch_loss: 0.0000, loss: 0.0000 ||:  90%|######### | 228/253 [01:22<00:08,  2.86it/s]batch_loss: 0.0000, loss: 0.0000 ||:  91%|######### | 230/253 [01:23<00:06,  3.49it/s]batch_loss: 0.0000, loss: 0.0000 ||:  92%|#########1| 232/253 [01:23<00:06,  3.35it/s]batch_loss: 0.0000, loss: 0.0000 ||:  92%|#########2| 233/253 [01:24<00:06,  3.03it/s]batch_loss: 0.0000, loss: 0.0000 ||:  92%|#########2| 234/253 [01:24<00:07,  2.44it/s]batch_loss: 0.0000, loss: 0.0000 ||:  93%|#########2| 235/253 [01:24<00:06,  2.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:  94%|#########3| 237/253 [01:25<00:04,  3.68it/s]batch_loss: 0.0000, loss: 0.0000 ||:  94%|#########4| 238/253 [01:25<00:05,  2.97it/s]batch_loss: 0.0000, loss: 0.0000 ||:  94%|#########4| 239/253 [01:26<00:04,  3.46it/s]batch_loss: 0.0000, loss: 0.0000 ||:  95%|#########4| 240/253 [01:26<00:05,  2.52it/s]batch_loss: 0.0000, loss: 0.0000 ||:  95%|#########5| 241/253 [01:27<00:06,  1.95it/s]batch_loss: 0.0000, loss: 0.0000 ||:  96%|#########6| 243/253 [01:27<00:03,  2.70it/s]batch_loss: 0.0000, loss: 0.0000 ||:  96%|#########6| 244/253 [01:28<00:03,  2.62it/s]batch_loss: 0.0000, loss: 0.0000 ||:  97%|#########6| 245/253 [01:28<00:02,  3.13it/s]batch_loss: 0.0000, loss: 0.0000 ||:  97%|#########7| 246/253 [01:28<00:01,  3.65it/s]batch_loss: 0.0000, loss: 0.0000 ||:  98%|#########8| 248/253 [01:29<00:01,  3.37it/s]batch_loss: 0.0000, loss: 0.0000 ||:  98%|#########8| 249/253 [01:29<00:01,  3.37it/s]batch_loss: 0.0000, loss: 0.0000 ||:  99%|#########8| 250/253 [01:30<00:01,  2.71it/s]batch_loss: 0.0000, loss: 0.0000 ||:  99%|#########9| 251/253 [01:30<00:00,  3.07it/s]batch_loss: 0.0000, loss: 0.0000 ||: 100%|#########9| 252/253 [01:30<00:00,  3.71it/s]batch_loss: 0.0000, loss: 0.0000 ||: 100%|##########| 253/253 [01:30<00:00,  3.54it/s]batch_loss: 0.0000, loss: 0.0000 ||: 100%|##########| 253/253 [01:30<00:00,  2.79it/s]
2023-11-07 18:45:41,902 - INFO - allennlp.training.callbacks.console_logger -                         Training |  Validation
2023-11-07 18:45:41,902 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB     |  13024.752  |       N/A
2023-11-07 18:45:41,902 - INFO - allennlp.training.callbacks.console_logger - iterx_famus_slot_f1 |     0.000  |     0.000
2023-11-07 18:45:41,902 - INFO - allennlp.training.callbacks.console_logger - iterx_famus_slot_p  |     0.000  |     0.000
2023-11-07 18:45:41,902 - INFO - allennlp.training.callbacks.console_logger - iterx_famus_slot_r  |     0.000  |     0.000
2023-11-07 18:45:41,902 - INFO - allennlp.training.callbacks.console_logger - loss                |     0.252  |     0.000
2023-11-07 18:45:41,902 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB  |  8647.445  |       N/A
2023-11-07 18:45:46,716 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:05:41.363089
2023-11-07 18:45:46,717 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 13:56:46
2023-11-07 18:45:46,717 - INFO - allennlp.training.gradient_descent_trainer - Epoch 2/149
2023-11-07 18:45:46,717 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 8.4G
2023-11-07 18:45:46,717 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 13G
2023-11-07 18:45:46,719 - INFO - allennlp.training.gradient_descent_trainer - Training
  0%|          | 0/759 [00:00<?, ?it/s]batch_loss: 0.1986, loss: 0.1986 ||:   0%|          | 1/759 [00:00<02:25,  5.22it/s]batch_loss: 0.0539, loss: 0.1263 ||:   0%|          | 2/759 [00:00<03:25,  3.68it/s]batch_loss: 0.1239, loss: 0.1255 ||:   0%|          | 3/759 [00:00<04:36,  2.73it/s]batch_loss: 0.1210, loss: 0.1244 ||:   1%|          | 4/759 [00:01<05:25,  2.32it/s]batch_loss: 0.2229, loss: 0.1441 ||:   1%|          | 5/759 [00:01<04:10,  3.01it/s]batch_loss: 0.4883, loss: 0.2014 ||:   1%|          | 6/759 [00:01<03:12,  3.92it/s]batch_loss: 0.1566, loss: 0.1950 ||:   1%|          | 7/759 [00:02<03:33,  3.52it/s]batch_loss: 0.1100, loss: 0.1844 ||:   1%|1         | 8/759 [00:02<03:49,  3.27it/s]batch_loss: 0.5670, loss: 0.2269 ||:   1%|1         | 9/759 [00:02<03:06,  4.02it/s]batch_loss: 0.0470, loss: 0.2089 ||:   1%|1         | 10/759 [00:03<05:34,  2.24it/s]batch_loss: 0.0720, loss: 0.2347 ||:   2%|1         | 12/759 [00:04<04:37,  2.69it/s]batch_loss: 0.1152, loss: 0.2215 ||:   2%|1         | 14/759 [00:04<03:48,  3.26it/s]batch_loss: 0.7295, loss: 0.2554 ||:   2%|1         | 15/759 [00:04<03:23,  3.65it/s]batch_loss: 0.1099, loss: 0.2463 ||:   2%|2         | 16/759 [00:05<03:44,  3.32it/s]batch_loss: 0.1943, loss: 0.2432 ||:   2%|2         | 17/759 [00:05<03:53,  3.17it/s]batch_loss: 0.1418, loss: 0.2376 ||:   2%|2         | 18/759 [00:05<04:22,  2.83it/s]batch_loss: 0.1092, loss: 0.2308 ||:   3%|2         | 19/759 [00:06<04:44,  2.60it/s]batch_loss: 0.1942, loss: 0.2290 ||:   3%|2         | 20/759 [00:06<04:00,  3.07it/s]batch_loss: 0.1377, loss: 0.2395 ||:   3%|2         | 22/759 [00:06<02:51,  4.29it/s]batch_loss: 0.2557, loss: 0.2392 ||:   3%|3         | 24/759 [00:06<02:18,  5.31it/s]batch_loss: 0.1488, loss: 0.2356 ||:   3%|3         | 25/759 [00:07<03:10,  3.84it/s]batch_loss: 0.1514, loss: 0.2323 ||:   3%|3         | 26/759 [00:07<03:27,  3.54it/s]batch_loss: 0.2423, loss: 0.2327 ||:   4%|3         | 27/759 [00:08<03:39,  3.34it/s]batch_loss: 0.1274, loss: 0.2342 ||:   4%|3         | 29/759 [00:08<03:17,  3.70it/s]batch_loss: 0.1389, loss: 0.2310 ||:   4%|3         | 30/759 [00:09<05:22,  2.26it/s]batch_loss: 0.1485, loss: 0.2284 ||:   4%|4         | 31/759 [00:10<05:03,  2.40it/s]batch_loss: 0.3965, loss: 0.2336 ||:   4%|4         | 32/759 [00:10<04:14,  2.86it/s]batch_loss: 0.3581, loss: 0.2374 ||:   4%|4         | 33/759 [00:10<04:09,  2.91it/s]batch_loss: 0.4754, loss: 0.2444 ||:   4%|4         | 34/759 [00:10<03:37,  3.33it/s]batch_loss: 0.0492, loss: 0.2388 ||:   5%|4         | 35/759 [00:11<03:47,  3.18it/s]batch_loss: 0.1160, loss: 0.2354 ||:   5%|4         | 36/759 [00:11<05:10,  2.33it/s]batch_loss: 0.0738, loss: 0.2310 ||:   5%|4         | 37/759 [00:12<06:05,  1.98it/s]batch_loss: 0.2612, loss: 0.2318 ||:   5%|5         | 38/759 [00:12<04:52,  2.46it/s]batch_loss: 0.1476, loss: 0.2297 ||:   5%|5         | 39/759 [00:12<03:58,  3.02it/s]batch_loss: 0.1428, loss: 0.2275 ||:   5%|5         | 40/759 [00:13<03:56,  3.04it/s]batch_loss: 0.1755, loss: 0.2262 ||:   5%|5         | 41/759 [00:13<04:30,  2.66it/s]batch_loss: 0.1397, loss: 0.2242 ||:   6%|5         | 42/759 [00:13<03:52,  3.09it/s]batch_loss: 0.3791, loss: 0.2278 ||:   6%|5         | 43/759 [00:13<03:04,  3.88it/s]batch_loss: 0.1295, loss: 0.2255 ||:   6%|5         | 44/759 [00:14<03:17,  3.62it/s]